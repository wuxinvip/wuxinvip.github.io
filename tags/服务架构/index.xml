<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>服务架构 on 无心技术簿</title>
    <link>https://wuxinvip.github.io/tags/%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/</link>
    <description>Recent content in 服务架构 on 无心技术簿</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 24 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://wuxinvip.github.io/tags/%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>分布式架构</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-1/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-1/</guid>
      <description>最近几年关于架构的信息 高并发架构、 异地多活架构【出自淘宝】 容器化【知名docker、阿里Pouch】 微服务架构【spring cloud】 高可用架构 弹性架构【DB中间件 需要极致的弹性】
相关的技术 DevOps、应用监控、自动化运维、SOA服务治理、去IOE等等
分布式能解决的两大问题： 1、系统容量更大 面对的业务量与日俱增、垂直水平拆分系统业务 2、系统可用性更强 整个系统不会因为一个单点故障而导致整个系统不可用 分布式冗余节点、以消除单点故障
分布式优势： 1、模块化、系统模块重用度更高 2、模块化、服务开发、发布更快 3、系统扩展性更高 4、团队协作更有效率
分布式存在的问题： 1、设计复杂 2、部署单个简单、部署多个复杂 3、系统吞吐量增大、系统反应变慢 4、运维复杂 5、学习难度加大 6、测试复杂 7、技术复杂、带来维护复杂 8、系统中的服务调度、监控等等复杂
分布式前景： 可以说分布式是无法避免的、随着业务量的增大不可能单点跑应用、不同的应用场景会产出不用的服务架构、学习成本是逐渐加大的、等分布式更加成熟形成了体系、应该会产出更加系统的学习方案和应用方案、</description>
    </item>
    
    <item>
      <title>医院总流程图</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-2/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>千万级别系统搭建</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-3/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-3/</guid>
      <description>结合springcloud 1.服务分级SOA 2.流量带宽(1M17用户.按一千万并发计算需要600M带宽.当然1M17用户前提是网页数据60kb) 3.数据库分层(垂直.横向是最基本的.还有灵活添加\卸载数据库). 千万级别用户.数据量至少亿级别.数据库每百万分一个. 至少100+数据库(若要是应对高并发.高峰流量冲击可以使用AliSQL) 4.那么准备zuul网关服务器就得有(按每个tomcat并发1500计算) 要考虑的点. 网卡属性.tomcat支持并发. nginx支持并发(用来放在zuul前面可以支持一台服务器部署多个zuul). 内存容量.cpu计算能力. 内存用量和cpu计算能力 限定了数据库和tomcat计算能力.所以也不是单看一个点就能估算并发能力 5.还有访问深度.访问跟踪各个服务跟踪状况. 还是得学习啊  限流
限制瞬时并发 限制总并发数 限制时间窗口内平均速率 相关算法：滑动窗口协议 漏桶--- 令牌桶----应对突发流量 计数器  </description>
    </item>
    
    <item>
      <title>用户池系统 构思</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-4/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-4/</guid>
      <description>以年龄为例 task 获取数据以年龄排序 将数据以list形式存放到redis中 key 可从配置中获取 .
为了安全不建议直接把key发送到前端.可在配置做一个等价替换
不同的key对应不同的分组数据
前端访问webapp 获取key的加密值 和 默认信息列表
获取不同排序 前端发送不同的key值即可
跑批任务最后可以在redis数据失效之前把数据存储到mysql 优点: 1.不需要修改webapp和webview 2.只需要修改跑批任务(算法添加、不同算法对应的配置更新到cloud config) 3.数据存储样式可用“配置_list”存储
问题: 1.用户抢单后要削减用户列表.元列表并发存在修改问题 解决:在最终抢单时候来后台查询已抢单列表就行.修不修改的有错误也没事.
(使用mysql存储数据? 也不能解决这个问题 反而会触及mysql并发量问题)
目前方案:mysql主从复制.从&amp;rdquo;从sql&amp;rdquo;查询数据</description>
    </item>
    
    <item>
      <title>系统设计一些基本原则</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-0/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-0/</guid>
      <description>参照梁飞&amp;ndash;dubbo创始人&amp;mdash;一些设计上的基本常识
不看不知道 一看吓一跳 原来一个复杂的服务系统这么多门道
以前只晓得 API SPI分离。。。
API 与 SPI 分离 服务域/实体域/会话域分离 在重要的过程上设置拦截接口 重要的状态的变更发送事件并留出监听接口 扩展接口职责尽可能单一，具有可组合性 微核插件式，平等对待第三方 不要控制外部对象的生命周期 可配置一定可编程，并保持友好的 CoC 约定 区分命令与查询，明确前置条件与后置条件 增量式扩展，而不要扩充原始核心概念  </description>
    </item>
    
    <item>
      <title>风控系统</title>
      <link>https://wuxinvip.github.io/blog/service-design/design-5/</link>
      <pubDate>Sat, 24 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://wuxinvip.github.io/blog/service-design/design-5/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>